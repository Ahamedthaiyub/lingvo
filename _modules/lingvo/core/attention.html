

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.core.attention &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.attention</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Attention models.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">quant_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">inplace_ops</span>  <span class="c1"># pylint:disable=g-direct-tensorflow-import</span>


<span class="c1"># Currently, quantization statistics cannot be accumulated across arbitrary</span>
<span class="c1"># defuns, so we allow them to be disabled. A potentially more robust fix is</span>
<span class="c1"># to save and merge the attention state across the defun boundary as is</span>
<span class="c1"># done in recurrent.py.</span>
<div class="viewcode-block" id="_ConditionalCallDefun"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention._ConditionalCallDefun">[docs]</a><span class="k">def</span> <span class="nf">_ConditionalCallDefun</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">cond</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span></div>


<div class="viewcode-block" id="_ApplyAttentionDropout"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention._ApplyAttentionDropout">[docs]</a><span class="k">def</span> <span class="nf">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply attention dropout according to the given parameters.</span>

<span class="sd">  If `params.atten_dropout_deterministic` is set to True, the dropout will be</span>
<span class="sd">  fully deterministic.</span>

<span class="sd">  Args:</span>
<span class="sd">    params: The parameters of attention layer.</span>
<span class="sd">    x: A float Tensor on which to apply dropout.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same shape as `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>

  <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateStepSeedPair</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">DeterministicDropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
                                         <span class="n">seeds</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="SafeCumprod"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.SafeCumprod">[docs]</a><span class="k">def</span> <span class="nf">SafeCumprod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes cumprod of x in logspace using cumsum to avoid underflow.</span>

<span class="sd">  The cumprod function and its gradient can result in numerical instabilities</span>
<span class="sd">  when its argument has very small and/or zero values.  As long as the argument</span>
<span class="sd">  is all positive, we can instead compute the cumulative product as</span>
<span class="sd">  exp(cumsum(log(x))).  This function can be called identically to</span>
<span class="sd">  tf.math.cumprod.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: Tensor to take the cumulative product of.</span>
<span class="sd">    *args: Passed on to cumsum; these are identical to those in cumprod.</span>
<span class="sd">    **kwargs: Passed on to cumsum; these are identical to those in cumprod.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Cumulative product of x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;SafeCumprod&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">tiny</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">as_numpy_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">CumSum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tiny</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span></div>


<span class="c1"># pyformat: disable</span>
<div class="viewcode-block" id="MonotonicAttentionProb"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttentionProb">[docs]</a><span class="k">def</span> <span class="nf">MonotonicAttentionProb</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">,</span> <span class="n">previous_attention</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute monotonic attention distribution from choosing probabilities.</span>

<span class="sd">  Monotonic attention implies that the input sequence is processed in an</span>
<span class="sd">  explicitly left-to-right manner when generating the output sequence.  In</span>
<span class="sd">  addition, once an input sequence element is attended to at a given output</span>
<span class="sd">  timestep, elements occurring before it cannot be attended to at subsequent</span>
<span class="sd">  output timesteps.  This function generates attention distributions according</span>
<span class="sd">  to these assumptions.  For more information, see `Online and Linear-Time</span>
<span class="sd">  Attention by Enforcing Monotonic Alignments`.</span>

<span class="sd">  Args:</span>
<span class="sd">    p_choose_i: Probability of choosing input sequence/memory element i.  Should</span>
<span class="sd">      be of shape (batch_size, input_sequence_length), and should all be in the</span>
<span class="sd">      range [0, 1].</span>
<span class="sd">    previous_attention: The attention distribution from the previous output</span>
<span class="sd">      timestep.  Should be of shape (batch_size, input_sequence_length).  For</span>
<span class="sd">      the first output timestep, preevious_attention[n] should be [1, 0, 0, ...,</span>
<span class="sd">      0] for all n in [0, ... batch_size - 1].</span>
<span class="sd">    mode: How to compute the attention distribution. Must be one of `recursive`,</span>
<span class="sd">      `parallel`, or `hard`.</span>

<span class="sd">      * recursive: uses tf.scan to recursively compute the distribution. This is</span>
<span class="sd">        slowest but is exact, general, and does not suffer from numerical</span>
<span class="sd">        instabilities.</span>
<span class="sd">      * parallel: uses parallelized cumulative-sum and cumulative-product</span>
<span class="sd">        operations to compute a closed-form solution to the recurrence relation</span>
<span class="sd">        defining the attention distribution.  This makes it more efficient than</span>
<span class="sd">        &#39;recursive&#39;, but it requires numerical checks which make the</span>
<span class="sd">        distribution non-exact.  This can be a problem in particular when</span>
<span class="sd">        input_sequence_length is long and/or p_choose_i has entries very close</span>
<span class="sd">        to 0 or 1.</span>
<span class="sd">      * hard: requires that the probabilities in p_choose_i are all either 0 or</span>
<span class="sd">        1, and subsequently uses a more efficient and exact solution.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of shape (batch_size, input_sequence_length) representing the</span>
<span class="sd">    attention distributions for each sequence in the batch.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: mode is not one of &#39;recursive&#39;, &#39;parallel&#39;, &#39;hard&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pyformat: enable</span>
  <span class="c1"># Force things to be tensors</span>
  <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;p_choose_i&#39;</span><span class="p">)</span>
  <span class="n">previous_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
      <span class="n">previous_attention</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;previous_attention&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;recursive&#39;</span><span class="p">:</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="c1"># Compute [1, 1 - p_choose_i[0], 1 - p_choose_i[1], ..., 1 - p_choose_i[-2]]</span>
    <span class="n">shifted_1mp_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p_choose_i</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Compute attention distribution recursively as</span>
    <span class="c1"># q[i] = (1 - p_choose_i[i - 1])*q[i - 1] + previous_attention[i]</span>
    <span class="c1"># attention[i] = p_choose_i[i]*q[i]</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">p_choose_i</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
            <span class="c1"># Need to use reshape to remind TF of the shape between loop</span>
            <span class="c1"># iterations.</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">yz</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">yz</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">yz</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)),</span>
            <span class="c1"># Loop variables yz[0] and yz[1]</span>
            <span class="p">[</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">shifted_1mp_choose_i</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">previous_attention</span><span class="p">)</span>
            <span class="p">],</span>
            <span class="c1"># Initial value of x is just zeros</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,))))</span>
  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;parallel&#39;</span><span class="p">:</span>
    <span class="c1"># SafeCumprod computes cumprod in logspace with numeric checks</span>
    <span class="n">cumprod_1mp_choose_i</span> <span class="o">=</span> <span class="n">SafeCumprod</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_choose_i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Compute recurrence relation solution</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">p_choose_i</span> <span class="o">*</span> <span class="n">cumprod_1mp_choose_i</span> <span class="o">*</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CumSum</span><span class="p">(</span>
        <span class="n">previous_attention</span> <span class="o">/</span>
        <span class="c1"># Clip cumprod_1mp to avoid divide-by-zero</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">cumprod_1mp_choose_i</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;hard&#39;</span><span class="p">:</span>
    <span class="c1"># Remove any probabilities before the index chosen last time step</span>
    <span class="n">p_choose_i</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">previous_attention</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Now, use exclusive cumprod to remove probabilities after the first</span>
    <span class="c1"># chosen index, like so:</span>
    <span class="c1"># p_choose_i = [0, 0, 0, 1, 1, 0, 1, 1]</span>
    <span class="c1"># cumprod(1 - p_choose_i, exclusive=True) = [1, 1, 1, 1, 0, 0, 0, 0]</span>
    <span class="c1"># Product of above: [0, 0, 0, 1, 0, 0, 0, 0]</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">p_choose_i</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span>
        <span class="mi">1</span> <span class="o">-</span> <span class="n">p_choose_i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode must be &#39;recursive&#39;, &#39;parallel&#39;, or &#39;hard&#39;.&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">attention</span></div>


<div class="viewcode-block" id="BaseAttentionLayer"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer">[docs]</a><span class="k">class</span> <span class="nc">BaseAttentionLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A base class for all attention layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BaseAttentionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_dropout_deterministic&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether to dropout in a fully deterministic way, which is more &#39;</span>
        <span class="s1">&#39;suitable for TPU.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;QDomain for the internal softmax.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fullyconnected&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Fully connected layers are fed &#39;</span>
        <span class="s1">&#39;into activation functions which have known input ranges&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a BaseAttentionLayer object.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;params.name is not set.&#39;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="BaseAttentionLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;fullyconnected&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.InitForSourcePacked"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.InitForSourcePacked">[docs]</a>  <span class="k">def</span> <span class="nf">InitForSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">theta</span><span class="p">,</span>
                          <span class="n">source_vecs</span><span class="p">,</span>
                          <span class="n">source_contexts</span><span class="p">,</span>
                          <span class="n">source_padding</span><span class="p">,</span>
                          <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize attention for the given source vectors.</span>

<span class="sd">    Must set `_source_init_done` to True in the function.</span>

<span class="sd">    Note: `source_segment_id`, if present, should always have the same shape as</span>
<span class="sd">    `source_padding`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size]. source_segment_id</span>
<span class="sd">        is not None for packed inputs where one training example may pack</span>
<span class="sd">        multiple sequences.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` object to be passed to ComputeContextVectorWithSource.</span>
<span class="sd">      The internal structure of the return value should be considered an</span>
<span class="sd">      implementation detail of the attention mechanism and should not be</span>
<span class="sd">      inspected or modified by its callers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_packed_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">,</span>
                                       <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_src</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors.</span>

<span class="sd">    Does not change attention state.</span>

<span class="sd">    Note: `source_segment_id`, if present, should always have the same shape as</span>
<span class="sd">    `source_padding`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size]. source_segment_id</span>
<span class="sd">        is not None for packed inputs where one training example may pack</span>
<span class="sd">        multiple sequences.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` object to be passed to ComputeContextVectorWithSource.</span>
<span class="sd">      The internal structure of the return value should be considered an</span>
<span class="sd">      implementation detail of the attention mechanism and should not be</span>
<span class="sd">      inspected or modified by its callers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Abstract method.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should have shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors</span>
<span class="sd">        with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Abstract method.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.ComputeContextVector"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.ComputeContextVector">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">theta</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span>
                           <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Unlike `ComputeContextVectorWithSource` which explicitly asks for the packed</span>
<span class="sd">    source tensors, `ComputeContextVector` uses the class&#39; internal variables.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector.</span>
<span class="sd">      - The attention probability vector.</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_src</span><span class="p">,</span>
                                               <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span>
                                               <span class="n">per_step_source_padding</span><span class="p">,</span>
                                               <span class="n">query_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.GetInitializationSourceState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.GetInitializationSourceState">[docs]</a>  <span class="k">def</span> <span class="nf">GetInitializationSourceState</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the attention initialization state.</span>

<span class="sd">    The base class only preserves the `concated_source_vecs`,</span>
<span class="sd">    `concated_source_contexts` and `source_padding`. If subclasses use more</span>
<span class="sd">    state than this and need to interact with inference code that must</span>
<span class="sd">    fetch and reload state, this and `SetInitializationSourceState` must</span>
<span class="sd">    be overridden.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` of Tensors that can be preserved and reset via</span>
<span class="sd">      `SetInitializationSourceState()` at a later point. This allows, for</span>
<span class="sd">      example, for attention computations to span session runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_src</span></div>

<div class="viewcode-block" id="BaseAttentionLayer.SetInitializationSourceState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer.SetInitializationSourceState">[docs]</a>  <span class="k">def</span> <span class="nf">SetInitializationSourceState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_init_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the attention initialization state.</span>

<span class="sd">    Args:</span>
<span class="sd">      new_init_state: A `.NestedMap` matching what was returned from</span>
<span class="sd">        `GetInitializationSourceState`, which will return this layer to that</span>
<span class="sd">        initialization state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_source_init_done</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_packed_src</span> <span class="o">=</span> <span class="n">new_init_state</span><span class="o">.</span><span class="n">DeepCopy</span><span class="p">()</span></div>

<div class="viewcode-block" id="BaseAttentionLayer._PaddedSoftmax"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer._PaddedSoftmax">[docs]</a>  <span class="k">def</span> <span class="nf">_PaddedSoftmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">narrow_to_asym_bit_depth</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Performs a softmax as if padding were applied after exponentiation.</span>

<span class="sd">    The default implementation uses numerical techniques to approximate this</span>
<span class="sd">    with a standard `tf.nn.softmax` (using large negative logits for padded</span>
<span class="sd">    values). It defers to a `Defun` that may be replaced on low-range</span>
<span class="sd">    implementations with a version that is numerically correct.</span>

<span class="sd">    Args:</span>
<span class="sd">      logits: Logits.</span>
<span class="sd">      padding: Padding (must be the same shape as logits).</span>
<span class="sd">      narrow_to_asym_bit_depth: Narrows the bit depth, removing the upper limit</span>
<span class="sd">        value. This is to accommodate certain interpreters that would cover a 0</span>
<span class="sd">        .... 2**bits - 1 range for quantization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Result of the softmax.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>

    <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">)</span>
    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">)</span>
    <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padding</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="c1"># TFLite hardcodes the range of qsoftmax, setting explicitly to avoid</span>
    <span class="c1"># incompatible concats.</span>
    <span class="k">return</span> <span class="n">fns</span><span class="o">.</span><span class="n">qsoftmax</span><span class="p">(</span>
        <span class="n">padded_logits</span><span class="p">,</span>
        <span class="n">qdomain</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
        <span class="n">narrow_to_asym_bit_depth</span><span class="o">=</span><span class="n">narrow_to_asym_bit_depth</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseAttentionLayer._UpdatePaddingWithPackedInputMask"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.BaseAttentionLayer._UpdatePaddingWithPackedInputMask">[docs]</a>  <span class="k">def</span> <span class="nf">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">source_segment_ids</span><span class="p">,</span>
                                        <span class="n">query_segment_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates an attention mask based on source and query segment ids.</span>

<span class="sd">    This creates a mask that removes invalid attention, where the query vector</span>
<span class="sd">    might assign some weight to neighboring sequences in a packed input example.</span>
<span class="sd">    Assumes `n = target_batch // source_batch`.</span>

<span class="sd">    Args:</span>
<span class="sd">      padding: Padding for logits, a tensor of shape [time, n, source_batch].</span>
<span class="sd">      source_segment_ids: a tensor of shape [time, source_batch].</span>
<span class="sd">      query_segment_ids: a tensor of shape [target_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Logits with mask applied.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generating packed input mask for attention padding.</span>
    <span class="n">source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">query_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">query_segment_ids</span><span class="p">,</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">)[</span><span class="mi">2</span><span class="p">]])</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">source_segment_ids</span><span class="p">,</span> <span class="n">query_segment_ids</span><span class="p">),</span> <span class="n">padding</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">padding</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">padding</span></div></div>


<div class="viewcode-block" id="AdditiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention">[docs]</a><span class="k">class</span> <span class="nc">AdditiveAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements additive attention (also known as &quot;Bahdanau Attention&quot;).</span>

<span class="sd">  Described in:</span>

<span class="sd">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span>
<span class="sd">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span>
<span class="sd">  ICLR 2015.</span>
<span class="sd">  https://arxiv.org/abs/1409.0473</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="AdditiveAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this `AdditiveAttention` class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">GaussianSqrtDim</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;same_batch_size&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True iff the source and target sequence has the same batch size.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an `AdditiveAttention` object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates probs.&quot;&quot;&quot;</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">per_step_source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>

      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">concated_source_vecs</span> <span class="o">+</span> <span class="n">inputs</span><span class="o">.</span><span class="n">query_vec_reshaped</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="c1">#</span>
      <span class="c1"># Another equivalent way is to do:</span>
      <span class="c1">#  logits = tf.reduce_sum(summed *</span>
      <span class="c1">#                         tf.reshape(v, [1, 1, 1, hidden_dim]), 3)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
      <span class="c1"># Take out the padding states.</span>
      <span class="c1"># _source_padding is of shape [source_length, source_batch].</span>
      <span class="c1"># reshaped to [source_length, 1, source_batch].</span>
      <span class="c1"># per_step_source_padding is reshaped to the same but with &#39;multiplier&#39;</span>
      <span class="c1"># for the second dim.</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">per_step_source_padding</span><span class="p">),</span>
          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
            <span class="n">source_padding</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">query_segment_id</span><span class="p">)</span>
      <span class="c1"># Reshape logits to a matrix of shape [target_batch, source_length] and</span>
      <span class="c1"># takes the softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">]))</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">]))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>

    <span class="c1"># Adds the atten function into the graph&#39;s library.</span>
    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: hidden weight. [hidden_dim, 1].</span>
<span class="sd">        w: query weight. [query_dim, hidden_dim].</span>
<span class="sd">        source_padding: [source_length, source_batch].</span>
<span class="sd">        source_segment_id: [source_lentgh, source_batch]</span>
<span class="sd">        concated_source_vecs: [source_length, source_batch, hidden_dim].</span>
<span class="sd">        concated_source_contexts: [source_batch, source_length, context_dim]</span>
<span class="sd">        query_vec: [target_batch, query_dim]</span>
<span class="sd">        query_segment_id: [target_batch]</span>
<span class="sd">        per_step_source_padding: [target_batch, source_length]</span>
<span class="sd">      Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">        attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">        concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">        attention context vector is computed as a weighted average of the</span>
<span class="sd">        concated_source_contexts, using the scores that were computed using</span>
<span class="sd">        concated_source_vecs.</span>

<span class="sd">      Returns:</span>
<span class="sd">        attention context vectors and probabilities.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="c1"># concated_source_vecs is reshaped to</span>
      <span class="c1"># [source_length, 1, source_batch, hidden_dims]</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

      <span class="c1"># query_vec is reshaped to</span>
      <span class="c1"># [1, target_batch/source_batch, source_batch, hidden_dims].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">query_vec_transformed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
      <span class="c1"># probs is of shape [target_batch, source_length]</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">AttenProbs</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">concated_source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
              <span class="n">query_vec_reshaped</span><span class="o">=</span><span class="n">query_vec_reshaped</span><span class="p">,</span>
              <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="o">=</span><span class="n">per_step_source_padding</span><span class="p">,</span>
              <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_segment_id</span><span class="p">))</span>
      <span class="n">probs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="c1"># Apply dropout to weights if applicable.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

      <span class="c1"># Reshape probs to be of shape</span>
      <span class="c1"># [target_batch/source_batch, source_batch, source_length]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape</span>
      <span class="c1"># [source_batch, target_batch/source_batch, source_length]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># Batched matmul</span>
      <span class="c1"># [source_batch, target_batch/source_batch, source_length] *</span>
      <span class="c1"># [source_batch, source_length, context_dim] =</span>
      <span class="c1"># [source_batch, target_batch/source_batch, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>

      <span class="c1"># summed is of shape</span>
      <span class="c1"># [target_batch/source_batch, source_batch, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">probs</span>

    <span class="c1"># The source batch size equals to the target batch size.</span>
    <span class="k">def</span> <span class="nf">AttenSameBatchSize</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
                           <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">,</span>
                           <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: hidden weight. [hidden_dim].</span>
<span class="sd">        w: query weight. [query_dim, hidden_dim].</span>
<span class="sd">        source_padding: [sl, b]</span>
<span class="sd">        source_segment_id: [sl, b]</span>
<span class="sd">        concated_source_vecs: [sl, b, hidden_dim].</span>
<span class="sd">        concated_source_contexts: [b, sl, context_dim]</span>
<span class="sd">        query_vec: [b, query_dim]</span>
<span class="sd">        query_segment_id: [b]</span>
<span class="sd">        per_step_source_padding: [b, sl]</span>

<span class="sd">      Returns:</span>
<span class="sd">        attention context vectors and probabilities.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># TODO(jiaye): support dropout</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>

      <span class="c1"># [b, hidden_dim]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
      <span class="c1"># [sl, b]</span>
      <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates atten probs with padding.&quot;&quot;&quot;</span>
        <span class="c1"># tf.tanh(x+y) shape [sl, b, hidden_dim]</span>
        <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">inputs</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
        <span class="c1"># [-1, hidden_dim] * [hidden_dim, 1] = [-1, 1]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Reshape res to [sl, b]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Take out the padding states. _source_padding is of shape [sl, b].</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">source_padding</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">per_step_source_padding</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
          <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">inputs</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">inputs</span><span class="o">.</span><span class="n">query_segment_id</span><span class="p">)</span>
          <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># [b, sl]</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="c1"># softmax to compute the probabilities. [b, sl]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs</span>

      <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">AttenProbs</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">x</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
              <span class="n">y</span><span class="o">=</span><span class="n">query_vec</span><span class="p">,</span>
              <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="o">=</span><span class="n">per_step_source_padding</span><span class="p">,</span>
              <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_segment_id</span><span class="p">))</span>
      <span class="n">probs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="c1"># contexts[i, :] is a weighted (probs[i, :]) average of</span>
      <span class="c1"># concated_source_vecs[i, :, :].</span>
      <span class="c1"># Reshaped probs is of shape [b, 1, sl]</span>
      <span class="n">reshaped_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="c1"># [b, 1, sl] * [b, sl, context_dim] = [b, 1, context_dim]</span>
      <span class="n">contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">reshaped_probs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># Reshaped context is of shape [b, context_dim]</span>
      <span class="n">contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">contexts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">probs</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">AttenSameBatchSize</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Prepares source vec and ctx.&quot;&quot;&quot;</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="n">src_w</span><span class="p">)</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
          <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;source_vecs_projected&#39;</span><span class="p">)</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">transposed_ctxs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;source_ctx&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="AdditiveAttention._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;AdditiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdditiveAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors.</span>

<span class="sd">    Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      source_vecs: A single tensor of shape [time, batch_size, source_dim].</span>
<span class="sd">      source_contexts: A single tensor of shape [time, batch_size, some_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, batch_size].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, batch_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A NestedMap containing the packed source.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>

      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="c1"># [time, batch_size, hidden_dim].</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="c1"># [batch_size, time, context_dim].</span>
        <span class="c1"># Note the mismatch between `source_vecs` and `source_contexts`. In</span>
        <span class="c1"># `source_vecs`, time is the first dim, while it is the second dim in</span>
        <span class="c1"># `source_contexts`.</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdditiveAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># This is just a dummy state. The first dimension of the state has to match</span>
    <span class="c1"># decoder_batch_size.</span>
    <span class="n">zs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">zs</span></div>

<div class="viewcode-block" id="AdditiveAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.AdditiveAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Note: `packed_src.source_vecs` are the vectors that are used to compute the</span>
<span class="sd">    attention score between the `query_vec` and each `packed_src.source_vecs`.</span>
<span class="sd">    The `packed_src.source_contexts` are the vectors that compose the result.</span>
<span class="sd">    The attention context vector is computed as a weighted average of the</span>
<span class="sd">    `packed_src.source_contexts`, using the scores that were computed using</span>
<span class="sd">    `packed_src.source_vecs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">        `AdditiveAttention`, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_length</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_length</span><span class="p">])</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
                                  <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                                  <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
                                  <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">attention_state</span></div></div>


<div class="viewcode-block" id="DotProductAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention">[docs]</a><span class="k">class</span> <span class="nc">DotProductAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements dot-product attention (also known as &quot;Luong Attention&quot;).</span>

<span class="sd">  Described in:</span>

<span class="sd">  Minh-Thang Luong, Hieu Pham, Christopher D. Manning.</span>
<span class="sd">  &quot;Effective Approaches to Attention-based Neural Machine Translation.&quot;</span>
<span class="sd">  EMNLP 2015.</span>
<span class="sd">  https://arxiv.org/abs/1508.04025</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="DotProductAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `DotProductAttention`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether or not to use per_dim_scale to scale &#39;</span>
        <span class="s1">&#39;the individual dims when calculating attention probabilities. It can &#39;</span>
        <span class="s1">&#39;increase training stability when set to False.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a DotProductAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># TODO(yonghui): relax these constraints.</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

    <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">      target_batch = source_batch * n where n is an integer &gt;= 1.</span>
<span class="sd">      In this case inputs.query_vec contains:</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">           0  |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">           1  |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">                           ...</span>
<span class="sd">              -------------------------</span>
<span class="sd">              | instance    1         |</span>
<span class="sd">              | instance    2         |</span>
<span class="sd">          n-1 |          ...          |</span>
<span class="sd">              | instance source_batch |</span>
<span class="sd">              -------------------------</span>
<span class="sd">      One use case is beam search where n = beam size.</span>

<span class="sd">      Args:</span>
<span class="sd">        inputs: a NestedMap containing:</span>
<span class="sd">          - per_dim_scale:         [source_dim], a vec to scale individual dims.</span>
<span class="sd">          - source_padding:          [time, source_batch].</span>
<span class="sd">          - concated_source_vecs:    [source_batch, time, source_dim].</span>
<span class="sd">          - query_vec:               [target_batch, source_dim].</span>
<span class="sd">          - per_step_source_padding: [target_batch, source_length]</span>
<span class="sd">          - source_segment_id:       [time, source_batch].</span>
<span class="sd">          - query_segment_id:        [target_batch].</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits [target_batch, source_time].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">concated_source_vecs</span>

      <span class="n">logit_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
                  <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))))</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">query_vec</span> <span class="o">*</span> <span class="n">inputs</span><span class="o">.</span><span class="n">per_dim_scale</span>
      <span class="c1"># The n here refers to the &quot;n&quot; described in the comment above.</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, source_dim, n]</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="c1"># =&gt; [n, source_batch, source_sequence_len]</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">per_step_source_padding</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, source_sequence_len, n]</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="c1"># Dot-product part.</span>
      <span class="c1"># Calls batch_mat_mul since dim &gt; 2 for per-instance matmul.</span>
      <span class="c1"># [source_batch, time, source_dim] * [source_batch, source_dim, n]</span>
      <span class="c1"># =&gt; [source_batch, time, n]</span>
      <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtActActInputs</span><span class="p">(</span>
          <span class="n">act_lhs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
          <span class="n">act_rhs</span><span class="o">=</span><span class="n">query_vec</span><span class="p">,</span>
          <span class="n">act_lhs_distribution</span><span class="o">=</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">InputDistribution</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
          <span class="n">act_rhs_distribution</span><span class="o">=</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">InputDistribution</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtActActMatmul</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

      <span class="n">logits</span> <span class="o">*=</span> <span class="n">logit_scale</span>
      <span class="c1"># Exclude padding frames.</span>
      <span class="c1"># [source_batch, time] =&gt; [source_batch, time, 1]</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UpdatePaddingWithPackedInputMask</span><span class="p">(</span>
            <span class="n">source_padding</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">query_segment_id</span><span class="p">)</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

      <span class="c1"># =&gt; [n, source_batch, time]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

      <span class="c1"># =&gt; [n * source_batch, time].</span>
      <span class="c1"># This makes logits store content in the same order as query_vec.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">probs</span>

    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">      Args:</span>
<span class="sd">        per_dim_scale:            [source_dim], a vec to scale individual dims.</span>
<span class="sd">        source_padding:           [time, source_batch].</span>
<span class="sd">        source_segment_id:        [time, source_batch].</span>
<span class="sd">        concated_source_vecs:     [time, source_batch, source_dim].</span>
<span class="sd">        concated_source_contexts: [source_batch, time, context_dim].</span>
<span class="sd">        query_vec:                [target_batch, source_dim].</span>
<span class="sd">        query_segment_id:         [target_batch].</span>
<span class="sd">        per_step_source_padding:  [target_batch, source_length]</span>
<span class="sd">      Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">        attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">        concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">        attention context vector is computed as a weighted average of the</span>
<span class="sd">        concated_source_contexts, using the scores that were computed using</span>
<span class="sd">        concated_source_vecs.</span>

<span class="sd">      Returns:</span>
<span class="sd">        Two tensors:</span>

<span class="sd">        - context_vector: [target_batch, context_dim].</span>
<span class="sd">        - probs:          [target_batch, time].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]],</span>
                                  <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span>
      <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]],</span>
                                  <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">)])</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
          <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concated_source_vecs&#39;</span><span class="p">)</span>
      <span class="n">returned_probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">AttenProbs</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">per_dim_scale</span><span class="o">=</span><span class="n">per_dim_scale</span><span class="p">,</span>
              <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
              <span class="n">concated_source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">query_vec</span><span class="o">=</span><span class="n">query_vec</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="o">=</span><span class="n">per_step_source_padding</span><span class="p">,</span>
              <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">,</span>
              <span class="n">query_segment_id</span><span class="o">=</span><span class="n">query_segment_id</span><span class="p">))</span>
      <span class="n">returned_probs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="c1"># =&gt; [n, source_batch, time].</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">returned_probs</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># =&gt; [source_batch, n, time].</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="c1"># Apply dropout to weights if applicable.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_ApplyAttentionDropout</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

      <span class="c1"># Weight each frame with the probability and sum them.</span>
      <span class="c1"># [source_batch, n, time] * [source_batch, time, context_dim]</span>
      <span class="c1"># =&gt; [source_batch, n, context_dim].</span>
      <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
          <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concated_source_contexts&#39;</span><span class="p">)</span>
      <span class="n">probs</span><span class="p">,</span> <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtActActInputs</span><span class="p">(</span>
          <span class="n">act_lhs</span><span class="o">=</span><span class="n">probs</span><span class="p">,</span>
          <span class="n">act_rhs</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
          <span class="n">act_lhs_distribution</span><span class="o">=</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">InputDistribution</span><span class="o">.</span><span class="n">POSITIVE</span><span class="p">,</span>
          <span class="n">act_rhs_distribution</span><span class="o">=</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">InputDistribution</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">)</span>

      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtActActMatmul</span><span class="p">(</span><span class="n">context_vector</span><span class="p">)</span>

      <span class="c1"># =&gt; [n, source_batch, context_dim].</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># =&gt; [n * source_batch, context_dim].</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">context_vector</span><span class="p">,</span> <span class="n">returned_probs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

<div class="viewcode-block" id="DotProductAttention._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_dim_scale</span><span class="p">:</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;DotProductAttention_vars&#39;</span><span class="p">])</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="DotProductAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors.</span>

<span class="sd">    Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (concated_source_vecs, concated_source_contexts, source_padding)</span>
<span class="sd">      where `concated_source_vecs` is a tensor of shape [time, batch_size,</span>
<span class="sd">      hidden_dim], `concated_source_contexts` is a tensor of shape</span>
<span class="sd">      [batch_size, time, some_dim] and `source_padding` is a tensor of shape</span>
<span class="sd">      [time, batch_size].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="c1"># [time, batch_size, hidden_dim].</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="c1"># [batch_size, time, context_dim].</span>
        <span class="c1"># Note the mismatch between `source_vecs` and `source_contexts`. In</span>
        <span class="c1"># `source_vecs`, time is the first dim, while it is the second dim in</span>
        <span class="c1"># `source_contexts`.</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="DotProductAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># No states to keep track of currently.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span></div>

<div class="viewcode-block" id="DotProductAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.DotProductAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim], where target_batch</span>
<span class="sd">        = n * source_batch (e.g., n = num_hyps_per_beam in beamsearch). Along</span>
<span class="sd">        the target_batch dimension, there are n groups of consecutive rows, each</span>
<span class="sd">        group containing source_batch rows.</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">        AdditiveAttention, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch, source_length].</span>
<span class="sd">      query_segment_id: Query segment id with shape [target_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors</span>
<span class="sd">        with dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span>

    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_sequence_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span>
          <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_sequence_length</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_sequence_length</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ScaleFn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">use_dim_scale</span><span class="p">:</span>
      <span class="n">per_dim_scale_var</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">per_dim_scale_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span>
        <span class="n">ScaleFn</span><span class="p">(</span><span class="n">per_dim_scale_var</span><span class="p">),</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">,</span>
        <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
        <span class="n">query_segment_id</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">attention_state</span></div></div>


<div class="viewcode-block" id="_RecursiveReshape"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention._RecursiveReshape">[docs]</a><span class="k">def</span> <span class="nf">_RecursiveReshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MultiHeadedAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">,</span> <span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Attention with multiple attention heads.</span>

<span class="sd">  Conceptually, the algorithm works as follows:</span>

<span class="sd">  1. Source vectors (attention keys) are first projected to vectors of dim</span>
<span class="sd">     p.hidden_dim.</span>
<span class="sd">  2. Query vectors are projected to vectors of dim p.hidden_dim as well.</span>
<span class="sd">  3. Context vectors (attention values) are not projected by default, unless</span>
<span class="sd">     `enable_ctx_pre_proj` is True.</span>
<span class="sd">  4. Source vectors, query vectors and context vectors are all split into</span>
<span class="sd">     p.num_attention_heads chunks.</span>
<span class="sd">  5. The inner atten mechanism is computed separately on each of the chunks.</span>
<span class="sd">  6. Attention contexts from each of the chunk are concatenated to form the</span>
<span class="sd">     final context.</span>
<span class="sd">  7. Attention probs from each of the chunk are averaged to form the final</span>
<span class="sd">     attention prob.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for MultiHeadedAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;context_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of context nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_attention_heads&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_source_vec_as_attention_value&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;Whether or not to use source_vec as the attention value as well.&#39;</span>
        <span class="s1">&#39; If True, we expect source_vec and source_contexts are the same.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_source_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If False, source side linear projection is disabled.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_query_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If False, query side linear projection is disabled.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;inner_atten_params&#39;</span><span class="p">,</span> <span class="n">DotProductAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Params for underlying attention mechanism.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_ctx_pre_proj&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, context is pre-projected before processing into&#39;</span>
        <span class="s1">&#39; hidden_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_ctx_post_proj&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, computed context is post projected into&#39;</span>
        <span class="s1">&#39; ctx_post_proj_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of post projection nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_post_proj&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Number of post projections, usually the same as &#39;</span>
        <span class="s1">&#39;number of tasks. Each task may choose to use one of the post &#39;</span>
        <span class="s1">&#39;projection layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;proj_init&#39;</span><span class="p">,</span> <span class="s1">&#39;default&#39;</span><span class="p">,</span> <span class="s1">&#39;Initialization approach for projection &#39;</span>
        <span class="s1">&#39;layers:&#39;</span>
        <span class="s1">&#39;uniform: Use uniform initialization. &#39;</span>
        <span class="s1">&#39;default: Use the default Xavier initialization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;attention_head_prob_index&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;If &gt; 0, instead of averaging &#39;</span>
        <span class="s1">&#39;the probabilities of all attention heads when returning the &#39;</span>
        <span class="s1">&#39;attention probability, instead return the selected index prob.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use bias for projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_per_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use per_dim_scale in inner_atten.&#39;</span><span class="p">)</span>

    <span class="c1"># Often the attention context output needs to be concated</span>
    <span class="c1"># with tensors from another layer. This allows them to share</span>
    <span class="c1"># quantization parameters. By convention, all attention layers</span>
    <span class="c1"># need to include their context output vectors in this domain.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="s1">&#39;Quantization domain for attention context.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> mod </span><span class="si">%s</span><span class="s1"> != 0&#39;</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_init</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;default&#39;</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown proj_init: </span><span class="si">%s</span><span class="s1">!&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_init</span><span class="p">)</span>

    <span class="n">att_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>

    <span class="n">att_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">inner_atten_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">source_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
        <span class="n">query_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">att_dim</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">atten_dropout_deterministic</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_deterministic</span><span class="p">,</span>
        <span class="n">packed_input</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">att_p</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">DotProductAttention</span><span class="p">:</span>
      <span class="n">att_p</span><span class="o">.</span><span class="n">use_dim_scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">att_p</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="n">att_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;inner_att&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">att_p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_head_prob_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_head_prob_index</span> <span class="o">&lt;</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
        <span class="s1">&#39;query_proj_aqt&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
        <span class="s1">&#39;source_proj_aqt&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
        <span class="s1">&#39;ctx_pre_proj_aqt&#39;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
        <span class="s1">&#39;ctx_post_proj_aqt&#39;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">],</span>
        <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttention._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">InitProj</span><span class="p">(</span><span class="n">layer_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_init</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
        <span class="c1"># Note we also initialize bias with uniform distribution here, following</span>
        <span class="c1"># the default Pytorch implementation:</span>
        <span class="c1"># https://pytorch.org/docs/stable/nn.html#linear</span>
        <span class="n">proj_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">layer_dim</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_init</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
        <span class="n">proj_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="n">p</span><span class="o">.</span><span class="n">params_init</span>
      <span class="k">return</span> <span class="n">proj_init</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_pre_proj</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">context_dim</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">context_dim</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">pc_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">]</span>
        <span class="n">pc_b_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">]</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">pc_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span><span class="p">]</span>
        <span class="n">pc_b_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_post_proj must &gt; 0!&#39;</span><span class="p">)</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">pc_shape</span><span class="p">,</span>
          <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="n">pc_bias_post_proj</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">pc_b_shape</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">InitProj</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj_b&#39;</span><span class="p">,</span> <span class="n">pc_bias_post_proj</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;source_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;source_proj_add&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;query_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;query_proj_add&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;ctx_pre_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;ctx_pre_proj_add&#39;</span><span class="p">)</span>
    <span class="c1"># TODO(suderman): Remove the self.do_eval check below once brop quant within</span>
    <span class="c1"># defun is fixed on the training side. This is less than ideal as-is because</span>
    <span class="c1"># training will just trend to match downstream quant constraints vs force</span>
    <span class="c1"># alignment.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span>
        <span class="s1">&#39;ctx_post_proj_matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;ctx_post_proj_add&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;atten_context&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.SetOutputContextDim"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.SetOutputContextDim">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetOutputContextDim</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">ctx_post_proj_dim</span> <span class="o">=</span> <span class="n">out_dim</span></div>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/PackSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Packs source vectors.</span>

<span class="sd">    Does not change attention state.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      source_vecs: A tensor of shape [time, source_batch, source_dim].</span>
<span class="sd">      source_contexts: A tensor of shape [time, source_batch, context_dim].</span>
<span class="sd">      source_padding: A tensor of shape [time, source_batch].</span>
<span class="sd">      source_segment_id: A tensor of shape [time, source_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A NestedMap representing packed src. It will have the same structure</span>
<span class="sd">      as the one returned by the inner atten, except that source_batch will be</span>
<span class="sd">      source_batch * num_heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
        <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span>
                                        <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">))</span>
      <span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="c1"># source_projected shape [time * source_batch, hidden]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0a&#39;</span><span class="p">):</span>
        <span class="n">source_vec_depth</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__0b&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_source_proj</span><span class="p">:</span>
          <span class="n">w_source_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span>
              <span class="s1">&#39;source_proj_aqt&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">source_proj</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
          <span class="n">w_source_proj</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">w_source_proj</span><span class="p">)</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="p">(</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_vec_depth</span><span class="p">]),</span>
                  <span class="n">w_source_proj</span><span class="p">,</span>
                  <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;source_proj_matmul&#39;</span><span class="p">))</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;source_proj_aqt&#39;</span><span class="p">,</span>
                                                <span class="n">source_projected</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="n">source_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
                <span class="n">source_projected</span><span class="p">,</span>
                <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_proj_b</span><span class="p">),</span>
                <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;source_proj_add&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">source_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_vec_depth</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__1&#39;</span><span class="p">):</span>
      <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
      <span class="c1"># =&gt; [time, source_batch * num_heads, hidden / num_heads]</span>
      <span class="n">source_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_projected</span><span class="p">,</span> <span class="p">[</span>
          <span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span>
          <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">)</span>
      <span class="p">])</span>
      <span class="n">source_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ProcessProjectionVec</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">source_projected</span><span class="p">,</span>
                                                   <span class="s1">&#39;source&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_source_vec_as_attention_value</span><span class="p">:</span>
        <span class="n">source_contexts_reshaped</span> <span class="o">=</span> <span class="n">source_projected</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_pre_proj</span><span class="p">:</span>
          <span class="n">w_ctx_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span>
              <span class="s1">&#39;ctx_pre_proj_aqt&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_proj</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
          <span class="n">w_ctx_proj</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">w_ctx_proj</span><span class="p">)</span>

          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">,</span>
                         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_contexts</span><span class="p">)[</span><span class="mi">2</span><span class="p">]]),</span>
              <span class="n">w_ctx_proj</span><span class="p">,</span>
              <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_pre_proj_matmul&#39;</span><span class="p">)</span>
          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span>
              <span class="s1">&#39;ctx_pre_proj_aqt&#39;</span><span class="p">,</span> <span class="n">source_contexts_projected</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
                <span class="n">source_contexts_projected</span><span class="p">,</span>
                <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_proj_b</span><span class="p">),</span>
                <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_pre_proj_add&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="n">source_contexts</span>

        <span class="n">source_context_depth</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_contexts_projected</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">source_contexts_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_contexts_projected</span><span class="p">,</span> <span class="p">[</span>
            <span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">source_context_depth</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="p">])</span>
        <span class="n">source_contexts_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ProcessProjectionVec</span><span class="p">(</span>
            <span class="n">theta</span><span class="p">,</span> <span class="n">source_contexts_projected</span><span class="p">,</span> <span class="s1">&#39;ctx&#39;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;init__2&#39;</span><span class="p">):</span>
      <span class="n">source_padding_replicated</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding_replicated</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">source_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>

      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">PackSource</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">source_projected</span><span class="p">,</span>
                                   <span class="n">source_contexts_reshaped</span><span class="p">,</span>
                                   <span class="n">source_padding_replicated</span><span class="p">,</span>
                                   <span class="n">source_segment_id_repl</span><span class="p">)</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/ExtendSourcePacked&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ExtendSourcePacked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">theta</span><span class="p">,</span>
                         <span class="n">new_source_vecs</span><span class="p">,</span>
                         <span class="n">new_source_contexts</span><span class="p">,</span>
                         <span class="n">new_source_paddings</span><span class="p">,</span>
                         <span class="n">new_source_segment_ids</span><span class="p">,</span>
                         <span class="n">cached_packed_src</span><span class="p">,</span>
                         <span class="n">t</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extend cached source_vecs and source_contexts by one more timestep.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      new_source_vecs: A tensor of shape [source_batch, source_dim].</span>
<span class="sd">      new_source_contexts: A tensor of shape [source_batch, context_dim].</span>
<span class="sd">        new_source_vecs and new_source_contexts are source_vecs and</span>
<span class="sd">        source_contexts for the new timestep to be extended.</span>
<span class="sd">      new_source_paddings: If not None, a tensor of shape [source_batch].</span>
<span class="sd">        source_padding for the new timestep.</span>
<span class="sd">      new_source_segment_ids: If not None, a tensor of shape [source_batch].</span>
<span class="sd">        source_segment_id for the new timestep.</span>
<span class="sd">      cached_packed_src: a `.NestedMap` object, containing already preprocessed</span>
<span class="sd">        source_vecs and source_contexts for the previous t-1 steps. To support</span>
<span class="sd">        tf.while_loop on TPU (satisfying static shape requirement), instead of</span>
<span class="sd">        using tf.concat to update the cached vectors, the time dimension of each</span>
<span class="sd">        cached vector is fixed as the max_sequence_length and inplace</span>
<span class="sd">        update op is used to update the information for each time step:</span>
<span class="sd">        * source_vecs: A tensor of shape [max_sequence_length, source_batch,</span>
<span class="sd">          hidden_dim]. [:t, :, :] contains valid preprocessed source_vecs in the</span>
<span class="sd">            previous t - 1 timesteps, the rests are invalid data.</span>
<span class="sd">        * source_contexts: A tensor of shape [max_sequence_length, source_batch,</span>
<span class="sd">          hidden_dim]. [:t, :, :] contains valid preprocessed source_contexts in</span>
<span class="sd">            the previous t - 1 timesteps, the rests are invalid data.</span>
<span class="sd">        * source_padding: If not None, a tensor of shape [max_sequence_length,</span>
<span class="sd">          source_batch, num_heads]. [:t, :, :] contains cached source padding</span>
<span class="sd">            for the previous t - 1 timesteps, the rests are invalid data.</span>
<span class="sd">        * source_segment_id: If not None, a tensor of shape</span>
<span class="sd">          [max_sequence_length, source_batch, num_heads]. [:t, :, :] contains</span>
<span class="sd">            cached source segment id for the previous t - 1 timesteps, the rests</span>
<span class="sd">            are invalid data.</span>
<span class="sd">        When t is None (not running on TPU or the while loop is unrolled):</span>
<span class="sd">        * source_vecs: A tensor of shape [t - 1, source_batch, hidden_dim].</span>
<span class="sd">        * source_contexts: A tensor of shape [t - 1, source_batch, hidden_dim].</span>
<span class="sd">        * source_padding: If not None, a tensor of shape [t - 1, source_batch,</span>
<span class="sd">          num_heads], cached source padding for the previous t - 1 timesteps.</span>
<span class="sd">        * source_segment_id: If not None, a tensor of shape [t - 1,</span>
<span class="sd">          source_batch, num_heads], cached source segment id for the previous t</span>
<span class="sd">          - 1 timesteps.</span>
<span class="sd">      t: a scalar, the current time step, 0-based.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Extended cached source_vecs, source_contexts, source_paddings, and</span>
<span class="sd">      source_segment_ids. The time dimension of each cached state is fixed:</span>
<span class="sd">      &#39;extended_source_vec&#39; is of shape [max_sequence_length, batch_size,</span>
<span class="sd">      num_heads * dim];</span>
<span class="sd">      &#39;extended_source_context&#39; is of shape [max_sequence_length, batch_size,</span>
<span class="sd">      num_heads * dim];</span>
<span class="sd">      &#39;source_padding&#39; is of shape [max_sequence_length, batch_size, num_heads];</span>
<span class="sd">      &#39;source_segment_id&#39; is of shape [max_sequence_length, batch_size,</span>
<span class="sd">      num_heads].</span>
<span class="sd">      But only [:(t + 1), :, :] contains valid data.</span>
<span class="sd">      If t is not given,</span>
<span class="sd">      &#39;extended_source_vec&#39; is of shape [t, batch_size, num_heads * dim];</span>
<span class="sd">      &#39;extended_source_context&#39; is of shape [t, batch_size, num_heads * dim];</span>
<span class="sd">      &#39;source_padding&#39; is of shape [t, batch_size, num_heads];</span>
<span class="sd">      &#39;source_segment_id&#39; is of shape [t, batch_size, num_heads].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">new_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">new_source_paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_source_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">new_source_vecs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_source_segment_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_source_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">],</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">new_source_vecs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">processed_packed_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_vecs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_contexts</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_paddings</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_source_segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">extended_packed_src</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;source_vecs&#39;</span><span class="p">,</span> <span class="s1">&#39;source_contexts&#39;</span><span class="p">,</span> <span class="s1">&#39;source_padding&#39;</span><span class="p">,</span>
                <span class="s1">&#39;source_segment_id&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">cached_packed_src</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">extended_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">processed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
          <span class="c1"># Make sure t is a scaler instead of tensors having shape like [1,].</span>
          <span class="c1"># This could happen in cases where function is called by recurrent.py</span>
          <span class="c1"># (for example target_sequence_sampler.)</span>
          <span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[])</span>
          <span class="n">extended_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
              <span class="n">cached_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">t</span><span class="p">,</span> <span class="n">processed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">processed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">processed_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
          <span class="n">extended_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
              <span class="p">[</span><span class="n">cached_packed_src</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">processed</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">extended_packed_src</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span><span class="s1">&#39;MultiHeadedAttention/ZeroAttentionState&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">zero_att_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span>
        <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
    <span class="c1"># [batch * num_heads, length] =&gt; [batch, num_heads * length].</span>
    <span class="n">zero_att_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">zero_att_state</span><span class="p">,</span> <span class="p">[</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">nested_map_zero_att_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">inner</span><span class="o">=</span><span class="n">zero_att_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">attention_head_prob_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">selected_prob_head</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">source_length</span><span class="p">],</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
      <span class="n">nested_map_zero_att_state</span><span class="p">[</span>
          <span class="s1">&#39;selected_attention_head_probs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">selected_prob_head</span>
    <span class="k">return</span> <span class="n">nested_map_zero_att_state</span>

<div class="viewcode-block" id="MultiHeadedAttention.ProcessProjectionVec"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.ProcessProjectionVec">[docs]</a>  <span class="k">def</span> <span class="nf">ProcessProjectionVec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">projection_vec</span><span class="p">,</span> <span class="n">projection_type</span><span class="p">):</span>
    <span class="c1"># no-op for this class but allows subclasses to override to process</span>
    <span class="c1"># projected vectors.</span>
    <span class="k">return</span> <span class="n">projection_vec</span></div>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">atten_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim].</span>
<span class="sd">      attention_state: A NestedMap. &#39;inner&#39; contains the inner attention</span>
<span class="sd">        state. It is not used in AdditiveAttention, and is simply passed</span>
<span class="sd">        through. Optionally, if attention_head_prob_index &gt;= 0, then</span>
<span class="sd">        &#39;selected_attention_head_probs&#39; contains the selected attention</span>
<span class="sd">        probability head.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [target_batch].</span>
<span class="sd">      atten_idx: If not None, then apply a different attention projection for</span>
<span class="sd">        different samples in a batch, each of which may come from different</span>
<span class="sd">        tasks. This is usually used in multi-task setting. A tensor of shape</span>
<span class="sd">        [target_batch].</span>
<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">      concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">      attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The new attention mechanism state: A nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch, ...]. See input &#39;attention_state&#39; for</span>
<span class="sd">        description of items in the nested tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="n">source_seq_len</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">static_inner_atten_dim</span> <span class="o">=</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">query_vec_projected_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">static_inner_atten_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_query_proj</span><span class="p">:</span>
      <span class="n">w_query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span>
          <span class="s1">&#39;query_proj_aqt&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_proj</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">w_query_proj</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">w_query_proj</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="n">w_query_proj</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;query_proj_matmul&#39;</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;query_proj_aqt&#39;</span><span class="p">,</span>
                                               <span class="n">query_vec_projected</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
            <span class="n">query_vec_projected</span><span class="p">,</span>
            <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query_proj_b</span><span class="p">),</span>
            <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;query_proj_add&#39;</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec_projected</span><span class="p">,</span>
                                       <span class="n">query_vec_projected_shape</span><span class="p">)</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ProcessProjectionVec</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span>
                                                      <span class="n">query_vec_projected</span><span class="p">,</span>
                                                      <span class="s1">&#39;query&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query_vec_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">query_vec_projected_shape</span><span class="p">)</span>

    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">query_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">query_batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query_segment_id_repl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_segment_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">])</span>
      <span class="n">query_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_segment_id_repl</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">],</span>
                                        <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">])</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_seq_len</span><span class="p">])</span>
    <span class="n">attention_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span>
                                        <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">):</span>
      <span class="k">if</span> <span class="s1">&#39;emit_probs&#39;</span> <span class="ow">in</span> <span class="n">attention_state</span><span class="p">:</span>
        <span class="n">inner_state</span> <span class="o">=</span> <span class="n">attention_state</span>
      <span class="k">elif</span> <span class="s1">&#39;inner&#39;</span> <span class="ow">in</span> <span class="n">attention_state</span><span class="p">:</span>
        <span class="n">inner_state</span> <span class="o">=</span> <span class="n">attention_state</span><span class="o">.</span><span class="n">inner</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inner_state</span> <span class="o">=</span> <span class="n">attention_state</span>
    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">new_inner_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">packed_src</span><span class="p">,</span> <span class="n">query_vec_projected</span><span class="p">,</span> <span class="n">inner_state</span><span class="p">,</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">atten_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
            <span class="s1">&#39;atten_idx is None, this means there is no need to select &#39;</span>
            <span class="s1">&#39;different post projections, and p.num_post_proj is supposed to be &#39;</span>
            <span class="s1">&#39;1. However you set p.num_post_proj=</span><span class="si">%s</span><span class="s1"> .&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span><span class="p">)</span>
        <span class="n">w_ctx_post_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span>
            <span class="s1">&#39;ctx_post_proj_aqt&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">w_ctx_post_proj</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">w_ctx_post_proj</span><span class="p">)</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
            <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">w_ctx_post_proj</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_post_proj_matmul&#39;</span><span class="p">)</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;ctx_post_proj_aqt&#39;</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
          <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
              <span class="n">ctx_vec</span><span class="p">,</span>
              <span class="n">fns</span><span class="o">.</span><span class="n">qweight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj_b</span><span class="p">),</span>
              <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;ctx_post_proj_add&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
            <span class="s1">&#39;atten_idx is not None, this means there are multiple post &#39;</span>
            <span class="s1">&#39;projections, and p.num_post_proj is supposed to be &gt; 1. However &#39;</span>
            <span class="s1">&#39;you set p.num_post_proj=</span><span class="si">%s</span><span class="s1"> .&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_post_proj</span><span class="p">)</span>
        <span class="n">bs_range</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="n">select</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">bs_range</span><span class="p">,</span> <span class="p">[</span><span class="n">atten_idx</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="c1"># =&gt; [batch, dim, num_langs]</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ab,bcd-&gt;acd&#39;</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
          <span class="n">ctx_vec</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj_b</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># =&gt; [batch, num_langs, dim]</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="c1"># =&gt; [batch, dim]</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="n">select</span><span class="p">)</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ProcessProjectionVec</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="s1">&#39;ctx_post&#39;</span><span class="p">)</span>

    <span class="c1"># explicitly name this tensor for potential future reference</span>
    <span class="n">multi_headed_atten_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">prob</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multi_headed_atten_prob&#39;</span><span class="p">)</span>
    <span class="c1"># TODO(laurenzo): Use a better named range function (we want to represent</span>
    <span class="c1"># 0..1 probs).</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRSoftmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">multi_headed_atten_prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">):</span>
      <span class="n">att_state</span> <span class="o">=</span> <span class="n">attention_state</span>
      <span class="k">if</span> <span class="s1">&#39;emit_probs&#39;</span> <span class="ow">in</span> <span class="n">attention_state</span><span class="p">:</span>
        <span class="n">att_state</span> <span class="o">=</span> <span class="n">new_inner_state</span>
      <span class="k">elif</span> <span class="s1">&#39;inner&#39;</span> <span class="ow">in</span> <span class="n">attention_state</span><span class="p">:</span>
        <span class="n">att_state</span><span class="o">.</span><span class="n">inner</span> <span class="o">=</span> <span class="n">new_inner_state</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">att_state</span> <span class="o">=</span> <span class="n">new_inner_state</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_head_prob_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">selected_prob_head</span> <span class="o">=</span> <span class="n">multi_headed_atten_prob</span><span class="p">[:,</span> <span class="n">p</span><span class="o">.</span>
                                                   <span class="n">attention_head_prob_index</span><span class="p">,</span> <span class="p">:]</span>
      <span class="n">att_state</span><span class="o">.</span><span class="n">selected_attention_head_probs</span> <span class="o">=</span> <span class="n">selected_prob_head</span>
    <span class="n">att_state</span> <span class="o">=</span> <span class="n">_RecursiveReshape</span><span class="p">(</span><span class="n">att_state</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">att_state</span>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithAttenProbs&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithAttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">packed_context</span><span class="p">,</span>
                                         <span class="n">atten_probs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the attention probailities.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_context: Concated source contexts with shape [ batch_size *</span>
<span class="sd">        num_heads, time, context_dim // num_heads].</span>
<span class="sd">      atten_probs: The attention probability vector: [batch_size * num_heads,</span>
<span class="sd">        time].</span>

<span class="sd">    Returns:</span>
<span class="sd">      The attention context vector shaped [target_batch, source_dim].</span>
<span class="sd">      If p.enable_ctx_post_proj is false, source_dim = context_dim,</span>
<span class="sd">      otherwise, source_dim = p.ctx_post_proj_dim.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="c1"># packed_context: [batch_size * num_head, num_style,</span>
    <span class="c1"># hidden_dim / num_head]</span>
    <span class="c1"># inp: [batch_size * num_head, num_style]</span>
    <span class="n">packed_context</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">packed_context</span><span class="p">)[</span><span class="mi">0</span><span class="p">]],</span>
                                    <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="p">],</span> <span class="n">packed_context</span><span class="p">)</span>
    <span class="n">b_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">packed_context</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">packed_context</span><span class="p">),</span> <span class="p">[</span><span class="n">b_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_ctx_post_proj</span><span class="p">:</span>
      <span class="n">ctx_vec_proj</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj</span><span class="p">)</span>
      <span class="n">ctx_vec_proj</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">ctx_post_proj_b</span>
      <span class="n">ctx_vec_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ProcessProjectionVec</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ctx_vec_proj</span><span class="p">,</span> <span class="s1">&#39;ctx_post&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ctx_vec_proj</span> <span class="o">=</span> <span class="n">ctx_vec</span>
    <span class="k">return</span> <span class="n">ctx_vec_proj</span><span class="p">,</span> <span class="n">ctx_vec</span>

<div class="viewcode-block" id="MultiHeadedAttention.PackCachedSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiHeadedAttention.PackCachedSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackCachedSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cached_src</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">cached_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">cached_src</span><span class="o">.</span><span class="n">source_contexts</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">cached_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">cached_src</span><span class="o">.</span><span class="n">source_segment_id</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">src_seq_len</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="n">packed_src</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># TODO(yonghui): Rewrite the following with just one transpose.</span>
    <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">concated_source_contexts</span><span class="p">,</span>
                   <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">source_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">packed_src</span><span class="o">.</span><span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">source_segment_id</span><span class="p">,</span> <span class="p">[</span><span class="n">src_seq_len</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">packed_src</span></div>

  <span class="nd">@py_utils</span><span class="o">.</span><span class="n">NameScopeDecorator</span><span class="p">(</span>
      <span class="s1">&#39;MultiHeadedAttention/ComputeContextVectorWithCachedSource&#39;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">ComputeContextVectorWithCachedSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                           <span class="n">theta</span><span class="p">,</span>
                                           <span class="n">cached_src</span><span class="p">,</span>
                                           <span class="n">query_vec</span><span class="p">,</span>
                                           <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                           <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                           <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Same as the ComputeContextVectorWithSource api above, except values ...</span>

<span class="sd">    in source_vecs, source_contexts and source_padding are ordered differently.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      cached_src: A `.NestedMap` object returned by ExtendSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim].</span>
<span class="sd">      attention_state: previous attention state. It is not used in</span>
<span class="sd">        AdditiveAttention, and is simply passed through.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [target_batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 tensors:</span>

<span class="sd">      - The attention context vector: [target_batch, source_dim]</span>
<span class="sd">      - The attention probability vector: [target_batch, time]</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch....]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">PackCachedSource</span><span class="p">(</span><span class="n">cached_src</span><span class="p">),</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">)</span></div>


<div class="viewcode-block" id="LocationSensitiveAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention">[docs]</a><span class="k">class</span> <span class="nc">LocationSensitiveAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An attention that also takes into account previously attended locations.</span>

<span class="sd">  See section 2.2 of this paper for a description of this technique:</span>
<span class="sd">  http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocationSensitiveAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this LocationSensitiveAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;location_filter_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Location filter size, should be an odd number e.g. 31.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;location_num_filters&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of location filters, e.g. 32.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;same_batch_size&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True iff the source and target sequence has the same batch size.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;location_features&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;PREV_PROBS&#39;</span><span class="p">],</span>
        <span class="s1">&#39;List signals to run the convolutions on. Possible options are: &#39;</span>
        <span class="s1">&#39;PREV_PROBS, CUMULATIVE_PROBS.&#39;</span><span class="p">)</span>

    <span class="c1"># Often the attention context output needs to be concated</span>
    <span class="c1"># with tensors from another layer. This allows them to share</span>
    <span class="c1"># quantization parameters. By convention, all attention layers</span>
    <span class="c1"># need to include their context output vectors in this domain.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="s1">&#39;Quantization domain for attention context.&#39;</span><span class="p">)</span>

    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">GaussianSqrtDim</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an LocationSensitiveAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_quantized</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input is not supported yet for &#39;</span>
                                <span class="s1">&#39;LocationSensitiveAttention.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">AttenLogits</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates logits.&quot;&quot;&quot;</span>
      <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>

      <span class="k">def</span> <span class="nf">CollapseOutDim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="c1"># =&gt; [sl, sb, hd]</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">location_feats</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">CollapseOutDim</span><span class="p">(</span><span class="n">location_feats</span><span class="p">),</span> <span class="n">inputs</span><span class="o">.</span><span class="n">location_var</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_mul&#39;</span><span class="p">)</span>

      <span class="n">sl</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">hd</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">location_var</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">sl</span><span class="p">,</span> <span class="n">tb</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>
      <span class="n">sb</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">query_vec_reshaped</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
      <span class="n">bs_mult</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">query_vec_reshaped</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">sl</span><span class="p">,</span> <span class="n">bs_mult</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>

      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
          <span class="n">inputs</span><span class="o">.</span><span class="n">concated_source_vecs</span><span class="p">,</span>
          <span class="n">inputs</span><span class="o">.</span><span class="n">query_vec_reshaped</span><span class="p">,</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_add&#39;</span><span class="p">)</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="n">location_hidden</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_bias&#39;</span><span class="p">)</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qtanh</span><span class="p">(</span><span class="n">summed</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits&#39;</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">AttenLogitsSameBatchSize</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Generates logits.</span>

<span class="sd">      Optimized code path for when the target and the source have the same batch</span>
<span class="sd">      size.</span>

<span class="sd">      Args:</span>
<span class="sd">        inputs: a NestedMap containing:</span>
<span class="sd">          - concated_source_vecs: Tensor of shape [sl, batch, dim]</span>
<span class="sd">          - query_vec_transformed: Tensor of shape [batch, dim]</span>
<span class="sd">          - hidden_v: Tensor of shape [dim]</span>
<span class="sd">          - location_feats: Tensor of shape [batch, location_feature_dim, sl]</span>
<span class="sd">          - location_var: Tensor of shape [location_feature_dim, dim]</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits in the shape [sl, batch_size].</span>
<span class="sd">      &quot;&quot;&quot;</span>

      <span class="k">def</span> <span class="nf">CollapseOutDim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
      <span class="c1"># =&gt; [sl, sb, hd]</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">location_feats</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">CollapseOutDim</span><span class="p">(</span><span class="n">location_feats</span><span class="p">),</span> <span class="n">inputs</span><span class="o">.</span><span class="n">location_var</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_mul&#39;</span><span class="p">)</span>
      <span class="n">sl</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">location_feats</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">hd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">location_var</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">location_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">location_hidden</span><span class="p">,</span> <span class="p">[</span><span class="n">sl</span><span class="p">,</span> <span class="n">tb</span><span class="p">,</span> <span class="n">hd</span><span class="p">])</span>

      <span class="c1"># Shape of summed is [sl, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span>
          <span class="n">inputs</span><span class="o">.</span><span class="n">concated_source_vecs</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">query_vec_transformed</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_add&#39;</span><span class="p">)</span>

      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qadd</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="n">location_hidden</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits_bias&#39;</span><span class="p">)</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qtanh</span><span class="p">(</span><span class="n">summed</span><span class="p">)</span>

      <span class="c1"># logits is of shape [sl * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;logits&#39;</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">summed</span><span class="p">)[:</span><span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span>
              <span class="n">location_filter_var</span><span class="p">,</span> <span class="n">location_var</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.&quot;&quot;&quot;</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
      <span class="c1"># attention_state shape [batch, len(p.location_features), slen]</span>
      <span class="c1"># it contains previous and accumulated attention probabilites.</span>
      <span class="n">attention_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span>
                                          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

      <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyConv</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">)</span>

      <span class="c1"># concated_source_vecs is of shape [sl, sb, dims]</span>
      <span class="c1"># concated_source_contexts is of shape [sb, sl, context_dim]</span>
      <span class="c1"># query_vec is of shape [tb, dims]</span>
      <span class="n">sb</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">tb</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>
      <span class="c1"># concated_source_vecs is reshaped to [sl, 1, sb, hidden_dims]</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;atten_matmul&#39;</span><span class="p">)</span>
      <span class="c1"># query_vec is reshaped to [1, tb/sb, sb, hidden_dims].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec_transformed</span><span class="p">,</span>
                                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>
      <span class="c1"># logits is of shape [sl, tb/sb, sb]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">_ConditionalCallDefun</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_is_quantized</span><span class="p">,</span> <span class="n">AttenLogits</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">concated_source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">query_vec_reshaped</span><span class="o">=</span><span class="n">query_vec_reshaped</span><span class="p">,</span>
              <span class="n">hidden_v</span><span class="o">=</span><span class="n">hidden_var</span><span class="p">,</span>
              <span class="n">location_feats</span><span class="o">=</span><span class="n">location_feats</span><span class="p">,</span>
              <span class="n">location_var</span><span class="o">=</span><span class="n">location_var</span><span class="p">))</span>
      <span class="c1"># Take out the padding states.</span>
      <span class="c1"># _source_padding is of shape [sl, sb].</span>
      <span class="c1"># reshaped to [sl, 1,  sb].</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">])</span>

      <span class="n">source_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRPadding</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">))</span>

      <span class="c1"># Reshape logits to a matrix of shape [tb, sl] and takes the</span>
      <span class="c1"># softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]))</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span>
          <span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">narrow_to_asym_bit_depth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1"># Reshape probs to be of shape [tb/sb, sb, sl].</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape [sb, tb/sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># [sb, tb/sb, sl] * [sb, sl, context_dim] = [sb, tb/sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">concated_source_contexts</span><span class="p">,</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;atten_context&#39;</span><span class="p">)</span>
      <span class="c1"># summed is of shape [tb/sb, sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">probs</span>

    <span class="k">def</span> <span class="nf">AttenSameBatchSize</span><span class="p">(</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
                           <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
                           <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">,</span>
                           <span class="n">location_var</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Optimized code path for when source and target have the same batch size.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">del</span> <span class="n">per_step_source_padding</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
      <span class="c1"># attention_state shape [batch, len(p.location_features), slen]</span>
      <span class="c1"># it contains previous and accumulated attention probabilites.</span>
      <span class="n">attention_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span>
                                          <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

      <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyConv</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">)</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
          <span class="n">query_vec</span><span class="p">,</span> <span class="n">query_var</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;atten_matmul&#39;</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl, sb]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">_ConditionalCallDefun</span><span class="p">(</span>
          <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quantized</span><span class="p">,</span> <span class="n">AttenLogitsSameBatchSize</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">concated_source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">query_vec_transformed</span><span class="o">=</span><span class="n">query_vec_transformed</span><span class="p">,</span>
              <span class="n">hidden_v</span><span class="o">=</span><span class="n">hidden_var</span><span class="p">,</span>
              <span class="n">location_feats</span><span class="o">=</span><span class="n">location_feats</span><span class="p">,</span>
              <span class="n">location_var</span><span class="o">=</span><span class="n">location_var</span><span class="p">))</span>
      <span class="c1"># =&gt; [sl, tb]</span>
      <span class="n">logits</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">source_padding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="c1"># Reshape logits to a matrix of shape [tb, sl] and takes the</span>
      <span class="c1"># softmax to compute the probabilities.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PaddedSoftmax</span><span class="p">(</span>
          <span class="n">logits</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span> <span class="n">narrow_to_asym_bit_depth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qbatchmatmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">concated_source_contexts</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">concated_source_contexts</span><span class="p">,</span>
          <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;atten_context&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">probs</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">AttenSameBatchSize</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">fns</span><span class="o">.</span><span class="n">qmatmul</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">]),</span> <span class="n">src_w</span><span class="p">,</span> <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;encode_matmul&#39;</span><span class="p">),</span>
          <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="LocationSensitiveAttention._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">location_filter_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="n">location_filter_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">p</span><span class="o">.</span><span class="n">location_filter_size</span><span class="p">,</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span>
    <span class="p">]</span>
    <span class="c1"># TODO(yonghui): Don&#39;t hard code how params are initialized.</span>
    <span class="n">location_filter_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">location_filter_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;location_filter_var&#39;</span><span class="p">,</span> <span class="n">location_filter_pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>
    <span class="n">location_var_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">location_num_filters</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]</span>
    <span class="n">location_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">location_var_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LocationSensitiveAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;location_var&#39;</span><span class="p">,</span> <span class="n">location_pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;atten_conv&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;atten_context&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;atten_context&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span>
        <span class="s1">&#39;atten_matmul&#39;</span><span class="p">,</span>
        <span class="s1">&#39;logits_add&#39;</span><span class="p">,</span>
        <span class="s1">&#39;encode_matmul&#39;</span><span class="p">,</span>
        <span class="s1">&#39;logits_mul&#39;</span><span class="p">,</span>
        <span class="s1">&#39;logits_bias&#39;</span><span class="p">,</span>
        <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;fullyconnected&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention._ApplyConv"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention._ApplyConv">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter_var</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies the convolution on attention state.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fns</span>
    <span class="n">attention_state_f32</span> <span class="o">=</span> <span class="n">attention_state</span>
    <span class="n">location_filter_var_f32</span> <span class="o">=</span> <span class="n">location_filter_var</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">attention_state_f32</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">attention_state</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">location_filter_var_f32</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_filter_var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">data_format</span> <span class="o">=</span> <span class="s1">&#39;NCW&#39;</span>
    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_xla</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
      <span class="c1"># NCW format is not supported on CPU.</span>
      <span class="n">attention_state_f32</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">attention_state_f32</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">data_format</span> <span class="o">=</span> <span class="s1">&#39;NWC&#39;</span>
    <span class="n">location_feats</span> <span class="o">=</span> <span class="n">fns</span><span class="o">.</span><span class="n">qconv1d</span><span class="p">(</span>
        <span class="n">attention_state_f32</span><span class="p">,</span>
        <span class="n">location_filter_var_f32</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="n">qt</span><span class="o">=</span><span class="s1">&#39;atten_conv&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_xla</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">location_feats</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">location_feats</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">location_feats</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># [sb, hd, sl]</span>
    <span class="k">return</span> <span class="n">location_feats</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">),</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="c1"># [time, batch_size, hidden_dim].</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="c1"># [batch_size, time, context_dim].</span>
        <span class="c1"># Note the mismatch between `source_vecs` and `source_contexts`. In</span>
        <span class="c1"># `source_vecs`, time is the first dim, while it is the second dim in</span>
        <span class="c1"># `source_contexts`.</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">source_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
      <span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

      <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRSoftmax</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">state</span></div>

<div class="viewcode-block" id="LocationSensitiveAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.LocationSensitiveAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: If `params().location_features == [&#39;PREV_PROBS&#39;,</span>
<span class="sd">        &#39;CUMULATIVE_PROBS&#39;]`, then `attention_state` is a tensor of shape</span>
<span class="sd">        [batch_size, 2, src_len].</span>

<span class="sd">        - attention_state[:, 0, :] contains previous attention probabilities.</span>
<span class="sd">        - attention_state[:, 1, :] contains a sum over previous timesteps of</span>
<span class="sd">          attention probabilities.</span>

<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: Query segment id with shape [batch_size].</span>

<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">      concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">      attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The new attention mechanism state: possibly nested tuple of tensors with</span>
<span class="sd">        dimensions [target_batch, ...]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">same_batch_size</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="n">query_batch_size</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_length</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
        <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">query_batch_size</span><span class="p">,</span> <span class="n">source_length</span><span class="p">])</span>

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">location_filter</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">location_filter_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">location</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">location_var</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">source_padding</span><span class="p">,</span>
                                  <span class="n">concated_source_vecs</span><span class="p">,</span>
                                  <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span>
                                  <span class="n">attention_state</span><span class="p">,</span> <span class="n">location_filter</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span>
                                  <span class="n">per_step_source_padding</span><span class="p">)</span>

    <span class="n">new_feats</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;PREV_PROBS&#39;</span><span class="p">:</span> <span class="n">prob</span><span class="p">}</span>
    <span class="k">if</span> <span class="s1">&#39;CUMULATIVE_PROBS&#39;</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">:</span>
      <span class="c1"># Quantization must match the _PaddedSoftmax method.</span>
      <span class="n">cum_prob_index</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;CUMULATIVE_PROBS&#39;</span><span class="p">)</span>
      <span class="n">new_feats</span><span class="p">[</span><span class="s1">&#39;CUMULATIVE_PROBS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRSoftmax</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">[:,</span> <span class="n">cum_prob_index</span><span class="p">,</span> <span class="p">:]),</span>
          <span class="n">narrow_to_asym_bit_depth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">new_attention_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">new_feats</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">location_features</span><span class="p">],</span>
                                   <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">new_attention_state</span></div></div>


<div class="viewcode-block" id="MergeSourcePaddingWithPerStepSourcePadding"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergeSourcePaddingWithPerStepSourcePadding">[docs]</a><span class="k">def</span> <span class="nf">MergeSourcePaddingWithPerStepSourcePadding</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span>
                                               <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">tb</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merges source padding with per-step source padding.</span>

<span class="sd">  Args:</span>
<span class="sd">    source_padding: [sl, sb].</span>
<span class="sd">    per_step_source_padding: [tb, sl].</span>
<span class="sd">    tb: target batch size.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of shape [tb, sl].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># source_padding is of shape [sl, sb].</span>
  <span class="n">sl</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">sb</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">source_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">tb</span><span class="p">,</span> <span class="n">sl</span><span class="p">],</span> <span class="n">zero</span><span class="p">)</span>
  <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="n">sl</span><span class="p">])</span>

  <span class="c1"># Transpose and reshape source_padding to [1, sb,  sl].</span>
  <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="c1"># Merge source_padding and per_step_source_padding.</span>
  <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">sl</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>


<div class="viewcode-block" id="MonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention">[docs]</a><span class="k">class</span> <span class="nc">MonotonicAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An attention mechanism which enforces monotonic alignments.</span>

<span class="sd">  This layer implements the monotonic attention mechanism described in</span>
<span class="sd">  Online and Linear-Time Attention by Enforcing Mononotonic Alignments</span>
<span class="sd">  (https://arxiv.org/abs/1704.00784).  It is used in exactly the same way as</span>
<span class="sd">  AdditiveAttention, but both the attention distribution and the energy function</span>
<span class="sd">  are different.</span>

<span class="sd">  Rather than using a softmax, this mechanism feeds the attention energy into a</span>
<span class="sd">  (hard or soft) sigmoid and treats the output as Bernoulli probabilities</span>
<span class="sd">  representing the probability of attending to a given entry in the input</span>
<span class="sd">  sequence, processed from left-to-right.  Based on this interpretation, the</span>
<span class="sd">  resulting distribution over input sequence entries is computed with a dynamic</span>
<span class="sd">  program.  The intended use is to train with soft sigmoids according to the</span>
<span class="sd">  expected output (setting param hard_sigmoid=False), then use hard sigmoids at</span>
<span class="sd">  test time to allow for online and linear-time decoding.  To encourge the train</span>
<span class="sd">  and test-time behavior to be similar, noise can optionally be added to the</span>
<span class="sd">  sigmoid activations during training (param pre_sigmoid_noise).  For the energy</span>
<span class="sd">  function, rather than computing::</span>

<span class="sd">    E = dot(v, tanh(dot(W, query) + dot(W, encoder_states)))</span>

<span class="sd">  it computes::</span>

<span class="sd">    E = dot(g*v/||v||, tanh(dot(W, query) + dot(W, encoder_states) + b)) + r</span>

<span class="sd">  where g and r are scalars and b is a vector, and ||v|| is the L2 norm of v.</span>
<span class="sd">  instead.  These modifications address the fact that the sigmoids in the</span>
<span class="sd">  monotonic attention mechanism are sensitive to offset and a bit harder to</span>
<span class="sd">  train compared to the softmax function.  It can be helpful to initialize the</span>
<span class="sd">  energy bias scalar r to a negative value (param hidden_bias_init).</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MonotonicAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MonotonicAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pre_sigmoid_noise&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Standard deviation of pre-sigmoid noise.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_bias_init&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Initial value of hidden bias.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hard_sigmoid&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use a hard sigmoid.&#39;</span><span class="p">)</span>
    <span class="c1"># Fill in reasonable default for params init</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">GaussianSqrtDim</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an MonotonicAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Packed input not supported for Monotonic &#39;</span>
                                <span class="s1">&#39;Attention.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported&#39;</span><span class="p">)</span>

    <span class="c1"># When running eval, don&#39;t add pre-sigmoid noise, and use a hard sigmoid to</span>
    <span class="c1"># match behavior of online decoding.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">pre_sigmoid_noise</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">p</span><span class="o">.</span><span class="n">hard_sigmoid</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">src_w</span><span class="p">,</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transformed_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">]),</span> <span class="n">src_w</span><span class="p">),</span>
          <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">transformed_vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="MonotonicAttention._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># source is the weight matrix for the memory/encoder states</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">source_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;source_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="c1"># query is the weight matrix for the query/decoder RNN state</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;query_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="c1"># hidden is the pre-softmax vector which converts from tanh to scalar</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="c1"># energy_bias is the bias vector which appears inside of tanh</span>
    <span class="c1"># Initialize the bias vector to all zeros</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;energy_bias_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>

    <span class="c1"># hidden_scale is the weight normalization scale for hidden</span>
    <span class="c1"># Initialize so that the initial scale is 1/sqrt(hidden_dim)</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_scale_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>

    <span class="c1"># hidden_bias is the bias scalar applied before the sigmoid</span>
    <span class="c1"># Use the hidden_bias_init hyperparam to set the initial value</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_bias_init</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonotonicAttention_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;hidden_bias_var&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">source_var</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="c1"># [time, batch_size, hidden_dim].</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="c1"># [batch_size, time, context_dim].</span>
        <span class="c1"># Note the mismatch between `source_vecs` and `source_contexts`. In</span>
        <span class="c1"># `source_vecs`, time is the first dim, while it is the second dim in</span>
        <span class="c1"># `source_contexts`.</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="c1"># [time, batch_size].</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="c1"># Set initial previous attention to [1, 0, ... 0] to avoid special-casing</span>
      <span class="n">emit_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">decoder_batch_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
          <span class="n">source_length</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">emit_probs</span><span class="o">=</span><span class="n">emit_probs</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.ComputeProbabilities"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ComputeProbabilities">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeProbabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                           <span class="n">merged_source_padding</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes probabilities of emissions.&quot;&quot;&quot;</span>

    <span class="c1"># concated_source_contexts is of shape [sb, sl, context_dim]</span>
    <span class="c1"># query_vec is of shape [tb, dims]</span>
    <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">AttenLogits</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes logits from source, query, and variables.</span>

<span class="sd">      Args:</span>
<span class="sd">        inputs: a NestedMap containing:</span>
<span class="sd">          - concated_source_vecs: [sl, sb, hidden_dims].</span>
<span class="sd">          - query_vec: [tb, query_dim].</span>
<span class="sd">          - query_v: [query_dim, hidden_dim]</span>
<span class="sd">          - energy_b: [hidden_dim].</span>
<span class="sd">          - hidden_v: [hidden_dim].</span>
<span class="sd">          - hidden_g: [].</span>
<span class="sd">          - hidden_b: [].</span>

<span class="sd">      Returns:</span>
<span class="sd">        logits shaped [tb, sl].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># Apply query matrix to query. Becomes [tb, hidden_dim].</span>
      <span class="n">query_vec_transformed</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">inputs</span><span class="o">.</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">query_v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;query_transformation&#39;</span><span class="p">)</span>
      <span class="c1"># query_vec is reshaped to [1, tb/sb, sb, hidden_dim].</span>
      <span class="n">query_vec_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec_transformed</span><span class="p">,</span>
                                      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">])</span>

      <span class="c1"># [sl, 1, sb, hidden_dim].</span>
      <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">energy_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">energy_b</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Shape of summed is [sl, tb/sb, sb, hidden_dim].</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">concated_source_vecs</span> <span class="o">+</span> <span class="n">query_vec_reshaped</span> <span class="o">+</span> <span class="n">energy_b</span><span class="p">)</span>
      <span class="n">hidden_v</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">hidden_g</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">hidden_v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="c1"># logits is of shape [sl * tb/sb * sb, 1]. Computes dot product</span>
      <span class="c1"># between v with every rows in &#39;summed&#39;. Then we reshape the</span>
      <span class="c1"># result to be of shape [sl, tb/sb, sb].</span>
      <span class="c1">#</span>
      <span class="c1"># Another equivalent way is to do:</span>
      <span class="c1">#  logits = tf.reduce_sum(summed *</span>
      <span class="c1">#                         tf.reshape(v, [1, 1, 1, hidden_dim]), 3)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">]),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_v</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">hidden_b</span>
      <span class="c1"># [tb, sl].</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tb</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">logits</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">):</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">AttenLogits</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">concated_source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
              <span class="n">query_vec</span><span class="o">=</span><span class="n">query_vec</span><span class="p">,</span>
              <span class="n">query_v</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">query_var</span><span class="p">,</span>
              <span class="n">energy_b</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">energy_bias_var</span><span class="p">,</span>
              <span class="n">hidden_v</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">hidden_var</span><span class="p">,</span>
              <span class="n">hidden_g</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">hidden_scale_var</span><span class="p">,</span>
              <span class="n">hidden_b</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">hidden_bias_var</span><span class="p">))</span>

    <span class="n">previous_attention</span> <span class="o">=</span> <span class="n">attention_state</span><span class="o">.</span><span class="n">emit_probs</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;prob&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hard_sigmoid</span><span class="p">:</span>
        <span class="c1"># If using a hard sigmoid, just compare against 0</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Never choose padded values.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">merged_source_padding</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">),</span> <span class="n">p_choose_i</span><span class="p">)</span>
        <span class="c1"># Compute probability distribution assuming hard probabilities</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">MonotonicAttentionProb</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">,</span> <span class="n">previous_attention</span><span class="p">,</span> <span class="s1">&#39;hard&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute pre-sigmoid noise.</span>
        <span class="n">activation_noise</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_normal</span><span class="p">(</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Compute sigmoid probabilities.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">pre_sigmoid_noise</span> <span class="o">*</span>
                                   <span class="n">activation_noise</span><span class="p">)</span>
        <span class="c1"># Never choose padded values.</span>
        <span class="n">p_choose_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">merged_source_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
                              <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">),</span> <span class="n">p_choose_i</span><span class="p">)</span>
        <span class="c1"># Compute attention distribution</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">MonotonicAttentionProb</span><span class="p">(</span><span class="n">p_choose_i</span><span class="p">,</span> <span class="n">previous_attention</span><span class="p">,</span>
                                       <span class="s1">&#39;parallel&#39;</span><span class="p">)</span>

    <span class="c1"># [tb, sl].</span>
    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">emit_probs</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span></div>

<div class="viewcode-block" id="MonotonicAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MonotonicAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [batch_size, query_dim].</span>
<span class="sd">      attention_state: The attention probs computed at the previous timestep.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch_size, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [batch_size].</span>
<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">      concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">      attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [batch_size, context_dim]</span>
<span class="sd">      - The attention probability vector: [batch_size, time]</span>
<span class="sd">      - The attention probability vector: (again, to be interpreted as state).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>
    <span class="n">sb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">tb</span> <span class="o">//</span> <span class="n">sb</span>
    <span class="n">merged_source_padding</span> <span class="o">=</span> <span class="n">MergeSourcePaddingWithPerStepSourcePadding</span><span class="p">(</span>
        <span class="n">source_padding</span><span class="p">,</span> <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">tb</span><span class="p">)</span>

    <span class="n">probs</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ComputeProbabilities</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                                                 <span class="n">merged_source_padding</span><span class="p">,</span>
                                                 <span class="n">query_vec</span><span class="p">,</span> <span class="n">attention_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">):</span>
      <span class="c1"># Reshape probs to be of shape [tb/sb, sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Transpose probs to be of shape [sb, tb/sb, sl]</span>
      <span class="n">probs_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="c1"># Batched matmul</span>
      <span class="c1"># [sb, tb/sb, sl] * [sb, sl, context_dim] = [sb, tb/sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_reshaped</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span>
      <span class="c1"># summed is of shape [tb/sb, sb, context_dim]</span>
      <span class="n">summed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">summed</span><span class="p">,</span> <span class="p">[</span><span class="n">tb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="GmmMonotonicAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention">[docs]</a><span class="k">class</span> <span class="nc">GmmMonotonicAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A GMM-based monotonic attention module.</span>

<span class="sd">  Based on &quot;Generating Sequences With Recurrent Neural Networks&quot; by Alex Graves.</span>
<span class="sd">  Eq [46-51] in https://arxiv.org/abs/1308.0850.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GmmMonotonicAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MonotonicAttention class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span>
             <span class="s1">&#39;Number of hidden units for the MLP that predicts GMM params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;max_offset&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Max offset to move attention pointer, Enabled only when &gt; 0.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_mixtures&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;Number of location GMM components.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;normalize_probs&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether to normalize probabilities computed by GMM. Otherwise, &#39;</span>
        <span class="s1">&#39;the attention weights (i.e. probabilities) may not add up to &#39;</span>
        <span class="s1">&#39;1.0.&#39;</span><span class="p">)</span>

    <span class="c1"># TODO(ngyuzh): find a good initialize for both TTS and ASR. Consider split</span>
    <span class="c1"># the layer if it&#39;s very sensitive to the initialization</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a GMM-based monotonic attention module.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;dropout is not supported.&#39;</span><span class="p">)</span>

    <span class="c1"># TODO(ngyuzh): Compare Sigmoid and other activation functions.</span>
    <span class="n">ff_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">FeedForwardNet</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">,</span>
        <span class="n">hidden_layer_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span> <span class="o">*</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">activation</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">],</span>
        <span class="n">params_init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="o">.</span><span class="n">Copy</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;GMM&#39;</span><span class="p">,</span> <span class="n">ff_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ComputeProbs</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the location GMM probabilities at all encoder positions.</span>

<span class="sd">      This function assumes that the first 2 dimensions of `priors`, `means`,</span>
<span class="sd">      `variances`, and the return value:</span>
<span class="sd">      `multiplier (target_batch / source_batch)` and `source_batch` are</span>
<span class="sd">      transposed, and `encoder_positions` has only non-one dimensions.</span>

<span class="sd">      Args:</span>
<span class="sd">        encoder_positions: [source_batch, source_length]</span>
<span class="sd">        priors: [multiplier, source_batch, num_mixtures]</span>
<span class="sd">        means: [multiplier, source_batch, num_mixtures]</span>
<span class="sd">        variances: [multiplier, source_batch, num_mixtures]</span>

<span class="sd">      Returns:</span>
<span class="sd">        Probabilities shaped [multiplier, source_batch, source_length].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># [multiplier, source_batch, 1, num_mixtures]</span>
      <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>

      <span class="c1"># [source_batch, source_length, 1]</span>
      <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

      <span class="c1"># [multiplier, source_batch, source_length, num_mixtures]</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="p">((</span><span class="n">priors</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variances</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span>
               <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">encoder_positions</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span>
                      <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">variances</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)))</span>

      <span class="c1"># [multiplier, source_batch, source_length]</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">Atten</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">,</span>
              <span class="n">query_vec</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">encoder_positions</span><span class="p">,</span>
              <span class="n">per_step_source_padding</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Computes the attention context vector.</span>

<span class="sd">      Args:</span>
<span class="sd">        source_padding: [source_length, source_batch]</span>
<span class="sd">        concated_source_vecs: [source_length, source_batch, hidden_dim]</span>
<span class="sd">        concated_source_contexts: [source_batch, source_length, context_dim]</span>
<span class="sd">        query_vec: [target_batch, query_dim]</span>
<span class="sd">        priors: [target_batch, num_mixtures]</span>
<span class="sd">        means: [target_batch, num_mixtures]</span>
<span class="sd">        variances: [target_batch, num_mixtures]</span>
<span class="sd">        encoder_positions: [source_batch, source_length]</span>
<span class="sd">        per_step_source_padding: [target_batch, source_length]</span>

<span class="sd">      Returns:</span>
<span class="sd">        Tuple(context vector, atten probs):</span>

<span class="sd">        - context vector: [target_batch, context_dim]</span>
<span class="sd">        - attention probabilities: [target_batch, source_length]</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># Note: shape [target_batch] can be converted to</span>
      <span class="c1"># [multiplier, source_batch], not [source_batch, multiplier].</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">target_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">multiplier</span> <span class="o">=</span> <span class="n">target_batch</span> <span class="o">//</span> <span class="n">source_batch</span>

      <span class="c1"># [multiplier, source_batch, num_mixtures]</span>
      <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>
      <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>
      <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span>
                             <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">])</span>

      <span class="c1"># [multiplier, source_batch, source_length]</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">ComputeProbs</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">variances</span><span class="p">)</span>

      <span class="c1"># [source_batch, source_length]</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>

      <span class="c1"># [multiplier, source_batch, source_length]</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">source_padding</span> <span class="o">+=</span> <span class="n">per_step_source_padding</span>
      <span class="n">source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

      <span class="c1"># [multiplier, source_batch, source_length]</span>
      <span class="n">probs</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">source_padding</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">normalize_probs</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">/=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="mf">1e-12</span><span class="p">)</span>

      <span class="c1"># [source_batch, multiplier, source_length]</span>
      <span class="n">probs_transposed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="c1"># Matmul:</span>
      <span class="c1"># [source_batch, multiplier, source_length]</span>
      <span class="c1"># @ [source_batch, source_length, context_dim]</span>
      <span class="c1"># -&gt; [source_batch, multiplier, context_dim]</span>
      <span class="n">context_vector_transposed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs_transposed</span><span class="p">,</span>
                                            <span class="n">concated_source_contexts</span><span class="p">)</span>

      <span class="c1"># [multiplier, source_batch, context_dim]</span>
      <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">context_vector_transposed</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

      <span class="c1"># [target_batch, context_dim], [target_batch, source_length]</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span> <span class="o">=</span> <span class="n">Atten</span>

    <span class="k">def</span> <span class="nf">EncodeSource</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="n">ctxs</span><span class="p">):</span>
      <span class="c1"># TODO(ngyuzh): combine with content-base attention.</span>
      <span class="n">time</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">vecs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">ctxs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">transposed_ctxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ctxs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">vecs</span><span class="p">,</span> <span class="n">transposed_ctxs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span> <span class="o">=</span> <span class="n">EncodeSource</span>

<div class="viewcode-block" id="GmmMonotonicAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">source_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)</span>
      <span class="p">(</span><span class="n">concated_source_vecs</span><span class="p">,</span> <span class="n">concated_source_contexts</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_encode_source</span><span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_contexts</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="c1"># [source_length, source_batch, hidden_dim].</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="n">concated_source_vecs</span><span class="p">,</span>
        <span class="c1"># [source_batch, source_length, context_dim].</span>
        <span class="c1"># Note the mismatch between `source_vecs` and `source_contexts`. In</span>
        <span class="c1"># `source_vecs`, `source_length` is the first dim, while it is the</span>
        <span class="c1"># second dim in `source_contexts`.</span>
        <span class="n">source_contexts</span><span class="o">=</span><span class="n">concated_source_contexts</span><span class="p">,</span>
        <span class="c1"># [source_length, source_batch].</span>
        <span class="n">source_padding</span><span class="o">=</span><span class="n">source_padding</span><span class="p">,</span>
        <span class="c1"># [source_length, source_batch].</span>
        <span class="n">source_segment_id</span><span class="o">=</span><span class="n">source_segment_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="GmmMonotonicAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># [target_batch, num_mixtures]</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">position_offsets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">decoder_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_mixtures</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># [target_batch, num_mixtures, 4]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">position</span><span class="p">,</span> <span class="n">position_offsets</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">priors</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></div>

<div class="viewcode-block" id="GmmMonotonicAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.GmmMonotonicAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the context vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      packed_src: A `.NestedMap` object returned by PackSource or</span>
<span class="sd">        InitForSourcePacked.</span>
<span class="sd">      query_vec: a tensor of shape [target_batch, query_dim].</span>
<span class="sd">      attention_state: previous attention state, a tensor of shape</span>
<span class="sd">        [target_batch, num_mixtures, 4].</span>
<span class="sd">        - attention_state[:, :, 0] contains previous location</span>
<span class="sd">        - attention_state[:, :, 1] contains previous offset.</span>
<span class="sd">        - attention_state[:, :, 2] contains previous variance.</span>
<span class="sd">        - attention_state[:, :, 3] contains previous prior.</span>
<span class="sd">      per_step_source_padding: Source sequence padding to apply at this step. If</span>
<span class="sd">        not None, it should be of shape [target_batch, source_length].</span>
<span class="sd">      query_segment_id: a tensor of shape [target_batch].</span>
<span class="sd">    Note: concated_source_vecs are the vectors that are used to compute the</span>
<span class="sd">      attention score between the query_vec and each concated_source_vec. The</span>
<span class="sd">      concated_source_contexts are the vectors that compose the result. The</span>
<span class="sd">      attention context vector is computed as a weighted average of the</span>
<span class="sd">      concated_source_contexts, using the scores that were computed using</span>
<span class="sd">      concated_source_vecs.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of 3 elements.</span>

<span class="sd">      - The attention context vector: [target_batch, context_dim]</span>
<span class="sd">      - The attention probability vector: [target_batch, source_length]</span>
<span class="sd">      - The new attention state vector: [target_batch, num_mixtures, 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">query_segment_id</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">concated_source_vecs</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_vecs</span>
    <span class="n">concated_source_contexts</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_contexts</span>
    <span class="n">source_padding</span> <span class="o">=</span> <span class="n">packed_src</span><span class="o">.</span><span class="n">source_padding</span>

    <span class="n">target_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">source_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">source_padding</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># [target_batch, source_length]</span>
    <span class="k">if</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">source_length</span><span class="p">],</span>
                                         <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">per_step_source_padding</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">per_step_source_padding</span><span class="p">,</span>
                                                <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">source_length</span><span class="p">])</span>

    <span class="c1"># [target_batch, num_mixtures * 3]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GMM</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">GMM</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># [target_batch, num_mixtures]</span>
    <span class="n">priors_logits</span><span class="p">,</span> <span class="n">position_offset_logits</span><span class="p">,</span> <span class="n">log_variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GMM&#39;</span><span class="p">)</span>

    <span class="n">log_variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">log_variances</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LOG_SCALE_CLAMP_BOUND</span><span class="p">)</span>
    <span class="n">variances</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_variances</span><span class="p">)</span>

    <span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">priors_logits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">position_offset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">position_offset_logits</span><span class="p">)</span>
      <span class="n">position_offset</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">max_offset</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">position_offset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">position_offset_logits</span><span class="p">)</span>

    <span class="n">new_position</span> <span class="o">=</span> <span class="n">attention_state</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">position_offset</span>

    <span class="c1"># Tile and reshape encoder_positions to [source_batch, source_length]</span>
    <span class="c1"># so that it can be evaluated by locations GMMs in a vectorized way.</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">source_length</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">encoder_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">encoder_positions</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># [target_batch, context_dim], [target_batch, source_length]</span>
    <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx_vec</span><span class="p">(</span><span class="n">source_padding</span><span class="p">,</span> <span class="n">concated_source_vecs</span><span class="p">,</span>
                                  <span class="n">concated_source_contexts</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span>
                                  <span class="n">new_position</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">encoder_positions</span><span class="p">,</span>
                                  <span class="n">per_step_source_padding</span><span class="p">)</span>

    <span class="c1"># [target_batch, num_mixtures, 4]</span>
    <span class="n">new_atten_states</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">new_position</span><span class="p">,</span> <span class="n">position_offset</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">priors</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">new_atten_states</span></div></div>


<div class="viewcode-block" id="MergerLayer"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergerLayer">[docs]</a><span class="k">class</span> <span class="nc">MergerLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merges a list of input tensors with various options into a single tensor.</span>

<span class="sd">  Implements a merger/combiner operator given a list of tensors. The merger</span>
<span class="sd">  operator outputs a single tensor with the following options (merger_op):</span>

<span class="sd">  - atten: Applies attention over the set of input tensors given query vector.</span>
<span class="sd">  - mean: Takes the mean of input tensors.</span>
<span class="sd">  - concat: Concatenates the input tensors over the last dimension.</span>
<span class="sd">  - sum: Sum up all the input tensors.</span>
<span class="sd">  - weighted_sum: Use learnt weights to combine input tensors.</span>
<span class="sd">  - gated_avg: Learnt input dependent gates are used to average tensors.</span>

<span class="sd">  This class is expected to be called by multi-source/multi-column models.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MergerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MergerLayer class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;merger_op&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;How to merge input tensors.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of source nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_tpl&#39;</span><span class="p">,</span> <span class="n">AdditiveAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Attention used by the merger layer when merger_op is atten.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;pre_proj_input_dims&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If set, should be a list of depths for the tensors to be merged.&#39;</span>
        <span class="s1">&#39; Setting this will result in a pre-projection to source_dim&#39;</span>
        <span class="s1">&#39; before the merger.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;pre_proj_output_dims&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Should be a list of depths which the input tensors specified in &#39;</span>
        <span class="s1">&#39;pre_proj_input_dims need to be projected to. Should match the length &#39;</span>
        <span class="s1">&#39;of pre_proj_input_dims.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;proj_tpl&#39;</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">ProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s1">&#39;Configs template for the projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gated_avg_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">GatedAverageLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Configs template for the gated average layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_sources&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;If merger_op=weighted_sum, then must specify &#39;</span>
             <span class="s1">&#39;num of sources.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;post_proj&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Post projection for the merged context vector.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="c1"># Merging operation keys supported by this layer.</span>
  <span class="n">MERGER_OPS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="s1">&#39;concat&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;weighted_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;gated_avg&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Layer must have a specified name!&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">MERGER_OPS</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Merger op must be one of: &#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MERGER_OPS</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;atten&#39;</span><span class="p">:</span>
      <span class="n">atten_params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">atten_params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span>
      <span class="n">atten_params</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
      <span class="n">atten_params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">atten_params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
      <span class="k">if</span> <span class="n">atten_params</span><span class="o">.</span><span class="n">params_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">atten_params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span>
            <span class="mf">1.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">atten_params</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">+</span> <span class="n">atten_params</span><span class="o">.</span><span class="n">query_dim</span><span class="p">),</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">atten_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">pre_proj_output_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Output dims should be specified for projection.&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_output_dims</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;Output dims should be the same length as input dims. &#39;</span>
            <span class="s1">&#39;Expected: </span><span class="si">%s</span><span class="s1"> obtained: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
            <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_output_dims</span><span class="p">)))</span>
      <span class="n">pre_proj_params</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pre_proj_input_dim</span><span class="p">,</span> <span class="n">pre_proj_output_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
          <span class="nb">zip</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">pre_proj_output_dims</span><span class="p">)):</span>
        <span class="n">proj_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="n">proj_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;merger_pre_proj_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
        <span class="n">proj_p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">pre_proj_input_dim</span>
        <span class="n">proj_p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">pre_proj_output_dim</span>
        <span class="n">pre_proj_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;pre_proj&#39;</span><span class="p">,</span> <span class="n">pre_proj_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;gated_avg&#39;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;For merger_op=gated_avg, must specify &#39;</span>
                                 <span class="s1">&#39;num_sources &gt; 0.&#39;</span><span class="p">)</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_avg_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;g_avg_merger&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_nodes</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;gated_average&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">post_proj</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;post_proj&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">post_proj</span><span class="p">)</span>

<div class="viewcode-block" id="MergerLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergerLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;weighted_sum&#39;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;For merger_op=weighted_sum, must specify &#39;</span>
                                 <span class="s1">&#39;num_sources &gt; 0.&#39;</span><span class="p">)</span>
      <span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">)</span>
      <span class="c1"># Weights to be learned.</span>
      <span class="n">pw</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;sum_weight&#39;</span><span class="p">,</span> <span class="n">pw</span><span class="p">)</span></div>

<div class="viewcode-block" id="MergerLayer._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergerLayer._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Backwards compatibility: manually call child.InstantiateVariables()</span>
    <span class="c1"># outside of tf.variable_scope(p.name).</span>
    <span class="k">if</span> <span class="s1">&#39;atten&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="k">if</span> <span class="s1">&#39;gated_average&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">gated_average</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="k">if</span> <span class="s1">&#39;pre_proj&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">proj</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_proj</span><span class="p">:</span>
        <span class="n">proj</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

<div class="viewcode-block" id="MergerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MergerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">query_vec</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the list of input tensors into a single tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A list of tensors of shape [..., hidden_dim] or [...,</span>
<span class="sd">        [pre_proj_input_dims[i]]] if pre_proj_input_dims is specified.</span>
<span class="sd">      query_vec: A tensor of shape [..., hidden_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of the same shape with input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: p.merger_op is not defined.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">n_sources</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_sources</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;pre_proj_input_dims must be specified for each input.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_sources</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Pre-projection operation.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pre_proj_input_dims</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sources</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_proj</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pre_proj</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">tensor_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
      <span class="c1"># Simply take the mean, all dims must match.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t2</span><span class="p">))</span>
          <span class="k">for</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">tensor_pairs</span>
      <span class="p">]):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_sources</span>

    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
      <span class="c1"># Sum up all sources, all dims must match.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t2</span><span class="p">))</span>
          <span class="k">for</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">tensor_pairs</span>
      <span class="p">]):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;weighted_sum&#39;</span><span class="p">:</span>
      <span class="c1"># Weighted sum of all sources, all dims must match.</span>
      <span class="c1"># For weighted_sum, assume input is a list of rank 3 tensors</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t2</span><span class="p">))</span>
          <span class="k">for</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">tensor_pairs</span>
      <span class="p">]):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">sum_weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span>
            <span class="p">[</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">2</span><span class="p">],</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">3</span><span class="p">]])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;atten&#39;</span><span class="p">:</span>
      <span class="c1"># Apply attention over the concatenated tensor, all dims must match.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t2</span><span class="p">))</span>
          <span class="k">for</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">tensor_pairs</span>
      <span class="p">]):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_sources</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">ComputeContextVector</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span><span class="p">]))</span>

    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
      <span class="c1"># Concatenate over the last dim, all dims but last must match.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t2</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">tensor_pairs</span>
      <span class="p">]):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">merger_op</span> <span class="o">==</span> <span class="s1">&#39;gated_avg&#39;</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gated_average</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">gated_average</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unrecognized merge op!&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">post_proj</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post_proj</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="MultiSourceAttention"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiSourceAttention</span><span class="p">(</span><span class="n">BaseAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Attention with multiple source sub-attentions.</span>

<span class="sd">  It attends to multiple sources and uses one query as input to generates a</span>
<span class="sd">  combined attention context. The dimension of the combined context vector is a</span>
<span class="sd">  sum of all source context vectors. Each source attention has its separate</span>
<span class="sd">  params and is associated with a source key.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiSourceAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_atten_tpls&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;A list of (source_key, attention_param) &#39;</span>
             <span class="s1">&#39;pairs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Default source dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;query_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of query nodes. Child attention params &#39;</span>
        <span class="s1">&#39;must have query_dim less or euqal than 0 or equal to this value.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;primary_source_key&#39;</span><span class="p">,</span> <span class="s1">&#39;source_0&#39;</span><span class="p">,</span> <span class="s1">&#39;Key for the primary source &#39;</span>
        <span class="s1">&#39;whose attention probabilities will be used as an output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_merger_tpl&#39;</span><span class="p">,</span>
        <span class="n">MergerLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">params_init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">0.04</span><span class="p">),</span> <span class="n">merger_op</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">),</span>
        <span class="s1">&#39;Params to specify how to merge source attention vectors.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an MultiSourceAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">atten_p</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span><span class="p">:</span>
      <span class="n">child_p</span> <span class="o">=</span> <span class="n">atten_p</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">child_p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">child_p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
      <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">child_p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">child_p</span><span class="p">)</span>

    <span class="c1"># Initialize source context vector merging layer.</span>
    <span class="n">merger_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_merger_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_merger&#39;</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">source_dim</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">query_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_merger&#39;</span><span class="p">,</span> <span class="n">merger_p</span><span class="p">)</span>

<div class="viewcode-block" id="MultiSourceAttention.PackSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention.PackSource">[docs]</a>  <span class="k">def</span> <span class="nf">PackSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">source_vecs</span><span class="p">,</span>
                 <span class="n">source_contexts</span><span class="p">,</span>
                 <span class="n">source_padding</span><span class="p">,</span>
                 <span class="n">source_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">packed_src</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span><span class="p">:</span>
        <span class="n">packed_src</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">]</span><span class="o">.</span><span class="n">InitForSourcePacked</span><span class="p">(</span>
                <span class="n">theta</span><span class="p">[</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">],</span> <span class="n">source_vecs</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span>
                <span class="n">source_contexts</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span> <span class="n">source_padding</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span>
                <span class="n">source_segment_id</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span> <span class="k">if</span> <span class="n">source_segment_id</span> <span class="k">else</span> <span class="kc">None</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">packed_src</span></div>

<div class="viewcode-block" id="MultiSourceAttention.ZeroAttentionState"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention.ZeroAttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">ZeroAttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_seq_length</span><span class="p">,</span> <span class="n">decoder_batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">({</span>
          <span class="n">source_key</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">)</span><span class="o">.</span><span class="n">ZeroAttentionState</span><span class="p">(</span>
              <span class="n">source_seq_length</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span> <span class="n">decoder_batch_size</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span>
      <span class="p">})</span></div>

<div class="viewcode-block" id="MultiSourceAttention.ComputeContextVectorWithSource"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention.ComputeContextVectorWithSource">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeContextVectorWithSource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                     <span class="n">theta</span><span class="p">,</span>
                                     <span class="n">packed_src</span><span class="p">,</span>
                                     <span class="n">query_vec</span><span class="p">,</span>
                                     <span class="n">attention_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">per_step_source_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">query_segment_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">per_step_source_padding</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">result_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span><span class="p">:</span>
        <span class="n">result_map</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                          <span class="n">source_key</span><span class="p">]</span><span class="o">.</span><span class="n">ComputeContextVectorWithSource</span><span class="p">(</span>
                              <span class="n">theta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">),</span>
                              <span class="n">packed_src</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span> <span class="n">query_vec</span><span class="p">,</span>
                              <span class="n">attention_state</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span>
                              <span class="k">if</span> <span class="n">attention_state</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                              <span class="n">per_step_source_padding</span><span class="p">,</span> <span class="n">query_segment_id</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CombineContext</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">result_map</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiSourceAttention._CombineContext"><a class="viewcode-back" href="../../../lingvo.core.attention.html#lingvo.core.attention.MultiSourceAttention._CombineContext">[docs]</a>  <span class="k">def</span> <span class="nf">_CombineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">context_map</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">):</span>
    <span class="n">ctxs</span> <span class="o">=</span> <span class="n">context_map</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
    <span class="n">combined_context</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atten_merger</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_merger</span><span class="p">,</span> <span class="p">[</span><span class="n">ctx</span> <span class="k">for</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">ctxs</span><span class="p">],</span>
                                <span class="n">query_vec</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">combined_context</span><span class="p">,</span>
        <span class="c1"># Return atten_probs of the primary source.</span>
        <span class="c1"># TODO(huk): Maybe return a NestedMap.</span>
        <span class="n">context_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">primary_source_key</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">({</span>
            <span class="n">src_key</span><span class="p">:</span> <span class="n">context_map</span><span class="p">[</span><span class="n">src_key</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">src_key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">source_atten_tpls</span>
        <span class="p">}))</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>