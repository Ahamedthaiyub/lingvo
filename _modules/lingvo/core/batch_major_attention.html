

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.batch_major_attention &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.batch_major_attention</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.batch_major_attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2020 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Multi-headed attention layers for Transformer machine translation.</span>

<span class="sd">[1] Attention is all you need.</span>
<span class="sd">    https://arxiv.org/pdf/1706.03762.pdf Section 3.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">bisect</span>
<span class="kn">from</span> <span class="nn">lingvo</span> <span class="kn">import</span> <span class="n">compat</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">attention_util</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">builder</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">computation_cost</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">conv_layers_builder</span> <span class="k">as</span> <span class="n">conv_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gpipe</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers_with_attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">moe_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">tshape</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># pylint: disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">inplace_ops</span>

<span class="c1"># pylint: enable=g-direct-tensorflow-import</span>


<div class="viewcode-block" id="CausalSegmentMask"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.CausalSegmentMask">[docs]</a><span class="k">def</span> <span class="nf">CausalSegmentMask</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the masks which combines causal masking and segment masks.</span>

<span class="sd">  Args:</span>
<span class="sd">    segment_ids: a tensor of shape [b, slen], the segment that each token</span>
<span class="sd">      belongs to.</span>
<span class="sd">    dtype: tf dtype.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of shape [b, 1, slen, slen].</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">assert</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span>
  <span class="c1"># of shape [b, t, t].</span>
  <span class="n">segment_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">slen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">causal_mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">slen</span><span class="p">,</span> <span class="n">slen</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">causal_mask</span> <span class="o">+</span> <span class="n">segment_mask</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
  <span class="n">min_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">dtype</span><span class="o">.</span><span class="n">max</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">combined_mask</span> <span class="o">*</span> <span class="n">min_value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="CausalPadding"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.CausalPadding">[docs]</a><span class="k">def</span> <span class="nf">CausalPadding</span><span class="p">(</span><span class="n">slen</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">slen</span><span class="p">,</span> <span class="n">slen</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetDtypeMin"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GetDtypeMin">[docs]</a><span class="k">def</span> <span class="nf">GetDtypeMin</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">dtype</span><span class="o">.</span><span class="n">max</span></div>


<div class="viewcode-block" id="SegmentMask"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.SegmentMask">[docs]</a><span class="k">def</span> <span class="nf">SegmentMask</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span>
                <span class="n">source_segment_id</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">apply_dtype_min</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculates a segment mask for attention.</span>

<span class="sd">  Args:</span>
<span class="sd">    segment_id: [B, T]</span>
<span class="sd">    source_segment_id: [B, S]</span>
<span class="sd">    dtype: data type of generated mask.</span>
<span class="sd">    apply_dtype_min: Outputs a 0/1 padding mask if set to False. This is needed</span>
<span class="sd">      for GPipe layers to avoid nan issues.</span>

<span class="sd">  Returns:</span>
<span class="sd">    segment_mask: [B, 1, T, S]: A mask that is ready to</span>
<span class="sd">    be added to [B, N, T, S] attention logits. if apply_dtype_min is False,</span>
<span class="sd">    outputs a 0/1 padding mask instead.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">segment_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">source_segment_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="c1"># Compute [B, T, S] = [B, T, 1] != [B, 1, S]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">apply_dtype_min</span><span class="p">:</span>
    <span class="n">ret</span> <span class="o">*=</span> <span class="n">GetDtypeMin</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="c1"># [B, T, S] -&gt; [B, 1, T, S]</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="PerDimScaleLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer">[docs]</a><span class="k">class</span> <span class="nc">PerDimScaleLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer to scale individual dims of the input.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="PerDimScaleLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `PerDimScaleLayer`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of individual dims .&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="PerDimScaleLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="PerDimScaleLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return theta.scale * inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: weights defined in this layer.</span>
<span class="sd">      inputs: 4D tensor with shape [..., p.dim]</span>

<span class="sd">    Returns:</span>
<span class="sd">      outpus: 4D tensor with shape [..., p.dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">dim</span> <span class="o">=</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>

      <span class="c1"># 1.0/tf.nn.softplus(0.0) = 1.442695041. Hard code this number so that we</span>
      <span class="c1"># can avoid unnecessary XLA op fusion mess on TPU.</span>
      <span class="n">r_softplus_0</span> <span class="o">=</span> <span class="mf">1.442695041</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">r_softplus_0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="o">*</span> <span class="n">r_softplus_0</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

      <span class="n">scale</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">inputs</span> <span class="o">*</span> <span class="n">scale</span></div>

<div class="viewcode-block" id="PerDimScaleLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PerDimScaleLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="MultiHeadedProjectionLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedProjectionLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Layer that computes multi heads projection.</span>

<span class="sd">    This layer is expected to be used within MultiHeadedAttention below.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedProjectionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for MultiHeadedProjectionLayer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Input dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dim_per_head&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Size of each head.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;is_output_projection&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether it is out projection or not. If False, we use &#39;</span>
        <span class="s1">&#39;&quot;BTD,DNH-&gt;BTNH&quot; for query,key,value projection. Otherwise we use &#39;</span>
        <span class="s1">&#39;&quot;BTNH,DNH-&gt;BTD&quot; for output projection.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;make_output_proj_no_op&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True no output projection is &#39;</span>
        <span class="s1">&#39;applied. This should be set with is_output_projection True and will &#39;</span>
        <span class="s1">&#39;raise an error otherwise. This does not effect input projections. &#39;</span>
        <span class="s1">&#39;This is useful in combining different types of attention heads where&#39;</span>
        <span class="s1">&#39;mixing is done after getting all the different attention outputs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;If to add bias in projection.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;xla_num_partitions&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of SPMD partitions.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="MultiHeadedProjectionLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">make_output_proj_no_op</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;make_output_proj_no_op must be used with output &#39;</span>
                       <span class="s1">&#39;projection set to True.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">make_output_proj_no_op</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
        <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">pc_bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">pc_bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedProjectionLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the multi headed projection for inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A tensor of shape [batch_size, time_steps, num_heads,</span>
<span class="sd">        dim_per_head] or [batch_size, time_steps, hidden_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      The projected tensor with shape [[batch_size, time_steps, hidden_size] or</span>
<span class="sd">      [batch_size, time_steps, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">make_output_proj_no_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">inputs</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">:</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">moe_layers</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">,</span> <span class="n">use_sharding_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                     <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">)])</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTNH,DNH-&gt;BTD&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)])</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTD,DNH-&gt;BTNH&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_output_projection</span><span class="p">:</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">moe_layers</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">,</span> <span class="n">use_sharding_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">+=</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span>
      <span class="k">return</span> <span class="n">ret</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dot-product attention with multiple attention heads.</span>

<span class="sd">  This implementation heavily uses einsum (wrapped in py_utils.Einsum) to be</span>
<span class="sd">  efficient on TPUs.  We use the following capital letters to denote certain</span>
<span class="sd">  tensor parameters.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S = length of the key/value (source)</span>
<span class="sd">    T = length of the query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head.</span>

<span class="sd">  The algorithm is sketched as follows. Each intermediate tensor or weight</span>
<span class="sd">  tensor is annotated with its shape. E.g., Wq, the weight tensor for query&#39;s</span>
<span class="sd">  projection, its shape is [D, N, H].</span>

<span class="sd">  Trainable weights:</span>
<span class="sd">    Wq, Wk, Wv: [D, N, H]</span>
<span class="sd">    Wout: [D, N, H]</span>

<span class="sd">  Input q:[B, T, D]; k:[B, S, D]; v:[B, S, D]</span>
<span class="sd">  q_proj:[B, T, N, H] = einsum(&#39;BTD,DNH-&gt;BTNH&#39;, x, Wq)</span>
<span class="sd">  k_proj:[B, S, N, H] = einsum(&#39;BSD,DNH-&gt;BSNH&#39;, x, Wk)</span>
<span class="sd">  v_proj:[B, S, N, H] = einsum(&#39;BSD,DNH-&gt;BSNH&#39;, x, Wv)</span>
<span class="sd">  logits:[B, N, T, S] = einsum(&#39;BTNH,BSNH-&gt;BNTS&#39;, q_proj, k_proj) / sqrt(H)</span>
<span class="sd">  probs:[B, N, T, S] = softmax(logits)</span>
<span class="sd">  context:[B, T, N, H] = einsum(&#39;BNTS,BSNH-&gt;BTNH&#39;, probs, v_proj)</span>
<span class="sd">  Output y:[B, T, D] = einsum(&#39;BTNH,DNH&gt;BTD&#39;, context, Wout)</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for _MultiHeadedAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of key nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of hidden nodes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Params for dropout layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_value_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether value v is pre-projected &#39;</span>
        <span class="s1">&#39; before self attention or not.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_per_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether using per_dim_scale or scaling by a constant factor.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;proj_tpl&#39;</span><span class="p">,</span> <span class="n">MultiHeadedProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Params for &#39;</span>
             <span class="s1">&#39;projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether there is packed input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use bias for projection layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;xla_num_partitions&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of SPMD partitions.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_scaling_code_motion&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Move scalings from the side &#39;</span>
        <span class="s1">&#39;of T^2 to the side of T for better performance. This may result &#39;</span>
        <span class="s1">&#39;in model quality drops when using bf16 for some models due to &#39;</span>
        <span class="s1">&#39;different XLA fusion decisions.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_extra_logit&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Extra logit for attention softmax.&#39;</span>
        <span class="s1">&#39;Notice None and 0 are different.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a _MultiHeadedAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="s1">&#39;input_dim is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="s1">&#39;hidden_dim is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="c1"># if proj_tpl does not have dim_per_head set, set it</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">dim_per_head</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">dim_per_head</span>

    <span class="k">def</span> <span class="nf">ProjectInput</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">,</span>
          <span class="n">xla_num_partitions</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">,</span>
          <span class="n">make_output_proj_no_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">ProjectInput</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span>
          <span class="s1">&#39;per_dim_scale&#39;</span><span class="p">,</span>
          <span class="n">PerDimScaleLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">dim_per_head</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_dropout&#39;</span><span class="p">,</span>
                     <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">))</span>
    <span class="c1"># Setting is_output_projection=True to set the projection direction</span>
    <span class="c1"># from hidden dim to input dim.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span>
        <span class="s1">&#39;post&#39;</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">is_output_projection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">,</span>
            <span class="n">xla_num_partitions</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">))</span>

<div class="viewcode-block" id="MultiHeadedAttention._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes attention logits.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query: A Tensor of shape [B, T, N, H]</span>
<span class="sd">      key: A Tensor of shape [B, T, N, H]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [B, N, T, S]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">AttenLogits</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># [s, b, n]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.AttenProbs"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.AttenProbs">[docs]</a>  <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query</span><span class="p">,</span>
                 <span class="n">key</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute attention probability.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S, N, H].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent attention</span>
<span class="sd">        between different segments. This is already been converted into large</span>
<span class="sd">        negative logits. Only applied if packed_input = True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, S] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      probs: [B, N, T, S].</span>
<span class="sd">      probs_sum: [B, N, T, 1].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">segment_mask</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">segment_mask</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">):</span>
      <span class="c1"># Keep softmax computation in float32 otherwise the low precision can</span>
      <span class="c1"># can lead to worse quality.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Apply segment mask.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">and</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Paddings have been included in segment_mask.</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">segment_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Exclude padding frames.</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">paddings</span> <span class="o">+=</span> <span class="n">per_step_padding</span>

      <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">enable_scaling_code_motion</span><span class="p">:</span>
      <span class="c1"># Split the softmax into two parts. Do the 1st part here; the 2nd part</span>
      <span class="c1"># (scaling) is moved after _AttenContext for better performance.</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">padded_logits</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
      <span class="n">probs_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">probs_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">probs_sum</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">),</span>
          <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">probs_sum</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">probs_sum</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContext">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">AttenContext</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._AttenContextOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContextOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContextOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SBNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._DotAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="n">paddings</span><span class="p">,</span>
                <span class="n">segment_mask</span><span class="p">,</span>
                <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Main attention function.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S, N, H].</span>
<span class="sd">      value:    [B, S, N, H].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent attention</span>
<span class="sd">        between different segments. This is already been converted into large</span>
<span class="sd">        negative logits. Only applied if packed_input = True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, S] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Scale the query projection.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="c1"># Compute prob with shape [batch, heads, target_time, source_time].</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;probs&#39;</span><span class="p">):</span>
      <span class="n">probs</span><span class="p">,</span> <span class="n">probs_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AttenProbs</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                                         <span class="n">segment_mask</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">)</span>
      <span class="c1"># Apply dropout to probs.</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_dropout</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Compute the attention context vector.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;ctx&#39;</span><span class="p">):</span>
      <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenContext</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_scaling_code_motion</span><span class="p">:</span>
        <span class="c1"># The 2nd part of the softamx --- scaling.</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">encoded</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">probs_sum</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">probs</span></div>

<div class="viewcode-block" id="MultiHeadedAttention._DotAttenOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAttenOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAttenOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">theta</span><span class="p">,</span>
                       <span class="n">query</span><span class="p">,</span>
                       <span class="n">key</span><span class="p">,</span>
                       <span class="n">value</span><span class="p">,</span>
                       <span class="n">paddings</span><span class="p">,</span>
                       <span class="n">segment_mask</span><span class="p">,</span>
                       <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">time_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dot attention function for queries with 1 time step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, 1, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      value:    [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      paddings: [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]: A mask that is applied to prevent attention</span>
<span class="sd">        between different segments. This is already been converted into large</span>
<span class="sd">        negative logits. Only applied if packed_input = True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, 1, S] if</span>
<span class="sd">        not None.</span>
<span class="sd">      time_step: Current time step.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, 1, N, H].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Scale the query projection.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">query</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">]),</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">*</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_LongSeq</span><span class="p">():</span>
      <span class="sd">&quot;&quot;&quot;For long sequence, directly apply to the entire tensor with padding.&quot;&quot;&quot;</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogitsOneStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">)</span>

      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pad</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span>
          <span class="n">padded_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>

      <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenContextOneStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_ShortSeq</span><span class="p">():</span>
      <span class="sd">&quot;&quot;&quot;For short sequence, using while loop for early exit.&quot;&quot;&quot;</span>

      <span class="k">def</span> <span class="nf">_AttenStep</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">ts</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes logits for attention prob for one step.</span>

<span class="sd">        Args:</span>
<span class="sd">          o: the output logits of shape [S, B*N]</span>
<span class="sd">          k: cached key of shape [S, B, N*H/128, 8]</span>
<span class="sd">          q: query of shape [B, N, H]</span>
<span class="sd">          ts: a scala tensor to represent time_step</span>

<span class="sd">        Returns:</span>
<span class="sd">          Updated logits and time steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span> <span class="o">*</span> <span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">ot</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">ts</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">_o</span><span class="p">,</span> <span class="n">_k</span><span class="p">,</span> <span class="n">_q</span><span class="p">,</span> <span class="n">ts</span><span class="p">:</span> <span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">time_step</span><span class="p">,</span>
          <span class="n">_AttenStep</span><span class="p">,</span>
          <span class="n">loop_vars</span><span class="o">=</span><span class="p">(</span><span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span> <span class="o">*</span> <span class="n">n</span><span class="p">],</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                       <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span>
                     <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)))</span>

      <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pad</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span>
          <span class="n">padded_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">_DotStep</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes encoded activation.</span>

<span class="sd">        Args:</span>
<span class="sd">          o: the output activation of shape [B, N, H]</span>
<span class="sd">          p: probabiliy of shape [S, B*N]</span>
<span class="sd">          v: cached value of shape [S, B, N*H/128, 8]</span>
<span class="sd">          ts: a scala tensor to represent time_step</span>

<span class="sd">        Returns:</span>
<span class="sd">          Updated output and time steps.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">o</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]),</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span> <span class="o">+</span> <span class="mi">1</span>

      <span class="n">encoded</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">o</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ts</span><span class="p">:</span> <span class="n">ts</span> <span class="o">&lt;=</span> <span class="n">time_step</span><span class="p">,</span>
          <span class="n">_DotStep</span><span class="p">,</span>
          <span class="n">loop_vars</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span>
                              <span class="n">probs</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span>
                                                                   <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_ShortSeq</span><span class="p">()</span> <span class="k">if</span> <span class="n">use_short_seq_opt</span> <span class="k">else</span> <span class="n">_LongSeq</span><span class="p">()</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">key_vec</span><span class="p">,</span>
            <span class="n">value_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: [B, T, D].</span>
<span class="sd">      key_vec:   [B, S, D].</span>
<span class="sd">      value_vec: [B, S, D].</span>
<span class="sd">      paddings:  [B, S].</span>
<span class="sd">      segment_mask: [B, 1, T, S]. A mask only applied if packed_input=True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, T] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, D].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Project inputs to key, value and query, respectively has shape</span>
    <span class="c1"># [B, S, N, H], [B, S, N, H], and [B, T, N, H].</span>
    <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">key_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">key_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="n">value_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">value_vec</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">dh</span> <span class="o">=</span> <span class="n">d</span> <span class="o">//</span> <span class="n">h</span>
      <span class="c1"># TODO(b/119531146): Reshape is inefficient here. Use one-hot matmul</span>
      <span class="c1"># avoids the data formatting. Change this back to reshape once XLA</span>
      <span class="c1"># has optimized reshape performance.</span>
      <span class="n">rhs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">//</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">dh</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">value_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
              <span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dh</span><span class="p">])</span>
      <span class="n">value_proj</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTD,DNH-&gt;BTNH&#39;</span><span class="p">,</span> <span class="n">value_vec</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">encoded</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAtten</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">,</span> <span class="n">key_proj</span><span class="p">,</span>
                                          <span class="n">value_proj</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">segment_mask</span><span class="p">,</span>
                                          <span class="n">per_step_padding</span><span class="p">)</span>
    <span class="c1"># Post projection</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">atten_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">atten_dim</span><span class="p">:</span>
      <span class="n">atten_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">atten_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="c1"># TODO(shafey): Determine if we want to make the cached shape 128 to</span>
    <span class="c1"># avoid padding and more efficient interpolation in beamsearch.</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_max_length</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">dim_per_head</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
            <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">value</span><span class="o">=</span><span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_max_length</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">dim_per_head</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
            <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    This function is used by autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:        [B, 1, D].</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   - [T, B,</span>
<span class="sd">        N, H]. value - [T, B, N, H].</span>
<span class="sd">      paddings:         [B, T], or None if there is no padding.</span>
<span class="sd">      segment_mask:     [B, 1, T, S] or None.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, 1, T] if</span>
<span class="sd">        not None.</span>
<span class="sd">      time_step: A scalar or tensor with [B], current decode step, 0-based. if</span>
<span class="sd">        it&#39;s a scalar, all the time step are the same decode step. if it&#39;s a</span>
<span class="sd">        tensor, it represents current decode step for each sample.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded:           [B, 1, D].</span>
<span class="sd">      updated_key_vec:   [T, B, N, H].</span>
<span class="sd">      updated_value_vec: [T, B, N, H].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value projection must be enabled for Transformer &#39;</span>
                       <span class="s1">&#39;machine translation.&#39;</span><span class="p">)</span>

    <span class="n">time_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>
    <span class="n">synced_time_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">time_step</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Project inputs to key, value and query. Each has shape [B, 1, N, H].</span>
    <span class="n">new_key_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">new_value_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># Using a if condtion, in case it&#39;s more efficient to update the same index.</span>
    <span class="k">if</span> <span class="n">synced_time_step</span><span class="p">:</span>
      <span class="c1"># The extended_key and extended_value have shape [T, B, N, H].</span>
      <span class="n">extended_key</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
          <span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_key_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
      <span class="n">extended_value</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
          <span class="n">cached_states</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_value_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># The extended_key and extended_value have shape [T, B, N, H].</span>
      <span class="n">selected_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">time_step</span> <span class="o">*</span> <span class="n">b</span>
      <span class="n">extended_key</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]),</span> <span class="n">selected_indices</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_key_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
      <span class="n">extended_value</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]),</span> <span class="n">selected_indices</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_value_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
      <span class="n">extended_key</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">extended_key</span><span class="p">,</span> <span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
      <span class="n">extended_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">extended_value</span><span class="p">,</span> <span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">updated_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">extended_key</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">extended_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenOneStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">query_proj</span><span class="p">,</span>
        <span class="n">extended_key</span><span class="p">,</span>
        <span class="n">extended_value</span><span class="p">,</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">segment_mask</span><span class="p">,</span>
        <span class="n">per_step_padding</span><span class="p">,</span>
        <span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Post projection.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">updated_state</span></div>

<div class="viewcode-block" id="MultiHeadedAttention.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttention.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># args[0]: [b, t, d], args[1]: [b, s, d], args[2]: [b, s, d],</span>
    <span class="c1"># args[3]: [b, s], args[4]: [b, t, s] if not None</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="c1"># O(b * t * s * d) computation for self-attention and there are four</span>
    <span class="c1"># projection layers, two of which has O(b * t * d^2), the other two has</span>
    <span class="c1"># O(b * s * d^2). Each multiple-sum took 2 flops. Approximately</span>
    <span class="c1"># self_attention took 15 flops per element since softmax is expensive.</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">15</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">s</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">s</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)))</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttentionXL"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttentionXL</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer-XL multiheaded attention with relative positional embedding.</span>

<span class="sd">  https://arxiv.org/pdf/1901.02860.pdf section 3.3.</span>

<span class="sd">  Notice this is only intended for self attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttentionXL.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_term_b&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, skip term_b in the paper section 3.3.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttentionXL object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalide rel_pos_emb_dim: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>

    <span class="n">emb_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">PositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_emb&#39;</span><span class="p">,</span> <span class="n">emb_params</span><span class="p">)</span>

    <span class="c1"># Projection layer for relative position encoding</span>
    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttentionXL._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">u_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="n">v_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="n">u_pc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">v_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionXL._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># This layer only supports self attention.</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># [1, 2T - 1]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relative_pos&#39;</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="c1"># [1, 2T - 1, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [2T - 1, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">AttenLogitsTransformerXL</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">,</span>
                                                     <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span>
                                                     <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionXL._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step. if it&#39;s a scalar, all the time step are the</span>
<span class="sd">        same decode step. if it&#39;s a tensor, it represents current decode step</span>
<span class="sd">        for each sample.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">synced_time_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">time_step</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="c1"># Transformer_XL relative attention.</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`time_step` can not be None when using relative &#39;</span>
                       <span class="s1">&#39;position encoding in attention.&#39;</span><span class="p">)</span>
    <span class="c1"># term a and c.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">synced_time_step</span><span class="p">:</span>
      <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># [b, s]</span>
      <span class="n">position</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
    <span class="c1"># [b, s, emb_dim]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span>
    <span class="c1"># [b, s, n, h]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">synced_time_step</span><span class="p">:</span>
      <span class="c1"># [s, n, h]</span>
      <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="c1"># term b an d.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,SNH-&gt;SN&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># term b an d.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,BSNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,BSNH-&gt;BSN&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionXL.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): support use_short_seq_opt for TransofrmerXL attention.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">use_short_seq_opt</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                              <span class="n">segment_mask</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
                              <span class="n">use_short_seq_opt</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MultiHeadedAttentionRPE"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadedAttentionRPE</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiheaded attention with relative positional embedding ...</span>

<span class="sd">  See https://arxiv.org/pdf/1803.02155.pdf.</span>

<span class="sd">  Notice this is only intended for self attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_radius&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Relative distance is clipped to [-radius, radius].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_value_emb&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If skipping value positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_global_emb&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;If using global relative positional embedding. Only effective if &#39;</span>
        <span class="s1">&#39;`rel_pos_emb_tpl` is not None.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a MultiHeadedAttentionRPE object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid rel_pos_radius: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span>

    <span class="n">rel_pos_emb_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">RelativePositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">radius</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_radius</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rel_pos_emb_dim</span> <span class="o">!=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
      <span class="c1"># Projection layer for relative position encoding</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
          <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key_emb&#39;</span><span class="p">,</span> <span class="n">rel_pos_emb_tpl</span><span class="p">)</span>
    <span class="c1"># Add projection layer if rel_pos_emb_dim is different from hidden_dim.</span>
    <span class="k">if</span> <span class="n">pos_proj_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;key_pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value_emb&#39;</span><span class="p">,</span> <span class="n">rel_pos_emb_tpl</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">pos_proj_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value_pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">reuse</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AUTO_REUSE</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">use_global_emb</span> <span class="k">else</span> <span class="kc">False</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;key_emb&#39;</span><span class="p">,</span> <span class="s1">&#39;key_pos_proj&#39;</span><span class="p">,</span> <span class="s1">&#39;value_emb&#39;</span><span class="p">,</span> <span class="s1">&#39;value_pos_proj&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">child</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">child</span><span class="p">]</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._RelativePositionValueEmb"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._RelativePositionValueEmb">[docs]</a>  <span class="k">def</span> <span class="nf">_RelativePositionValueEmb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets relative positional value embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      key: The attention key, a tensor of shape [batch, seqlen, dim]</span>

<span class="sd">    Returns:</span>
<span class="sd">      Relative positional embedding, a Tensor of shape</span>
<span class="sd">      [tgt_time=seqlen, src_time=seqlen, num_heads, attenion_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">emb_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_emb</span>
    <span class="n">emb_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">value_emb</span>

    <span class="n">seqlen</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">src_time_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">seqlen</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tgt_time_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">])</span>

    <span class="c1"># [tgt_time=T, src_time=T, num_heads x hidden_dim]</span>
    <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">emb_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">emb_theta</span><span class="p">,</span> <span class="n">src_time_indices</span> <span class="o">-</span> <span class="n">tgt_time_indices</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">tgt_time</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">)</span>

    <span class="n">pos_proj_layer</span> <span class="o">=</span> <span class="s1">&#39;value_pos_proj&#39;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">)</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="nb">getattr</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">pos_proj_layer</span><span class="p">),</span> <span class="n">pos_emb</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">pos_emb</span><span class="p">,</span>
          <span class="p">[</span><span class="n">tgt_time</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">])</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): optimize it.</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># This layer only supports self attention.</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># [1, 2T - 1]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, 2T - 1, rel_pos_emb_dim]</span>
    <span class="n">abs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;key_pos_proj&#39;</span><span class="p">):</span>
      <span class="c1"># [1, 2T - 1, N, H]</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="p">,</span> <span class="n">abs_emb</span><span class="p">)</span>
      <span class="c1"># [2T - 1, N, H]</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">abs_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">abs_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">abs_emb</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">AttenLogitsRPE</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">abs_emb</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># Transformer_XL relative attention.</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`time_step` can not be None when using relative &#39;</span>
                       <span class="s1">&#39;position encoding in attention.&#39;</span><span class="p">)</span>
    <span class="c1"># Gets positional embedding.</span>
    <span class="c1"># [1, S]</span>
    <span class="n">rel_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, S, rel_pos_emb_dim]</span>
    <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_emb</span><span class="o">.</span><span class="n">FPropDefaultTheta</span><span class="p">(</span><span class="n">rel_dists</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;key_pos_proj&#39;</span><span class="p">):</span>
      <span class="c1"># [1, S, N, H]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key_pos_proj</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
      <span class="c1"># [S, 1, N, H]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span>
                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContext">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): optimize it.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNij,BjNH-&gt;BiNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
      <span class="n">encoded</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNij,ijNH-&gt;BiNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span>
                           <span class="bp">self</span><span class="o">.</span><span class="n">_RelativePositionValueEmb</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">encoded</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE._AttenContextOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContextOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContextOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SBNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">skip_value_emb</span><span class="p">:</span>
      <span class="c1"># [1, S]</span>
      <span class="n">rel_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
      <span class="c1"># [1, S, rel_pos_emb_dim]</span>
      <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value_emb</span><span class="p">,</span> <span class="n">rel_dists</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;value_pos_proj&#39;</span><span class="p">):</span>
        <span class="c1"># [1, S, N, H]</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value_pos_proj</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;SBN,SNH-&gt;BNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">pos_emb</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># TODO(jamesqin): support use_short_seq_opt.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">use_short_seq_opt</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                              <span class="n">segment_mask</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
                              <span class="n">use_short_seq_opt</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiHeadedAttentionRPE.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LocalSelfAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention">[docs]</a><span class="k">class</span> <span class="nc">LocalSelfAttention</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Dot-product causal self attention using a sliding window.</span>

<span class="sd">  We use the following capital letters to denote certain</span>
<span class="sd">  tensor parameters.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S=T= length of the key/value (source) and query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head</span>
<span class="sd">    W = block size</span>
<span class="sd">    L = left context size, including left L-1 positions and self</span>
<span class="sd">    R = right context size</span>
<span class="sd">    F = L + R = context size of one position.</span>
<span class="sd">    C = L + R + W - 1 = context size of a block of W positions.</span>
<span class="sd">    U = ceiling(T/W).</span>

<span class="sd">  The key difference to base class is on calculating logits:</span>
<span class="sd">    Base class:</span>
<span class="sd">      1)  Compute the full S x T attention.</span>
<span class="sd">      2)  Apply a S x T mask to enforce local attention window.</span>
<span class="sd">    This implementation:</span>
<span class="sd">      1)  Compute a W x C attention for each of the U blocks. Where the i-th</span>
<span class="sd">      block has query[W*i:W*(i+1)] and key[W*(i-1)-L-1:W*(i+1)+R].</span>
<span class="sd">      2)  Apply a W x C mask for each block.</span>

<span class="sd">  Effectively, we reduce both time and space complexities for computing the</span>
<span class="sd">  sliding window attention from O(S * T) to O(S * C). In practice we observe</span>
<span class="sd">  reduced HBM usage on TPU but no speed gains.</span>

<span class="sd">  Note: Cross attention is not supported. As a result in speech models this</span>
<span class="sd">  class can only be used for encoder.</span>

<span class="sd">  TODO(weihan): add masking based local attention to the base class.</span>

<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocalSelfAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for LocalSelfAttention.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Size of a processing block, if unset, default to &#39;</span>
        <span class="s1">&#39;max(1, left_context-1).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;left_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of left positions to attend &#39;</span>
        <span class="s1">&#39;(including current position).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;right_context&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of right positions to attend.&#39;</span><span class="p">)</span>

    <span class="c1"># The following are for streaming inference only.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;inference_step_max_length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Max inference step length &#39;</span>
        <span class="s1">&#39;(query_vec length). Used for efficient sunn inference on tpu. In case &#39;</span>
        <span class="s1">&#39;inference seq length is not static, set to None or negative, and a &#39;</span>
        <span class="s1">&#39;less optimized algorithm is used.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_3d_recurrent_state&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, recurrent state for streaming inference is [B, T, N*H] &#39;</span>
        <span class="s1">&#39;instead of [B, T, N, H]. This is for performance optimization &#39;</span>
        <span class="s1">&#39;and does not change math. Only effective if inference_step_max_length &#39;</span>
        <span class="s1">&#39;is not None and &gt; 0.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a LocalSelfAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Left context should be at least one.&#39;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;block_size not set, use default value </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">))</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span> <span class="s1">&#39;Packed input not implemented yet.&#39;</span>

<div class="viewcode-block" id="LocalSelfAttention._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUTNH,BUSNH-&gt;BNUTS&#39;</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention.AttenProbs"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.AttenProbs">[docs]</a>  <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query</span><span class="p">,</span>
                 <span class="n">key</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute attention probability.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, T, N, H].</span>
<span class="sd">      key:      [B, S=T, N, H].</span>
<span class="sd">      paddings: [B, T].</span>
<span class="sd">      segment_mask: [B, 1, T, S] not used right now.</span>
<span class="sd">      per_step_padding: Not used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      probs: [B, U, N, W, 2 * W]</span>
<span class="sd">      probs_sum: [B, U, N, W, 1].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">per_step_padding</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">])</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

    <span class="c1"># -&gt; [B, U, C, N, H]</span>
    <span class="n">key_block_context</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">key</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key_block_context</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, U, W, N, H]</span>
    <span class="n">query_blocks</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">ConvertToBlocks</span><span class="p">(</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_blocks</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, U, C]</span>
    <span class="n">paddings_block_context</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">,</span>
        <span class="n">padding_val</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># -&gt; [B, N, U, W, C]</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">paddings_block_context</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Make local causal paddings.</span>
    <span class="c1"># -&gt; [U, W, C]</span>
    <span class="n">local_causal_padding</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">MakeCausalPadding</span><span class="p">(</span>
        <span class="n">seq_len</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">paddings</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">+=</span> <span class="n">local_causal_padding</span>

    <span class="c1"># -&gt; [B, N, U, W, C]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttenLogits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_blocks</span><span class="p">,</span> <span class="n">key_block_context</span><span class="p">)</span>

    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_scaling_code_motion</span><span class="p">:</span>
      <span class="c1"># Split the softmax into two parts. Do the 1st part here; the 2nd part</span>
      <span class="c1"># (scaling) is moved after _AttenContext for better performance.</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">padded_logits</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">probs</span><span class="p">),</span> <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">probs_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">),</span>
          <span class="n">key</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">probs_sum</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">probs_sum</span></div>

<div class="viewcode-block" id="LocalSelfAttention._AttenContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._AttenContext">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the local attention context vector.</span>

<span class="sd">    Args:</span>
<span class="sd">     theta: Layer theta: NestedMap.</span>
<span class="sd">     probs: Local-self-MultiHeaded Attention probablities: [B, N, U, W, C].</span>
<span class="sd">     value: Input value vector: [B, S=T, N, H].</span>

<span class="sd">    Returns:</span>
<span class="sd">     encoded: Attention context vector: [B, T, N, H].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># -&gt; [B, U, C, N, H]</span>
    <span class="n">value_block_context</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">ExtractBlockContext</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">block_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">)</span>

    <span class="c1"># Compute the attention context vector.</span>
    <span class="c1"># -&gt; [B, U, W, N, H]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNUWC,BUCNH-&gt;BUWNH&#39;</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">value_block_context</span><span class="p">)</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">u</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="c1"># Remove the extra time padding introduced by converting to blocks.</span>
    <span class="c1"># Note: t0 works presently only for self-attention.</span>
    <span class="c1"># For cross-atten, needs query[1] which&#39;ll be different.</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">value</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encoded</span><span class="p">[:,</span> <span class="p">:</span><span class="n">t0</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">encoded</span></div>

<div class="viewcode-block" id="LocalSelfAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">key_vec</span><span class="p">,</span>
            <span class="n">value_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the current query output.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: [B, T, D].</span>
<span class="sd">      key_vec:   [B, S, D] with S == T (self-attention).</span>
<span class="sd">      value_vec: [B, S, D] with S == T (self-attention).</span>
<span class="sd">      paddings:  [B, S] with S == T (self-attention).</span>
<span class="sd">      segment_mask: [B, 1, T, S]. A mask only applied if packed_input=True.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, T, T] if</span>
<span class="sd">        not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, D].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># LocalSelfAttention doesn&#39;t support cross-attention at the moment.</span>
    <span class="c1"># Verify T == S, for query and value vector.</span>
    <span class="n">value_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">value_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">d</span><span class="p">])</span>
    <span class="n">key_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">key_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">d</span><span class="p">])</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">query_vec</span><span class="p">,</span>
        <span class="n">key_vec</span><span class="p">,</span>
        <span class="n">value_vec</span><span class="p">,</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
        <span class="n">per_step_padding</span><span class="o">=</span><span class="n">per_step_padding</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    This function is used by autoregressive decoding, as opposed to</span>
<span class="sd">    StreamStep which is for single step self attention.</span>

<span class="sd">    Note: When the context window size is much smaller than target sequence</span>
<span class="sd">    length, to make it run more efficent, T below can be just the window size.</span>
<span class="sd">    Then, time_step should be the relative decode step and not bigger than T.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:        [B, 1, D].</span>
<span class="sd">      cached_states:   A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   - [T, B,</span>
<span class="sd">        N, H]. value - [T, B, N, H].</span>
<span class="sd">      paddings:         [B, T], or None if there is no padding.</span>
<span class="sd">      segment_mask:     [B, 1, T, S] or None. Not used right now.</span>
<span class="sd">      per_step_padding: A mask used by decoder self-attention to prevent</span>
<span class="sd">        information flow from future (causal padding). It has shape [B, 1, T] if</span>
<span class="sd">        not None. Not used right now.</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization. Not</span>
<span class="sd">        supported right now.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded:           [B, 1, D].</span>
<span class="sd">      updated_key_vec:   [T, B, N, H].</span>
<span class="sd">      updated_value_vec: [T, B, N, H].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If right_context is non-zero.</span>
<span class="sd">      NotImplementedError: If use_short_seq_opt is true.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Right context must be zero for autoregressive decoding.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_short_seq_opt</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;use_short_seq_opt is not supported yet.&#39;</span><span class="p">)</span>

    <span class="c1"># Make local causal paddings, which have shape [B, T].</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">position_diff</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">time_step</span>
    <span class="n">valid_atten</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">position_diff</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span>
                                      <span class="n">position_diff</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">local_causal_padding</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">valid_atten</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">+=</span> <span class="n">local_causal_padding</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span>
                              <span class="n">segment_mask</span><span class="p">,</span> <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
                              <span class="n">use_short_seq_opt</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention.zero_state"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initial state given the batch size.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_state_static_length</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_state_dynamic_length</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention._zero_state_static_length"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_static_length">[docs]</a>  <span class="k">def</span> <span class="nf">_zero_state_static_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initial state given the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` object containing</span>

<span class="sd">      - key: [B, p.inference_step_max_length + p.left_context - 1, N, H] or</span>
<span class="sd">        [..., N * H] if p.use_3d_recurrent_state.</span>
<span class="sd">      - value: [B, p.inference_step_max_length + p.left_context - 1, N, H] or</span>
<span class="sd">        [..., N * H] if p.use_3d_recurrent_state.</span>
<span class="sd">      - masks: [B, p.inference_step_max_length + p.left_context-1]. A 0/1</span>
<span class="sd">        Tensor where 0s are masked out positions.</span>
<span class="sd">      - tail: [B, 1], currently only effective if use_3d_recurrent_state is</span>
<span class="sd">        True, the tail pointer to key, value and paddings circular buffers.</span>
<span class="sd">        Value range is [0, p.inference_step_max_length + p.left_context - 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">,</span> <span class="s1">&#39;Value projection must be enabled.&#39;</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;StreamStep does not support look-ahead&#39;</span>

    <span class="n">context_len</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
      <span class="n">key_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">],</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">key_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">],</span>
                           <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">value_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">key_state</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="c1"># At the beginning, all positions are masked out.</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">],</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">tail</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">tail</span><span class="o">=</span><span class="n">tail</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention._zero_state_dynamic_length"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_dynamic_length">[docs]</a>  <span class="k">def</span> <span class="nf">_zero_state_dynamic_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initial state given the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` object containing</span>

<span class="sd">      - key: [B, p.left_context - 1, N, H].</span>
<span class="sd">      - value: [B, p.left_context - 1, N, H].</span>
<span class="sd">      - masks: [B, p.left_context-1]. Tensor where 0s are masked out positions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">,</span> <span class="s1">&#39;Value projection must be enabled.&#39;</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;StreamStep does not support look-ahead&#39;</span>

    <span class="n">context_len</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">key_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">],</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">value_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">key_state</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="c1"># At the beginning, all positions are masked out.</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">],</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">key_state</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value_state</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention.StreamStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.StreamStep">[docs]</a>  <span class="k">def</span> <span class="nf">StreamStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    This differs from ExtendStep() which requires key/value seq lengths being</span>
<span class="sd">    known in advance.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of layer params.</span>
<span class="sd">      query_vec: A query vector of shape [B, Q, D].</span>
<span class="sd">      paddings: A 0/1 valued tensor of shape [B, Q].</span>
<span class="sd">      state0: A NestedMap of the same structure as returned by zero_state().</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: Output of the given query vector with shape [B, Q, D].</span>
<span class="sd">      padding: the same as input paddings.</span>
<span class="sd">      state1: Updated state of the same structure as state0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">,</span> <span class="s1">&#39;Value projection must be enabled.&#39;</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;StreamStep() does not support look ahead.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_StreamStepStaticLength</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_StreamStepDynamicLength</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttention._StreamStepStaticLength"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepStaticLength">[docs]</a>  <span class="k">def</span> <span class="nf">_StreamStepStaticLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;query_vec length is staticly known.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Sanity checks.</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">query_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;query_vec.shape[1] must be static.&#39;</span>
    <span class="k">assert</span> <span class="n">q</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;q: </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="s1"> should be less than p.inference_step_max_length: &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">q</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">state0</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">n</span> <span class="o">*</span> <span class="n">h</span><span class="p">])</span>
    <span class="n">state0</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                     <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">key</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">/StreamStep&#39;</span><span class="p">):</span>
      <span class="c1"># query projection.</span>
      <span class="c1"># [B, Q, N, H]</span>
      <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
        <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">query_proj</span> <span class="o">*=</span> <span class="n">h</span><span class="o">**-</span><span class="mf">0.5</span>

      <span class="k">def</span> <span class="nf">get_update_indices</span><span class="p">():</span>  <span class="c1"># pylint:disable=invalid-name</span>
        <span class="c1"># The following computes locations to update in the circular buffer.</span>
        <span class="c1"># [b, 1]</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># [b, q]</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="p">])</span>
        <span class="c1"># [1, q]</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># [b, q]</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="c1"># [b, q]</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">cols</span> <span class="o">+</span> <span class="n">state0</span><span class="o">.</span><span class="n">tail</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="c1"># [b, q, 2]</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span>
                         <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

      <span class="n">indices</span> <span class="o">=</span> <span class="n">get_update_indices</span><span class="p">()</span>

      <span class="k">def</span> <span class="nf">get_next_state</span><span class="p">(</span><span class="n">recur_state</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>  <span class="c1"># pylint:disable=invalid-name</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
          <span class="n">next_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">recur_state</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
          <span class="c1"># [B, S, N, H]</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># [B, S, N, H]</span>
          <span class="n">next_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">recur_state</span><span class="p">,</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="n">s</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">next_state</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
        <span class="c1"># [B, Q, N * H]</span>
        <span class="n">incr_key</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="s1">&#39;DH,BTD-&gt;BTH&#39;</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">n</span> <span class="o">*</span> <span class="n">h</span><span class="p">]),</span>
            <span class="n">query_vec</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># [B, Q, N * H]</span>
        <span class="n">incr_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="s1">&#39;DH,BTD-&gt;BTH&#39;</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">n</span> <span class="o">*</span> <span class="n">h</span><span class="p">]),</span>
            <span class="n">query_vec</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># [B, Q, N, H]</span>
        <span class="n">incr_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
        <span class="c1"># [B, Q, N, H]</span>
        <span class="n">incr_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="c1"># [B, S, N, H], [B, S, N, H] or [B, S, N * H] if use_3d_recurrent_state.</span>
      <span class="n">key</span><span class="p">,</span> <span class="n">next_key</span> <span class="o">=</span> <span class="n">get_next_state</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">incr_key</span><span class="p">)</span>
      <span class="c1"># [B, S, N, H], [B, S, N, H] or [B, S, N * H] if use_3d_recurrent_state.</span>
      <span class="n">value</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="n">get_next_state</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">incr_value</span><span class="p">)</span>
      <span class="c1"># [B, 1]</span>
      <span class="n">next_tail</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">tail</span> <span class="o">+</span> <span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

      <span class="c1"># paddings</span>
      <span class="c1"># [B, S]. 1s are masked positions.</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
        <span class="n">new_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">masks</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span>
                                                <span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">new_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">state0</span><span class="o">.</span><span class="n">masks</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="n">s</span><span class="p">:]</span>

      <span class="c1"># [B, Q, N, T]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BQNH,BTNH-&gt;BQNT&#39;</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

      <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;compute_padding&#39;</span><span class="p">):</span>
        <span class="c1"># Generate local atten mask.</span>
        <span class="c1"># [Q, 1]</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># [1, S]</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 1s are masked positions.</span>
        <span class="c1"># [Q, S]</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">cols</span> <span class="o">-</span> <span class="n">rows</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_recurrent_state</span><span class="p">:</span>
          <span class="c1"># [B, 1]</span>
          <span class="n">head</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">state0</span><span class="o">.</span><span class="n">tail</span> <span class="o">-</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">s</span><span class="p">)</span>
          <span class="c1"># [B, Q, S]</span>
          <span class="n">shifted_distance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># [Q, S]</span>
          <span class="n">shifted_distance</span> <span class="o">=</span> <span class="n">distance</span> <span class="o">-</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">inference_step_max_length</span> <span class="o">-</span> <span class="n">q</span><span class="p">)</span>
        <span class="c1"># [B, Q, S] or [Q, S]</span>
        <span class="n">local_atten_per_step_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">shifted_distance</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                           <span class="n">shifted_distance</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">shifted_distance</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">shifted_distance</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)))</span>
        <span class="c1"># [1, Q, S] or [B, Q, S]</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetRank</span><span class="p">(</span><span class="n">local_atten_per_step_paddings</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
          <span class="n">local_atten_per_step_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
              <span class="n">local_atten_per_step_paddings</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># [B, 1, S]</span>
        <span class="n">expanded_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">new_masks</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># [B, Q, S]</span>
        <span class="n">final_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">expanded_masks</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">local_atten_per_step_paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
        <span class="c1"># [B, Q, 1, S]</span>
        <span class="n">final_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">final_paddings</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

      <span class="c1"># [B, Q, N, S]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span>
                         <span class="n">final_paddings</span><span class="p">)</span> <span class="o">+</span> <span class="n">very_negative_logits</span> <span class="o">*</span> <span class="n">final_paddings</span>
      <span class="c1"># [B, Q, N, S]</span>
      <span class="n">posteriors</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span>
          <span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">)</span>
      <span class="c1"># [B, Q, N, H]</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BQNS,BSNH-&gt;BQNH&#39;</span><span class="p">,</span> <span class="n">posteriors</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

      <span class="c1"># Post projection.</span>
      <span class="c1"># [B, Q, D]</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

      <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">key</span><span class="o">=</span><span class="n">next_key</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">next_value</span><span class="p">,</span> <span class="n">masks</span><span class="o">=</span><span class="n">new_masks</span><span class="p">,</span> <span class="n">tail</span><span class="o">=</span><span class="n">next_tail</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state1</span></div>

<div class="viewcode-block" id="LocalSelfAttention._StreamStepDynamicLength"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepDynamicLength">[docs]</a>  <span class="k">def</span> <span class="nf">_StreamStepDynamicLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;query_vec length is dynamic.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Sanity checks.</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">unused_n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">unused_s</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">q</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">/StreamStep&#39;</span><span class="p">):</span>
      <span class="c1"># query projection.</span>
      <span class="c1"># [B, Q, N, H]</span>
      <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">:</span>
        <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">per_dim_scale</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">query_proj</span> <span class="o">*=</span> <span class="n">h</span><span class="o">**-</span><span class="mf">0.5</span>

      <span class="c1"># key, value, mask.</span>
      <span class="c1"># [B, S, N, H].</span>
      <span class="n">key</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">state0</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># [B, S, N, H]</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">state0</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># [B, S]. 1s are masked positions.</span>
      <span class="n">state_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">state0</span><span class="o">.</span><span class="n">masks</span><span class="p">,</span> <span class="n">paddings</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">state_paddings</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

      <span class="c1"># [B, Q, N, T]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BQNH,BTNH-&gt;BQNT&#39;</span><span class="p">,</span> <span class="n">query_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

      <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;compute_padding&#39;</span><span class="p">):</span>
        <span class="c1"># Generate local atten mask.</span>
        <span class="c1"># [Q, 1]</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># [1, T]</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 1s are masked positions.</span>
        <span class="c1"># [Q, S]</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">-</span> <span class="n">rows</span>
        <span class="n">local_atten_per_step_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">distance</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">distance</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)))</span>
        <span class="c1"># [1, Q, T]</span>
        <span class="n">local_atten_per_step_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">local_atten_per_step_paddings</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># [B, 1, T]</span>
        <span class="n">expanded_state_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state_paddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># [B, Q, T]</span>
        <span class="n">final_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">expanded_state_paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">local_atten_per_step_paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
        <span class="c1"># [B, Q, 1, T]</span>
        <span class="n">final_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">final_paddings</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

      <span class="c1"># [B, Q, N, T]</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span>
                         <span class="n">final_paddings</span><span class="p">)</span> <span class="o">+</span> <span class="n">very_negative_logits</span> <span class="o">*</span> <span class="n">final_paddings</span>
      <span class="c1"># [B, Q, N, T]</span>
      <span class="n">posteriors</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span>
          <span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_extra_logit</span><span class="p">)</span>
      <span class="c1"># [B, Q, N, H]</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BQNT,BTNH-&gt;BQNH&#39;</span><span class="p">,</span> <span class="n">posteriors</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

      <span class="c1"># Post projection.</span>
      <span class="c1"># [B,Q,D]</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

      <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">[:,</span> <span class="o">-</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):,</span> <span class="p">:,</span> <span class="p">:],</span>
          <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">[:,</span> <span class="o">-</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):,</span> <span class="p">:,</span> <span class="p">:],</span>
          <span class="n">masks</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">state_paddings</span><span class="p">[:,</span> <span class="o">-</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):])</span>
      <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state1</span></div>

<div class="viewcode-block" id="LocalSelfAttention.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttention.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LocalSelfAttentionXL"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL">[docs]</a><span class="k">class</span> <span class="nc">LocalSelfAttentionXL</span><span class="p">(</span><span class="n">LocalSelfAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Local causal version of transformer-xl self attention.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocalSelfAttentionXL.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Dimension of relative positional embedding.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_term_b&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, skip term_b in the paper section 3.3.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a LocalSelfAttentionXL object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid rel_pos_emb_dim: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>

    <span class="n">emb_params</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">PositionalEmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_emb&#39;</span><span class="p">,</span> <span class="n">emb_params</span><span class="p">)</span>

    <span class="c1"># Projection layer for relative position encoding</span>
    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">pos_proj_tpl</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;pos_proj&#39;</span><span class="p">,</span> <span class="n">pos_proj_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="LocalSelfAttentionXL._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">u_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="n">v_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="n">u_pc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">v_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocalSelfAttentionXL._AttenLogits"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">left_context</span>
    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">right_context</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="n">r</span>
    <span class="c1"># term a and c</span>
    <span class="n">term_ac</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUTNH,BUSNH-&gt;BNUTS&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="c1"># term b and d</span>
    <span class="c1"># [1, F]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="c1"># [1, F, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [F, N, H]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
      <span class="c1"># [B, N, U, W, F]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BUWNH,FNH-&gt;BNUWF&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>

      <span class="c1"># Perform relative shift in order to get [B, N, U, W, C]</span>
      <span class="c1"># Pads the input to [B, N, U, C, C+1]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span>
                       <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">w</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)))</span>

      <span class="c1"># Reshapes to [B, N, U, C+1, C]. Note the output last dim is 1-smaller</span>
      <span class="c1"># than the input, which &quot;pushses&quot; one element off to the next row for each</span>
      <span class="c1"># row. The accumulated effect is row_i is right-shifted i steps (i&gt;=0).</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

      <span class="c1"># Keeps useful slices. [B, N, U, W, C]</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">term_bd</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># [N, F]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,FNH-&gt;NF&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
      <span class="c1"># [N, W, F]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># [N, C, C+1]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">w</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)))</span>
      <span class="c1"># [N, C+1, C]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
      <span class="c1"># Keeps useful slices. [N, W, C]</span>
      <span class="n">term_d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">term_bd</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">term_d</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">term_ac</span> <span class="o">+</span> <span class="n">term_bd</span></div>

<div class="viewcode-block" id="LocalSelfAttentionXL._AttenLogitsOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogitsOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_AttenLogitsOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention logits for one single target (query) step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, N, H].</span>
<span class="sd">      key:      [S, B, N, H] or [S, B, N*H/128, 128].</span>
<span class="sd">      time_step: Current time step.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [S, B, N]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">n</span>

    <span class="c1"># Transformer_XL relative attention.</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`time_step` can not be None when using relative &#39;</span>
                       <span class="s1">&#39;position encoding in attention.&#39;</span><span class="p">)</span>
    <span class="c1"># term a and c.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SBNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">u</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">time_step</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># [1, s, emb_dim]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">FPropWithPosition</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_proj</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">pos_proj</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="c1"># [s, n, h]</span>
    <span class="n">sin_emb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sin_emb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># term b an d.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_term_b</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNH,SNH-&gt;SBN&#39;</span><span class="p">,</span> <span class="n">query</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;NH,SNH-&gt;SN&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">sin_emb</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="LocalSelfAttentionXL.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="LocalSelfAttentionXL.zero_state"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="LocalSelfAttentionXL.StreamStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LocalSelfAttentionXL.StreamStep">[docs]</a>  <span class="k">def</span> <span class="nf">StreamStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span></div></div>


<div class="viewcode-block" id="RoutingAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention">[docs]</a><span class="k">class</span> <span class="nc">RoutingAttention</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;&quot;Implements a sparse attention based on k-means clustering.</span>

<span class="sd">  This is used in the routing transformer https://arxiv.org/pdf/2003.05997.</span>

<span class="sd">  This verison of multi-headed attention differs from the full attention</span>
<span class="sd">  in that it uses k-means clusterting to cluster the queries and keys first,</span>
<span class="sd">  and each query only attend to a subset of keys that are close to the centroid</span>
<span class="sd">  closest to that query. As Euclidean distance is used to determine closeness,</span>
<span class="sd">  we layer normalize queries and keys first so that closeness lead to a larger</span>
<span class="sd">  dot product.</span>

<span class="sd">  TODO(zhouwk) This class is missing the following features:</span>
<span class="sd">    * propagate clustering loss;</span>
<span class="sd">    * supporting packed inputs;</span>
<span class="sd">    * support attention dropout;</span>
<span class="sd">    * support relative position encoding;</span>
<span class="sd">    * support using local attention on some heads.</span>

<span class="sd">  We use the following capital letters to denote shape parameters:</span>
<span class="sd">    B = batch size</span>
<span class="sd">    S = length of the source sequence</span>
<span class="sd">    T = length of the target sequence</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head</span>
<span class="sd">    D = model dimension</span>

<span class="sd">    K = number of clusters</span>
<span class="sd">    W = attention window</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="RoutingAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_clusters&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of clusters, typically around the square&#39;</span>
        <span class="s1">&#39; root of the sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_window&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;The number of keys each query attends to.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clustering&#39;</span><span class="p">,</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">KMeansClusteringForAtten</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;The params for a clustering layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;causal_masking&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether causal masking is enabled. When set, a query at position idx &#39;</span>
        <span class="s1">&#39;is only allowed to attend to keys/values at positions &lt;= idx.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fast_path&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;Whether to use a more efficient implementation. The fast path is &#39;</span>
        <span class="s1">&#39;signanificantly faster by grouping queries when determining which &#39;</span>
        <span class="s1">&#39;values to attend to (which might leave out some queries or duplicate &#39;</span>
        <span class="s1">&#39;others); fast_path=False computes this per each query.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;query_group_size_factor&#39;</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="s1">&#39;Only used when p.fast_path=True. When grouping queries, we make the &#39;</span>
        <span class="s1">&#39;group size larger by this multiplier to not leave out any queries due &#39;</span>
        <span class="s1">&#39;to potential cluster imbalance.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an instance of RoutingAttention.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_window</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>

    <span class="n">clustering_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">clustering</span>
    <span class="n">clustering_p</span><span class="o">.</span><span class="n">num_clusters</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span>
    <span class="n">clustering_p</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">clustering_p</span><span class="o">.</span><span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="c1"># We normalize manually prior so that we can reuse the same normalized</span>
    <span class="c1"># query/key to compute attention probs later.</span>
    <span class="n">clustering_p</span><span class="o">.</span><span class="n">apply_layer_norm</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;clustering&#39;</span><span class="p">,</span> <span class="n">clustering_p</span><span class="p">)</span>

<div class="viewcode-block" id="RoutingAttention._DotAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention._DotAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="n">paddings</span><span class="p">,</span>
                <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">query_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the attention.</span>

<span class="sd">    Each query selects &#39;p.attention_window&#39; number of keys to attend to. First</span>
<span class="sd">    we find the closest centroid to that query, and we only allow that query to</span>
<span class="sd">    attend to the &#39;p.attention_window&#39; closest keys to that centroid.</span>

<span class="sd">    In order to use K-means, this implementation applies layer normalization</span>
<span class="sd">    to both the queries and the keys, and uses the normalized results to compute</span>
<span class="sd">    attention weights.</span>

<span class="sd">    When &#39;p.attention_window&#39; is the source length, this should evalue to the</span>
<span class="sd">    full attention (using layer normalized queries and keys).</span>

<span class="sd">    The caller should pass in the paddings for both &#39;key&#39; and &#39;query&#39; because</span>
<span class="sd">    during training, when we update the clustering we need to know the paddings</span>
<span class="sd">    for both. (For the inference path only &#39;key_paddings&#39; is useful.)</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` of the values of this layer&#39;s weights.</span>
<span class="sd">      query: [B, T, N, H].</span>
<span class="sd">      key:   [B, S, N, H].</span>
<span class="sd">      value: [B, S, N, H].</span>
<span class="sd">      paddings:   [B, S], paddings for key.</span>
<span class="sd">      segment_mask: must be None.</span>
<span class="sd">      per_step_padding: must be None. Please use p.causal_masking.</span>
<span class="sd">      query_paddings: [B, T], or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, T, N, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Requires segment_mask=None and per_step_padding=None.&#39;</span><span class="p">)</span>
    <span class="n">key_paddings</span> <span class="o">=</span> <span class="n">paddings</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query_paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">query_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">key_paddings</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">is_self_attention</span> <span class="o">=</span> <span class="p">(</span><span class="n">query</span> <span class="ow">is</span> <span class="n">key</span><span class="p">)</span>
    <span class="c1"># Whether to update the centroids. Only do this during training.</span>
    <span class="n">update</span> <span class="o">=</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">KMeansClusteringForAtten</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="c1"># [B, T, N, K]</span>
    <span class="n">q_dists</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">clustering</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query_paddings</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="n">update</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_self_attention</span><span class="p">:</span>
      <span class="n">key</span> <span class="o">=</span> <span class="n">query</span>
      <span class="n">k_dists</span> <span class="o">=</span> <span class="n">q_dists</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">key</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">KMeansClusteringForAtten</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="c1"># [B, S, N, K]</span>
      <span class="n">k_dists</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">clustering</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="n">update</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">fast_path</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenFastPath</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">q_dists</span><span class="p">,</span> <span class="n">k_dists</span><span class="p">,</span>
                                    <span class="n">query_paddings</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenSlowPath</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">q_dists</span><span class="p">,</span> <span class="n">k_dists</span><span class="p">,</span>
                                    <span class="n">query_paddings</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoutingAttention.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize &#39;states&#39; with .key, .value, and .key_dists.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">states</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">)</span>
    <span class="n">states</span><span class="o">.</span><span class="n">key_dists</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">target_max_length</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
               <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
        <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">states</span></div>

<div class="viewcode-block" id="RoutingAttention.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    This function is used by autoregressive decoding. Used for self-attention</span>
<span class="sd">    (hence S=T) with p.causal_masking is True.</span>

<span class="sd">    We compute the key/value/key_dists at `time_step` and cache the updated</span>
<span class="sd">    full length results in `cache_states` to reduce duplicate computation.</span>

<span class="sd">    p.fast_path is ignored (as if p.fast_path=False) as at each step we only</span>
<span class="sd">    compute for query of length 1.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:         [B, 1, D].</span>
<span class="sd">      cached_states:     A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. It contains .key</span>
<span class="sd">        and .value with shape [T, B, N, H], and .key_dists with  shape [T, B, N,</span>
<span class="sd">        K]. Note that they are all time-major.</span>
<span class="sd">      paddings:          [B, T], or None if there is no padding.</span>
<span class="sd">      time_step:         Scalar, the current decode step, 0-based.</span>
<span class="sd">      segment_mask:      must be None.</span>
<span class="sd">      per_step_padding:  must be None. We obey causal masking.</span>
<span class="sd">      use_short_seq_opt: must be False.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded:           [B, 1, D].</span>
<span class="sd">      updated_states:    `.NestedMap` with .key, .value, .key_dists.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If value projection is disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_value_proj</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Value projection must be enabled: &#39;</span>
                       <span class="s1">&#39;set p.enable_value_proj = True.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">causal_masking</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;p.causal_masking must be true.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">per_step_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Requires segment_mask=None and per_step_padding=None.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_short_seq_opt</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Requires use_short_seq_opt=False.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">time_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Requires valid time_step, not None.&#39;</span><span class="p">)</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Project inputs to key, value and query. Each has shape [B, 1, N, H].</span>
    <span class="n">key_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">value_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="n">query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="n">query_proj</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">KMeansClusteringForAtten</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">query_proj</span><span class="p">)</span>
    <span class="n">key_proj</span> <span class="o">=</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">KMeansClusteringForAtten</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">key_proj</span><span class="p">)</span>
    <span class="c1"># [B, 1, N, K]</span>
    <span class="n">k_dists</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">clustering</span><span class="p">,</span> <span class="n">key_proj</span><span class="p">)</span>

    <span class="c1"># The updated_key and extended_value have shape [T, B, N, H].</span>
    <span class="n">updated_key</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
        <span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="n">updated_value</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
        <span class="n">cached_states</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value_proj</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">]))</span>
    <span class="c1"># Shape [T, B, N, K]</span>
    <span class="n">updated_key_dists</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span>
        <span class="n">cached_states</span><span class="o">.</span><span class="n">key_dists</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">k_dists</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">]))</span>
    <span class="n">updated_states</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="n">updated_key</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">updated_value</span><span class="p">,</span> <span class="n">key_dists</span><span class="o">=</span><span class="n">updated_key_dists</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Apply causal padding. Shape [B, T]</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">time_step</span><span class="p">)),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">query_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddings</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenOneStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">query_proj</span><span class="p">,</span>
        <span class="n">updated_states</span><span class="p">,</span>
        <span class="n">query_paddings</span><span class="o">=</span><span class="n">query_paddings</span><span class="p">,</span>
        <span class="n">key_paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
        <span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">)</span>
    <span class="c1"># Post projection.</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">post</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">updated_states</span></div>

<div class="viewcode-block" id="RoutingAttention._DotAttenOneStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention._DotAttenOneStep">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAttenOneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">query_paddings</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">,</span>
                       <span class="n">time_step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Dot attention function for queries with 1 time step.</span>

<span class="sd">    Called from ExtendStep(). Used for self-attention with p.causal_masking</span>
<span class="sd">    is True.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query:    [B, 1, N, H], already normalized.</span>
<span class="sd">      states:   .key and .value with shape [T, B, N, H], .key_dists with shape</span>
<span class="sd">        [T, B, N, K]. .key is normalized.</span>
<span class="sd">      query_paddings: [B, 1].</span>
<span class="sd">      key_paddings: [B, T].</span>
<span class="sd">      time_step: Scalar, the current decode step, 0-based.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, 1, N, H].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># [B, 1, N, K]</span>
    <span class="n">q_dists</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">clustering</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
    <span class="c1"># [B, T, N, K]</span>
    <span class="n">k_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">key_dists</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="n">very_large_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">k_dists</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
        <span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">k_dists</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">k_dists</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span>
    <span class="n">paddings_tiled</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key_paddings</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">])</span>
    <span class="n">k_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">paddings_tiled</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">very_large_dists</span><span class="p">,</span> <span class="n">k_dists</span><span class="p">)</span>

    <span class="n">key</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">encoded</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DotAttenSlowPath</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">query</span><span class="p">,</span>
        <span class="n">key</span><span class="p">,</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">q_dists</span><span class="p">,</span>
        <span class="n">k_dists</span><span class="p">,</span>
        <span class="n">query_paddings</span><span class="p">,</span>
        <span class="n">key_paddings</span><span class="p">,</span>
        <span class="n">query_relative_position_shift</span><span class="o">=</span><span class="n">time_step</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span></div>

<div class="viewcode-block" id="RoutingAttention._DotAttenSlowPath"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention._DotAttenSlowPath">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAttenSlowPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">theta</span><span class="p">,</span>
                        <span class="n">query</span><span class="p">,</span>
                        <span class="n">key</span><span class="p">,</span>
                        <span class="n">value</span><span class="p">,</span>
                        <span class="n">q_dists</span><span class="p">,</span>
                        <span class="n">k_dists</span><span class="p">,</span>
                        <span class="n">query_paddings</span><span class="p">,</span>
                        <span class="n">key_paddings</span><span class="p">,</span>
                        <span class="n">query_relative_position_shift</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the attention via the slow path.</span>

<span class="sd">    This implementation selects, on a per query basis, p.attention_window</span>
<span class="sd">    number of keys/values to attend to.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` of the values of this layer&#39;s weights.</span>
<span class="sd">      query: [B, T, N, H], already normalized.</span>
<span class="sd">      key:   [B, S, N, H], already normalized.</span>
<span class="sd">      value: [B, S, N, H].</span>
<span class="sd">      q_dists: [B, T, N, K].</span>
<span class="sd">      k_dists: [B, S, N, K].</span>
<span class="sd">      query_paddings: [B, T].</span>
<span class="sd">      key_paddings:   [B, S].</span>
<span class="sd">      query_relative_position_shift: scalar. The position (relative to key[0])</span>
<span class="sd">        of query[0]. This impacts relative position encoding (not yet</span>
<span class="sd">        implemented) and causal masking.</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, T, N, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># [B, N, K, S]</span>
    <span class="c1"># If key is padded in a position, &#39;k_dists&#39; is inf which ensures</span>
    <span class="c1"># that we consider all non-padded keys even if some padded keys</span>
    <span class="c1"># might appear closer.</span>
    <span class="n">k_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">k_dists</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># [B, N, K, W], for each centroid, the indices of closest key vecs.</span>
    <span class="c1"># It&#39;s okay if W is so larger such that a padded index is included,</span>
    <span class="c1"># because below in attention_util.ComputeSparseAttention() correctly</span>
    <span class="c1"># handles &#39;paddings&#39;.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">closest_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="o">-</span><span class="n">k_dists</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_window</span><span class="p">)</span>
    <span class="c1"># [B, T, N, K], one hot encoded closest centroid for each query vec.</span>
    <span class="n">nearest_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">q_dists</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">closest_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># For each query vec, we allow it to attend to those keys that are the</span>
    <span class="c1"># W closest to its centroid, where W is the attention window.</span>
    <span class="n">sparsity_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BTNK, BNKW -&gt; BTNW&#39;</span><span class="p">,</span> <span class="n">nearest_one_hot</span><span class="p">,</span>
                                 <span class="n">closest_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">causal_masking</span><span class="p">:</span>
      <span class="n">batch_size</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">query_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">q_length</span><span class="p">)</span> <span class="o">+</span> <span class="n">query_relative_position_shift</span>
      <span class="c1"># [B, T, N, W] where the T dimension is range(T)</span>
      <span class="n">query_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">query_positions</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                                <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_window</span><span class="p">])</span>
      <span class="n">masked_indices</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sparsity_indices</span><span class="p">)</span>
      <span class="c1"># Replace key positions in the future with -1 to indicate masking.</span>
      <span class="c1">#</span>
      <span class="c1"># Note that this is done after selecting top_k from &#39;k_dists&#39;, so for</span>
      <span class="c1"># example if all the closest keys are in the future, we waste</span>
      <span class="c1"># p.attention_window on padded keys when in theory we could have attended</span>
      <span class="c1"># to further away keys that are not in the future (in order to achieve</span>
      <span class="c1"># that we need to pick top_k from &#39;k_dists&#39; differently for each query).</span>
      <span class="n">sparsity_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">sparsity_indices</span><span class="p">,</span> <span class="n">query_positions</span><span class="p">),</span> <span class="n">masked_indices</span><span class="p">,</span>
          <span class="n">sparsity_indices</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">attention_util</span><span class="o">.</span><span class="n">ComputeSparseAttention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span>
                                                 <span class="n">sparsity_indices</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoutingAttention._DotAttenFastPath"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.RoutingAttention._DotAttenFastPath">[docs]</a>  <span class="k">def</span> <span class="nf">_DotAttenFastPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">q_dists</span><span class="p">,</span> <span class="n">k_dists</span><span class="p">,</span>
                        <span class="n">query_paddings</span><span class="p">,</span> <span class="n">key_paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the attention via the fast path.</span>

<span class="sd">    This implementation compute groups of queries, and for each group,</span>
<span class="sd">    selects a set of p.attention_window number of keys/values that each</span>
<span class="sd">    query in that group all attend to.</span>

<span class="sd">    There is no guarantee a query uniquely belong to a single group, although</span>
<span class="sd">    via clustering this should likely be the case. When a query belong to</span>
<span class="sd">    multiple groups, the attention is averaged post softmax; when a query</span>
<span class="sd">    does not belong to any group, the attention result is zero.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` of the values of this layer&#39;s weights.</span>
<span class="sd">      query: [B, T, N, H], already normalized.</span>
<span class="sd">      key:   [B, S, N, H], already normalized.</span>
<span class="sd">      value: [B, S, N, H].</span>
<span class="sd">      q_dists: [B, T, N, K].</span>
<span class="sd">      k_dists: [B, S, N, K].</span>
<span class="sd">      query_paddings: [B, T].</span>
<span class="sd">      key_paddings:   [B, S].</span>

<span class="sd">    Returns:</span>
<span class="sd">      encoded: [B, T, N, H].</span>
<span class="sd">      atten_probs: [B, T, N, S]. Note, N * S * T space complexity here.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># [B, N, K, S]</span>
    <span class="c1"># If key is padded in a position, &#39;k_dists&#39; is inf which ensures</span>
    <span class="c1"># that we consider all non-padded keys even if some padded keys</span>
    <span class="c1"># might appear closer.</span>
    <span class="n">k_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">k_dists</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># [B, N, K, W], for each centroid, the indices of closest key vecs.</span>
    <span class="c1"># closest_k may include padded positions.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">closest_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="o">-</span><span class="n">k_dists</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_window</span><span class="p">)</span>

    <span class="n">q_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">k_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">q_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">q_cluster_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">query_group_size_factor</span> <span class="o">*</span> <span class="n">q_length</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">)</span>
    <span class="c1"># Of shape [B, N, K, T]</span>
    <span class="n">q_dists</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">q_dists</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c1"># closest_q of shape [B, N, K, V], where V = q_cluster_size</span>
    <span class="c1"># closest_q may include padded positions.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">closest_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="o">-</span><span class="n">q_dists</span><span class="p">,</span> <span class="n">q_cluster_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">indx</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Gathers values from v.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: A tensor of shape [B, T, N, D]</span>
<span class="sd">        indx: A tensor of shape [B, N, K, W]</span>

<span class="sd">      Returns:</span>
<span class="sd">        A value of shape [B, N, K, W, D]</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># pylint: disable=invalid-name</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">indx</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
      <span class="c1"># pylint: enable=invalid-name</span>
      <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">B</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
      <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">indx</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
      <span class="n">head_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">N</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
      <span class="n">head_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">head_idx</span><span class="p">,</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">gather_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">,</span> <span class="n">head_idx</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">gather_idx</span><span class="p">)</span>

    <span class="c1"># c_ shorts for clustered.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="c1"># of shape [B, N, K, V, D]</span>
    <span class="n">c_query</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">closest_q</span><span class="p">)</span>
    <span class="c1"># of shape [B, N, K, W, D]</span>
    <span class="n">c_key</span><span class="p">,</span> <span class="n">c_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">gather</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">closest_k</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># of shape [B, N, K, W, 1]</span>
    <span class="n">c_key_paddings</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
        <span class="c1"># [B, T, N, 1]</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key_paddings</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
        <span class="n">closest_k</span><span class="p">)</span>
    <span class="c1"># of shape [B, N, K, V, W]</span>
    <span class="n">is_key_padded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c_key_paddings</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">q_cluster_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">causal_masking</span><span class="p">:</span>
      <span class="c1"># both position matrices of shape [B, N, K, V, W]</span>
      <span class="n">c_query_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">closest_q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_window</span><span class="p">])</span>
      <span class="n">c_key_positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">closest_k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span>
                                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">q_cluster_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># We pad the logit for future key positions relative to each query</span>
      <span class="n">is_key_padded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
          <span class="n">is_key_padded</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">c_key_positions</span><span class="p">,</span> <span class="n">c_query_positions</span><span class="p">))</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNKVD,BNKWD-&gt;BNKVW&#39;</span><span class="p">,</span> <span class="n">c_query</span><span class="p">,</span> <span class="n">c_key</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dim_per_head</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">very_negative_logits</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">max</span> <span class="o">*</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">padded_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_key_padded</span><span class="p">,</span> <span class="n">very_negative_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">c_atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">padded_logits</span><span class="p">)</span>
    <span class="n">c_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BNKWD,BNKVW-&gt;BNKVD&#39;</span><span class="p">,</span> <span class="n">c_value</span><span class="p">,</span> <span class="n">c_atten_probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">indx</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Scatters v according to indx.</span>

<span class="sd">      Args:</span>
<span class="sd">        v: A tensor of shape [B, N, K, V, D].</span>
<span class="sd">        indx: A tensor of shape [B, N, K, V].</span>
<span class="sd">        seq_len: sequence length of the output.</span>

<span class="sd">      Returns:</span>
<span class="sd">        output: A tensor of shape [B, T, N, D], where T = seq_len.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># Need to scatter outputs back to the original shape.</span>
      <span class="c1"># pylint: disable=invalid-name</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
      <span class="c1"># pylint: enable=invalid-name</span>
      <span class="c1"># [B, N, K, V, 1]</span>
      <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">B</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># [B, N, K, V, 1]</span>
      <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">indx</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
      <span class="c1"># [B, N, K, V, 1]</span>
      <span class="n">head_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">N</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">scatter_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">,</span> <span class="n">head_idx</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>
      <span class="n">scattered</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span>
          <span class="n">scatter_idx</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
          <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># We need to normaliz as one query vector may appear in multiple clusters.</span>
      <span class="n">scattered</span><span class="p">,</span> <span class="n">den</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">scattered</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># den = tf.squeeze(den, -1)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">scattered</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">den</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                                   <span class="n">den</span><span class="p">)</span>  <span class="c1"># [:, :, :, None])</span>
      <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">scatter_atten_prob</span><span class="p">(</span><span class="n">c_atten_probs</span><span class="p">,</span> <span class="n">closest_k</span><span class="p">,</span> <span class="n">closest_q</span><span class="p">,</span> <span class="n">k_length</span><span class="p">,</span>
                           <span class="n">q_length</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Scatters c_atten_probs.</span>

<span class="sd">      Args:</span>
<span class="sd">        c_atten_probs: A tensor of shape [B, N, K, V, W].</span>
<span class="sd">        closest_k: A tensor of shape [B, N, K, W].</span>
<span class="sd">        closest_q: A tensor of shape [B, N, K, V].</span>
<span class="sd">        k_length: Length of the key vectors.</span>
<span class="sd">        q_length: Length of the query vectors.</span>

<span class="sd">      Returns:</span>
<span class="sd">        output: A tensor of shape [B, q_length, N, k_length].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="c1"># Need to scatter outputs back to the original shape.</span>
      <span class="c1"># pylint: disable=invalid-name</span>
      <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">c_atten_probs</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
      <span class="c1"># pylint: enable=invalid-name</span>
      <span class="c1"># [B, N, K, V, W, 1]</span>
      <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">B</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="c1"># [B, N, K, V, W, 1]</span>
      <span class="n">k_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">closest_k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">q_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">closest_q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">head_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">N</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">scatter_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">head_idx</span><span class="p">,</span> <span class="n">k_idx</span><span class="p">],</span> <span class="mi">5</span><span class="p">)</span>
      <span class="n">scattered_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span><span class="n">scatter_idx</span><span class="p">,</span> <span class="n">c_atten_probs</span><span class="p">,</span>
                                     <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">k_length</span><span class="p">])</span>

      <span class="c1"># We need to normalize the attention prob as one query vector may appear</span>
      <span class="c1"># in multiple clusters.</span>
      <span class="c1"># [B, N, K, V, 3]</span>
      <span class="n">times_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">head_idx</span><span class="p">],</span> <span class="mi">5</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
      <span class="c1"># [B, q_length, N]</span>
      <span class="n">times</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span>
          <span class="n">times_idx</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">closest_q</span><span class="p">),</span> <span class="n">scattered_prob</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">q_length</span><span class="p">,</span> <span class="n">N</span><span class="p">])</span>
      <span class="n">times</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">times</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">scattered_prob</span> <span class="o">/</span> <span class="n">times</span>
      <span class="k">return</span> <span class="n">out</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">c_outputs</span><span class="p">,</span> <span class="n">closest_q</span><span class="p">,</span> <span class="n">q_length</span><span class="p">)</span>
    <span class="n">out_prob</span> <span class="o">=</span> <span class="n">scatter_atten_prob</span><span class="p">(</span><span class="n">c_atten_probs</span><span class="p">,</span> <span class="n">closest_k</span><span class="p">,</span> <span class="n">closest_q</span><span class="p">,</span> <span class="n">k_length</span><span class="p">,</span>
                                  <span class="n">q_length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">out_prob</span></div></div>


<div class="viewcode-block" id="MultiSourceAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiSourceAttention</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Batch major attention with multiple source sub-attentions.</span>

<span class="sd">  It attends to multiple sources and uses one query as input to generates a</span>
<span class="sd">  combined attention context. The dimension of the combined context vector is a</span>
<span class="sd">  sum of all source context vectors. Each source attention has its separate</span>
<span class="sd">  params and is associated with a source key.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiSourceAttention.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceAttention.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;source_atten_tpls&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;A list of (source_key, attention_param) pairs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Default key dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Default hidden dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;primary_source_key&#39;</span><span class="p">,</span> <span class="s1">&#39;source_0&#39;</span><span class="p">,</span> <span class="s1">&#39;Key for the primary source &#39;</span>
        <span class="s1">&#39;whose attention probabilities will be used as an output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_merger_tpl&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Params to specify how to merge source attention vectors.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an MultiSourceAttention object.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">primary_source_key</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span>
    <span class="p">],</span> <span class="p">(</span><span class="s1">&#39;Source attention must have the primary source key.&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">atten_p</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">atten_p</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">child_p_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">atten</span> <span class="ow">in</span> <span class="n">atten_p</span><span class="p">:</span>
          <span class="n">child_p</span> <span class="o">=</span> <span class="n">atten</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
          <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">child_p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
          <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">child_p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
          <span class="n">child_p_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_p</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">child_p_list</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">child_p</span> <span class="o">=</span> <span class="n">atten_p</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">child_p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
        <span class="k">if</span> <span class="n">child_p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">child_p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">child_p</span><span class="p">)</span>

    <span class="c1"># Initialize source context vector merging layer.</span>
    <span class="n">merger_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_merger_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_merger&#39;</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">source_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">merger_p</span><span class="o">.</span><span class="n">query_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten_merger&#39;</span><span class="p">,</span> <span class="n">merger_p</span><span class="p">)</span>

<div class="viewcode-block" id="MultiSourceAttention.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceAttention.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">key_vec</span><span class="p">,</span>
            <span class="n">value_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">result_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">source_key</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">source_atten_tpls</span><span class="p">:</span>
        <span class="n">result_map</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
                <span class="n">theta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">source_key</span><span class="p">),</span> <span class="n">query_vec</span><span class="p">,</span>
                <span class="n">key_vec</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span> <span class="n">value_vec</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span>
                <span class="n">paddings</span><span class="p">[</span><span class="n">source_key</span><span class="p">],</span>
                <span class="n">segment_mask</span><span class="p">[</span><span class="n">source_key</span><span class="p">]</span> <span class="k">if</span> <span class="n">segment_mask</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">per_step_padding</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CombineContext</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">result_map</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiSourceAttention._CombineContext"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceAttention._CombineContext">[docs]</a>  <span class="k">def</span> <span class="nf">_CombineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">enc_map</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">):</span>
    <span class="n">encs</span> <span class="o">=</span> <span class="n">enc_map</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
    <span class="n">combined_enc</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atten_merger</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten_merger</span><span class="p">,</span> <span class="p">[</span><span class="n">enc</span> <span class="k">for</span> <span class="n">enc</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">encs</span><span class="p">],</span>
                                <span class="n">query_vec</span><span class="p">))</span>
    <span class="c1"># Return atten_probs of the primary source.</span>
    <span class="k">return</span> <span class="n">combined_enc</span><span class="p">,</span> <span class="n">enc_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">primary_source_key</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span></div>

<div class="viewcode-block" id="MultiSourceAttention.AttenProbs"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceAttention.AttenProbs">[docs]</a>  <span class="k">def</span> <span class="nf">AttenProbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query</span><span class="p">,</span>
                 <span class="n">key</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="p">,</span>
                 <span class="n">segment_mask</span><span class="p">,</span>
                 <span class="n">per_step_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">primary_source_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">primary_source_key</span>
    <span class="n">child_name</span> <span class="o">=</span> <span class="s1">&#39;atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">primary_source_key</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">child_name</span><span class="p">]</span><span class="o">.</span><span class="n">AttenProbs</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">child_name</span><span class="p">),</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">[</span><span class="n">primary_source_key</span><span class="p">],</span>
        <span class="n">paddings</span><span class="p">[</span><span class="n">primary_source_key</span><span class="p">],</span>
        <span class="n">segment_mask</span><span class="p">[</span><span class="n">primary_source_key</span><span class="p">]</span> <span class="k">if</span> <span class="n">segment_mask</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">per_step_padding</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TransformerAttentionLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerAttentionLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiheaded attention sub-layer in Transformer layer.</span>

<span class="sd">  Input is first normalized using Layer Normalization. Output of layer</span>
<span class="sd">  normalization is processed using multi-headed attention. And finally, the</span>
<span class="sd">  output of the attention layer is combined with the residual connection.</span>
<span class="sd">  This module also allows mixing different attention heads by making num_heads</span>
<span class="sd">  and atten_tpl into lists of the same size, specifying the distribution of</span>
<span class="sd">  heads for each attention type.</span>

<span class="sd">  This layer will be used in the following two scenarios:</span>

<span class="sd">  1. Multi-Headed Self-Attention, where attention keys, values (source_vecs) and</span>
<span class="sd">     queries come from the same previous layer output.</span>
<span class="sd">  2. Masked Multi-Headed Self-Attention, where attention keys, values and</span>
<span class="sd">     queries all come from the same previous layer output, but rightward</span>
<span class="sd">     activations are masked to prevent information flow from future. This is the</span>
<span class="sd">     use case for Transformer decoder self-attention layers. Can be activated by</span>
<span class="sd">     setting is_masked flag of this layer.</span>
<span class="sd">  3. Multi-Headed Cross-Attention, where attention keys and values</span>
<span class="sd">     (source_vecs) are coming from a different source (output of the encoder),</span>
<span class="sd">     and queries coming from the previous layer outputs (decoder).</span>
<span class="sd">  4. Mixture of different heads, for example 2 LocalSelfAttention heads</span>
<span class="sd">     and 3 RoutingAttention heads can be specified by setting num_heads = [2, 3]</span>
<span class="sd">     and atten_tpl = [LocalSelfAttention, RoutingAttention].</span>

<span class="sd">  We use the same capital letters to denote certain tensor parameters as</span>
<span class="sd">  MultiHeadedAttention class.</span>

<span class="sd">    B = batch size</span>
<span class="sd">    S = length of the key/value (source)</span>
<span class="sd">    T = length of the query (target)</span>
<span class="sd">    D = model dimension</span>
<span class="sd">    N = number of attention heads</span>
<span class="sd">    H = dimensions of each attention head.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerAttentionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the attention hidden dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;Number of attention heads. This can be a list in&#39;</span>
        <span class="s1">&#39;case of mixture of attention types&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;is_masked&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If set, uses causal non local multiheaded attention.&#39;</span>
        <span class="s1">&#39;This option is not valid when atten_tpl is LocalSelfAttention &#39;</span>
        <span class="s1">&#39;or its subclass(es).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;Probability at which we apply dropout to the attention probs. &#39;</span>
        <span class="s1">&#39;This practically drops memory values at random positions.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;residual_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;Probability at which we apply dropout to the residual layers, &#39;</span>
        <span class="s1">&#39;such that, residual(x, y) = (x + dropout(y)).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If set, uses unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_skip_connection&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If True, add input (or normalized input) to the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Layer norm default params. No layernorm if set to None.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_tpl&#39;</span><span class="p">,</span>
        <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(),</span>
        <span class="s1">&#39;Multi-Headed Dot-Product Attention default params. This can be&#39;</span>
        <span class="s1">&#39;a list in the case of mixture of attentions, must be of same size&#39;</span>
        <span class="s1">&#39;as num_heads&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Residual dropout params template. keep_prop will be reset to &#39;</span>
        <span class="s1">&#39;(1.0 - residual_dropout_prob).&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.CommonParams"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.CommonParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">CommonParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                   <span class="n">input_dim</span><span class="p">,</span>
                   <span class="n">num_heads</span><span class="p">,</span>
                   <span class="n">is_masked</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">use_relative_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">relative_pos_emb_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">local_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">left_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">right_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="c1"># pylint: disable=g-doc-args</span>
    <span class="sd">&quot;&quot;&quot;Returns a hyperparam for the most representative cases.</span>

<span class="sd">    CommonParams is not expected to be extended to an omnipotent/generic builder</span>
<span class="sd">    method. Specific use cases should take the return value of it and apply</span>
<span class="sd">    further customization. It should be kept lean and only extended cautiously</span>
<span class="sd">    for very common cases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=g-doc-args</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_relative_atten</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">relative_pos_emb_dim</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">relative_pos_emb_dim</span> <span class="o">=</span> <span class="n">relative_pos_emb_dim</span> <span class="ow">or</span> <span class="n">input_dim</span>

    <span class="k">if</span> <span class="n">local_context</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">left_context</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">right_context</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;local_context and (left_context, right_context) can not be set &#39;</span>
          <span class="s1">&#39;at the same time.&#39;</span><span class="p">)</span>
      <span class="n">left_context</span> <span class="o">=</span> <span class="n">local_context</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># include &#39;self&#39; position.</span>
      <span class="n">right_context</span> <span class="o">=</span> <span class="n">local_context</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">is_masked</span><span class="o">=</span><span class="n">is_masked</span><span class="p">,</span>
        <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">,</span>
        <span class="n">residual_dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>

    <span class="n">is_local</span> <span class="o">=</span> <span class="n">left_context</span> <span class="ow">or</span> <span class="n">right_context</span>
    <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
      <span class="n">atten_cls</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">LocalSelfAttentionXL</span> <span class="k">if</span> <span class="n">use_relative_atten</span> <span class="k">else</span> <span class="n">LocalSelfAttention</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">atten_cls</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">MultiHeadedAttentionXL</span>
          <span class="k">if</span> <span class="n">use_relative_atten</span> <span class="k">else</span> <span class="n">MultiHeadedAttention</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">atten_cls</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">use_relative_atten</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">relative_pos_emb_dim</span>
    <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">left_context</span><span class="o">=</span><span class="n">left_context</span><span class="p">,</span> <span class="n">right_context</span><span class="o">=</span><span class="n">right_context</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer._InitAttentionParams"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer._InitAttentionParams">[docs]</a>  <span class="k">def</span> <span class="nf">_InitAttentionParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">atten_tpl</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns an initialized transformer attention parameters.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Make sure atten_tpl and num_heads are scalars or lists of same size</span>
    <span class="k">def</span> <span class="nf">_RaiseTypeError</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_heads and atten_tpl should both be lists &#39;</span>
                       <span class="s1">&#39;of the same size or both scalars.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="o">==</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">atten_tpl</span><span class="p">):</span>
          <span class="n">_RaiseTypeError</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">_RaiseTypeError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_SetCommonParams</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">make_output_proj_no_op</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Each dim per head is now divided among all heads</span>
        <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
        <span class="n">params</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">dim_per_head</span>
      <span class="k">return</span> <span class="n">params</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">params_list</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">atten_tpl</span><span class="p">)):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">atten_tpl</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">_SetCommonParams</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="s1">&#39;mixed_atten_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                                  <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">params_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">params_list</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">_SetCommonParams</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="s1">&#39;multihead_atten&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="c1"># Initialize attention.</span>
    <span class="k">def</span> <span class="nf">_LocalAttentionError</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">LocalSelfAttention</span><span class="p">):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">is_masked</span><span class="se">\&#39;</span><span class="s1"> is not effective when used with &#39;</span>
                           <span class="s1">&#39;LocalSelfAttention and its subclass(es).&#39;</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_InitAttentionParams</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
        <span class="n">_LocalAttentionError</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
      <span class="c1"># Create head mixing variable</span>
      <span class="n">dim_per_head</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
      <span class="c1"># For the projection merging layer, parameter settings from the first</span>
      <span class="c1"># attention template in the list is used.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span>
          <span class="s1">&#39;w_mix_heads&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
              <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
              <span class="n">num_heads</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">),</span>
              <span class="n">dim_per_head</span><span class="o">=</span><span class="n">dim_per_head</span><span class="p">,</span>
              <span class="n">is_output_projection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">make_output_proj_no_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">use_bias</span><span class="p">,</span>
              <span class="n">xla_num_partitions</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">_LocalAttentionError</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize attention layer normalization.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="p">:</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;atten_ln&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize residual dropout.</span>
    <span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">dropout_tpl</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;residual_dropout&#39;</span><span class="p">,</span> <span class="n">dropout_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="TransformerAttentionLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">per_step_padding_override</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the result of Transformer attention layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:   [B, T, D].</span>
<span class="sd">      source_vecs: [B, S, D] (cross_attention) or None (self-attention).</span>
<span class="sd">      paddings:    [B, S].</span>
<span class="sd">      per_step_padding_override: [B, T, T] for self attention or [B, T, S] for</span>
<span class="sd">        cross attention.</span>
<span class="sd">      segment_mask: [B, 1, T, S].</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: [B, T, D].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">unnormalized_query_vec</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="c1"># Layer normalization.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="p">:</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># For self-attention: keys = queries.</span>
    <span class="k">if</span> <span class="n">source_vecs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">source_vecs</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="c1"># Generates mask, with shape [b, t, t].</span>
    <span class="k">if</span> <span class="n">per_step_padding_override</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span> <span class="ow">and</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># causal padding.</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">CausalPadding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">per_step_padding</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">per_step_padding_override</span>

    <span class="c1"># Multiheaded attention.</span>
    <span class="k">def</span> <span class="nf">_AttenFProp</span><span class="p">(</span><span class="n">atten</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>  <span class="c1"># query</span>
          <span class="n">source_vecs</span><span class="p">,</span>  <span class="c1"># key</span>
          <span class="n">source_vecs</span><span class="p">,</span>  <span class="c1"># value</span>
          <span class="n">paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
          <span class="n">per_step_padding</span><span class="o">=</span><span class="n">per_step_padding</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ctx_vec_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">atten_probs_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">)):</span>
          <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">_AttenFProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
          <span class="n">ctx_vec_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">)</span>
          <span class="n">atten_probs_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">)</span>
        <span class="c1"># Concat all the outputs together</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">ctx_vec_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># ctx_vec has shape [B, T, N, H] due to identity projection</span>
        <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_mix_heads</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">w_mix_heads</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">atten_probs_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">_AttenFProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">)</span>

    <span class="c1"># Residual connection.</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">unnormalized_query_vec</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="k">else</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>
      <span class="n">ctx_vec</span> <span class="o">+=</span> <span class="n">input_to_add</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">atten</span><span class="o">=</span><span class="p">[</span>
          <span class="n">a</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">a_theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">a_theta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
      <span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span>
                                 <span class="n">target_max_length</span><span class="p">)</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the result and update cached states for the current step.</span>

<span class="sd">    This function is used by autoregressive decoding. This function knows the</span>
<span class="sd">    length of full sequence, thus it is different from StreamStep.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec: [B, 1, D]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   - [T, B,</span>
<span class="sd">        N, H]. value - [T, B, N, H].</span>
<span class="sd">      time_step: A scalar or tensor with [B], current decode step, 0-based. if</span>
<span class="sd">        it&#39;s a scalar, all the time step are the same decode step. if it&#39;s a</span>
<span class="sd">        tensor, it represents current decode step for each sample.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [B, 1, D]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [T, B, N, H].</span>
<span class="sd">      value - [T, B, N, H].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If not used as masked/causal self-attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;ExtendStep should be used only by masked/causal self-attention.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">t</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">cached_states</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">unnormalized_query_vec</span> <span class="o">=</span> <span class="n">query_vec</span>
    <span class="n">time_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">time_step</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">batch_time_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">time_step</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[</span><span class="n">b</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">batch_time_step</span> <span class="o">=</span> <span class="n">time_step</span>

    <span class="c1"># Generates mask, with shape [b, 1, t].</span>
    <span class="n">zero_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># [b, t]</span>
    <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">batch_time_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">zero_padding</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">zero_padding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># [b, 1, t]</span>
    <span class="n">per_step_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">per_step_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Layer normalization.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="p">:</span>
      <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

    <span class="c1"># Multiheaded masked/causal self-attention.</span>
    <span class="k">def</span> <span class="nf">_AttenExtendStep</span><span class="p">(</span><span class="n">atten</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                              <span class="n">per_step_padding</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span> <span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">updated_states</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">atten</span><span class="o">=</span><span class="p">[])</span>
      <span class="n">ctx_vec_list</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">)):</span>
        <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">updated_atten_states</span> <span class="o">=</span> <span class="n">_AttenExtendStep</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cached_states</span><span class="o">.</span><span class="n">atten</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">ctx_vec_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctx_vec</span><span class="p">)</span>
        <span class="n">updated_states</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updated_atten_states</span><span class="p">)</span>
      <span class="c1"># Concat all attention heads together</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">ctx_vec_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
      <span class="c1"># ctx_vec has shape [B, T, N, H] due to identity projection</span>
      <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_mix_heads</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">w_mix_heads</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="n">_AttenExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span>
                                                 <span class="n">cached_states</span><span class="p">)</span>

    <span class="c1"># Residual connection.</span>
    <span class="n">ctx_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="n">ctx_vec</span><span class="p">)</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">unnormalized_query_vec</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="k">else</span> <span class="n">query_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>
      <span class="n">ctx_vec</span> <span class="o">+=</span> <span class="n">input_to_add</span>
    <span class="k">return</span> <span class="n">ctx_vec</span><span class="p">,</span> <span class="n">updated_states</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.zero_state"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initial state given the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">      state: The initial state for streaming inference.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">atten</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span></div>

<div class="viewcode-block" id="TransformerAttentionLayer.StreamStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerAttentionLayer.StreamStep">[docs]</a>  <span class="k">def</span> <span class="nf">StreamStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the value vector given the query of the current step.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: a NestedMap with layer weights.</span>
<span class="sd">      query_vec: A query vector of shape [B, T, D].</span>
<span class="sd">      paddings: A 0/1 valued tensor of shape [B, T].</span>
<span class="sd">      state0: A `.NestedMap` of the same structure as returned by zero_state().</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: Output of the given query vector with shape [B, T, D].</span>
<span class="sd">      padding: the same as input paddings.</span>
<span class="sd">      state: updated state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">LocalSelfAttention</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">/StreamStep&#39;</span><span class="p">):</span>
      <span class="n">unnormalized_query_vec</span> <span class="o">=</span> <span class="n">query_vec</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="p">:</span>
        <span class="n">query_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">)</span>

      <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">atten_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atten</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten</span><span class="p">)</span>

      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

      <span class="c1"># Residual connection.</span>
      <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">unnormalized_query_vec</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="k">else</span> <span class="n">query_vec</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">input_to_add</span>

      <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">atten</span><span class="o">=</span><span class="n">atten_state1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TransformerMultiSourceAttentionLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerMultiSourceAttentionLayer</span><span class="p">(</span><span class="n">TransformerAttentionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Batch major multi-source multi-headed attention.</span>

<span class="sd">  Only supports scenarios 3 described by comments on TransformerAttentionLayer:</span>

<span class="sd">  3. Multi-source multi-headed cross-attention, where attention keys and values</span>
<span class="sd">     (source_vecs) are coming from different sources (one of them is usually</span>
<span class="sd">     the outputs of the encoder), and queries coming from the previous layer</span>
<span class="sd">     outputs (decoder). Specifically, attention keys and values are NestedMaps</span>
<span class="sd">     containing encodings of different sources. This corresponds to a</span>
<span class="sd">     multi-source decoder-to-encoder attention mechanism, i.e., decoder attends</span>
<span class="sd">     to encoder outputs and other sources.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerMultiSourceAttentionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_source&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of sources to attend to.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;primary_source_index&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Index of the primary source whose &#39;</span>
        <span class="s1">&#39;attention probs will be returned.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;multi_source_atten&#39;</span><span class="p">,</span> <span class="n">MultiSourceAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Multi-source attention params.&#39;</span><span class="p">)</span>
    <span class="c1"># Only used for case 3.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">is_masked</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerMultiSourceAttentionLayer._InitAttentionParams"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer._InitAttentionParams">[docs]</a>  <span class="k">def</span> <span class="nf">_InitAttentionParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">atten_tpl</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns an initialized multi-source transformer attention parameters.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">source_atten_tpls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Set up each source attention.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_source</span><span class="p">):</span>
      <span class="n">src_key</span> <span class="o">=</span> <span class="s1">&#39;source_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
      <span class="n">src_atten</span> <span class="o">=</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">src_atten</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_InitAttentionParams</span><span class="p">(</span><span class="n">src_atten</span><span class="p">)</span>
      <span class="n">src_atten</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_atten_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">src_key</span>
      <span class="n">source_atten_tpls</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">src_key</span><span class="p">,</span> <span class="n">src_atten</span><span class="p">))</span>

    <span class="c1"># Initialize multi-source attention.</span>
    <span class="n">msa</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">multi_source_atten</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">msa</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multi_source_atten&#39;</span>
    <span class="n">msa</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">msa</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
    <span class="n">msa</span><span class="o">.</span><span class="n">source_atten_tpls</span> <span class="o">=</span> <span class="n">source_atten_tpls</span>
    <span class="n">msa</span><span class="o">.</span><span class="n">primary_source_key</span> <span class="o">=</span> <span class="s1">&#39;source_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">primary_source_index</span>
    <span class="k">return</span> <span class="n">msa</span></div></div>


<div class="viewcode-block" id="TransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer layer with multiheaded attention.</span>

<span class="sd">  Applies self-attention followed by a cross-attention and feed forward layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the transformer block output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Num of heads in self attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_aux_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mask_self_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, use masked self-attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;tr_atten_tpl&#39;</span><span class="p">,</span>
             <span class="n">TransformerAttentionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(),</span>
             <span class="s1">&#39;Transformer Attention Layer params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;tr_self_atten_tpl&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Attention template for self attention. If unset, use tr_atten_tpl.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;tr_fflayer_tpl&#39;</span><span class="p">,</span>
        <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">),</span> <span class="s1">&#39;Transformer Feed-Forward Layer params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;compute_flops&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True adds computation cost for the layer FLOPs&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerLayer.CommonParams"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.CommonParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">CommonParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                   <span class="n">input_dim</span><span class="p">,</span>
                   <span class="n">atten_num_heads</span><span class="p">,</span>
                   <span class="n">atten_is_relative</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">atten_local_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">atten_left_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">atten_right_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">has_aux_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">mask_self_atten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">fflayer_hidden_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fflayer_output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="c1"># pylint: disable=g-doc-args</span>
    <span class="sd">&quot;&quot;&quot;Returns a hyperparam for the most representative cases.</span>

<span class="sd">    CommonParams is not expected to be extended to an omnipotent/generic builder</span>
<span class="sd">    method. Specific use cases should take the return value of it and apply</span>
<span class="sd">    further customization. It should be kept lean and only extended cautiously</span>
<span class="sd">    for very common cases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: enable=g-doc-args</span>
    <span class="n">output_dim</span> <span class="o">=</span> <span class="n">fflayer_output_dim</span> <span class="ow">or</span> <span class="n">input_dim</span>
    <span class="n">fflayer_hidden_dim</span> <span class="o">=</span> <span class="n">fflayer_hidden_dim</span> <span class="ow">or</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">input_dim</span>
    <span class="c1"># TODO(jamesqin): check how mask_self_atten work with local atten.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;transformer_layer&#39;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">atten_num_heads</span><span class="p">,</span>
        <span class="n">has_aux_atten</span><span class="o">=</span><span class="n">has_aux_atten</span><span class="p">,</span>
        <span class="n">mask_self_atten</span><span class="o">=</span><span class="n">mask_self_atten</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="o">=</span> <span class="n">TransformerAttentionLayer</span><span class="o">.</span><span class="n">CommonParams</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">atten_num_heads</span><span class="p">,</span>
        <span class="n">is_masked</span><span class="o">=</span><span class="n">mask_self_atten</span><span class="p">,</span>
        <span class="n">local_context</span><span class="o">=</span><span class="n">atten_local_context</span><span class="p">,</span>
        <span class="n">left_context</span><span class="o">=</span><span class="n">atten_left_context</span><span class="p">,</span>
        <span class="n">right_context</span><span class="o">=</span><span class="n">atten_right_context</span><span class="p">,</span>
        <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">fflayer_hidden_dim</span><span class="p">,</span>
        <span class="n">residual_dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">,</span>
        <span class="n">relu_dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerLayer.SetNumInputNodes"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.SetNumInputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetNumInputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">num_input_nodes</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">num_input_nodes</span></div>

<div class="viewcode-block" id="TransformerLayer.NumOutputNodes"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.NumOutputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">NumOutputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Initialize masked multi-headed self-attention</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">self_atten_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">self_atten_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">self_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_self_atten&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">params</span><span class="o">.</span><span class="n">is_masked</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">atten</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">:</span>
        <span class="n">atten</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
      <span class="c1"># Initialize multi-headed cross-attention</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;multihead_cross_atten&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">atten</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">:</span>
          <span class="n">atten</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;cross_atten&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize feed-forward layer</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;tr_fflayer&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">params</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="TransformerLayer._GetSourceBatchSize"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer._GetSourceBatchSize">[docs]</a>  <span class="k">def</span> <span class="nf">_GetSourceBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>

<div class="viewcode-block" id="TransformerLayer._GetSourceLength"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer._GetSourceLength">[docs]</a>  <span class="k">def</span> <span class="nf">_GetSourceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span></div>

<div class="viewcode-block" id="TransformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">per_step_padding_override</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, target_time, dim].</span>
<span class="sd">      paddings:     [target_batch, target_time].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim].</span>
<span class="sd">      aux_paddings: [source_batch, source_time].</span>
<span class="sd">      per_step_padding_override: [target_batch, target_time, target_time].</span>
<span class="sd">      segment_mask:     [target_batch, 1, target_time, target_time].</span>
<span class="sd">      aux_segment_mask: [source_batch, 1, target_time, source_time].</span>

<span class="sd">    target_batch can be a multiple of source_batch, where samples in</span>
<span class="sd">    target_batch are arranged in the order of [m, source_batch] where m =</span>
<span class="sd">    target_batch / source_batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The fflayer output with shape [target_batch, target_time, dim].</span>
<span class="sd">      atten_probs: [B, N, T, S].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">compute_flops</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;Current FLOPs computation does not include auxiliary attention&#39;</span><span class="p">)</span>
      <span class="n">computation_cost</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span>
          <span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;flops&#39;</span><span class="p">,</span>
          <span class="n">TransformerFlops</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
              <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">TENSOR_VALUES</span><span class="p">,</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
              <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">TENSOR_VALUES</span><span class="p">,</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
              <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">TENSOR_VALUES</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)))</span>
    <span class="c1"># First the self-attention layer.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Need to specify segment_mask &#39;</span>
                                        <span class="s1">&#39;for packed input.&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">aux_segment_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Need to specify aux_segment_mask&#39;</span>
                                              <span class="s1">&#39;for packed input.&#39;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">):</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
          <span class="n">query_vec</span><span class="p">,</span>
          <span class="kc">None</span><span class="p">,</span>
          <span class="n">paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
          <span class="n">per_step_padding_override</span><span class="o">=</span><span class="n">per_step_padding_override</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;aux_atten&#39;</span><span class="p">):</span>
        <span class="c1"># Next the cross-attention layer.</span>
        <span class="n">target_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="n">source_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetSourceBatchSize</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)</span>
        <span class="n">source_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetSourceLength</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)</span>

        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">source_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">atten_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span>
            <span class="n">atten_vec</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">aux_segment_mask</span><span class="p">)</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">atten_probs</span><span class="p">,</span>
            <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">source_time</span><span class="p">])</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_probs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
        <span class="n">atten_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">atten_probs</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">source_time</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">target_time</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;fflayer&#39;</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">paddings</span><span class="p">),</span> <span class="n">atten_probs</span></div>

<div class="viewcode-block" id="TransformerLayer.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span> <span class="n">target_max_length</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="p">,</span>
                                      <span class="n">target_max_length</span><span class="p">)</span></div>

<div class="viewcode-block" id="TransformerLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head]. value -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [target_batch, 1, dim]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      value - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_batch</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># First the self-attention layer.</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>
      <span class="n">source_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetSourceBatchSize</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)</span>
      <span class="c1"># Next the cross-attention layer.</span>
      <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
                                            <span class="n">aux_vec</span><span class="p">,</span> <span class="n">aux_paddings</span><span class="p">)</span>
      <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">atten_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cur_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<span class="c1"># TODO(garrettaxel): Distribute the computation to downstream layers.</span>
<div class="viewcode-block" id="TransformerFlops"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFlops">[docs]</a><span class="k">def</span> <span class="nf">TransformerFlops</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">atten_dim</span><span class="p">,</span> <span class="n">model_dim</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute FLOPs for Transformer layer without auxiliary attention.</span>

<span class="sd">    Attention Layer FLOPs (N = num attention heads, H = dim per head):</span>
<span class="sd">      q, k, v projections, incl bias: 3 x &#39;BTD,DNH-&gt;BTNH&#39; -&gt; 6*N*H*D*B*T</span>
<span class="sd">      logits: &#39;BTNH,BDNH-&gt;BNTD&#39; -&gt; (2*H-1)*N*B*T^2</span>
<span class="sd">      softmax: 5 ops per element in BNTD -&gt; 5*N*D*B*T</span>
<span class="sd">      context: &#39;BNTD,BDNH-&gt;BTNH&#39; -&gt; (2*T-1)*N*H*B*T</span>
<span class="sd">      output proj: &#39;BTNH,DNH-&gt;BTD&#39; -&gt; (2*N-1)*(2*H-1)*D*B*T</span>

<span class="sd">    2 residuals FLOPs: 2*D*B*T</span>
<span class="sd">    1 FF layer FLOPs: 4*ff_hidden*D*B*T</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs:    Input dimensions to the layer, [Batch, Time, Dim].</span>
<span class="sd">    num_heads: Number of attention heads for layer.</span>
<span class="sd">    ff_dim:    Feedforward hidden dimension.</span>
<span class="sd">    atten_dim: Attention hidden dimension.</span>
<span class="sd">    model_dim: Dimension of the model.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Total FLOPs of the transformer layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ff_dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">atten_dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c1"># dim per head</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">multi_head_atten_flops</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                            <span class="mi">5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">residual_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d</span>
  <span class="n">ff_flops</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">f</span> <span class="o">*</span> <span class="n">d</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">multi_head_atten_flops</span> <span class="o">+</span> <span class="n">residual_flops</span> <span class="o">+</span> <span class="n">ff_flops</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">t</span></div>


<div class="viewcode-block" id="MultiSourceTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiSourceTransformerLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multi-source transformer layer with multiheaded attention.</span>

<span class="sd">  Multi-source attention is used for cross attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiSourceTransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_source&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of sources to attend to.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;primary_source_index&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Index for the primary source &#39;</span>
        <span class="s1">&#39;whose attention probabilities will be used as an output.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span>
                      <span class="n">TransformerMultiSourceAttentionLayer</span><span class="p">)</span>
    <span class="c1"># Set up multi-source attention layer</span>
    <span class="n">cross_atten_p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span>
    <span class="n">cross_atten_p</span><span class="o">.</span><span class="n">num_source</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">num_source</span>
    <span class="n">cross_atten_p</span><span class="o">.</span><span class="n">primary_source_index</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">primary_source_index</span>
    <span class="k">assert</span> <span class="n">params</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">primary_source_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;source_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">primary_source_index</span>

<div class="viewcode-block" id="MultiSourceTransformerLayer._GetSourceBatchSize"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceBatchSize">[docs]</a>  <span class="k">def</span> <span class="nf">_GetSourceBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">primary_source_key</span><span class="p">],</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>

<div class="viewcode-block" id="MultiSourceTransformerLayer._GetSourceLength"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceLength">[docs]</a>  <span class="k">def</span> <span class="nf">_GetSourceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">primary_source_key</span><span class="p">],</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span></div></div>


<span class="c1"># mt_attention_layer.MultiHeadedAttentionXL</span>
<span class="n">ATTEN_TRANSFORMER_XL</span> <span class="o">=</span> <span class="s1">&#39;transformer_xl&#39;</span>
<span class="c1"># mt_attention_layer.MultiHeadedAttentionRPE</span>
<span class="n">ATTEN_RPE</span> <span class="o">=</span> <span class="s1">&#39;rpe&#39;</span>


<div class="viewcode-block" id="UseRelativeAttentionInTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.UseRelativeAttentionInTransformerLayer">[docs]</a><span class="k">def</span> <span class="nf">UseRelativeAttentionInTransformerLayer</span><span class="p">(</span><span class="n">transformer_params</span><span class="p">,</span>
                                           <span class="n">rel_pos_emb_dim</span><span class="p">,</span>
                                           <span class="n">atten_type</span><span class="o">=</span><span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Uses transformer-xl attention for self attention of a transformer layer.</span>

<span class="sd">  Args:</span>
<span class="sd">    transformer_params: A mt_attention_layer.TransformerLayer.Params() object.</span>
<span class="sd">    rel_pos_emb_dim: (int) Relative positional embedding dim to be set.</span>
<span class="sd">    atten_type: (string) Attention type. Supported:</span>
<span class="sd">      - &#39;transformer_xl&#39;: mt_attention_layer.MultiHeadedAttentionXL</span>
<span class="sd">      - &#39;rpe&#39;: mt_attention_layer.MultiHeadedAttentionRPE</span>

<span class="sd">  Returns:</span>
<span class="sd">    A mt_attention_layer.TransformerLayer.Params() object with relative pos emb.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported input transformer layer: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                     <span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">atten_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">,</span> <span class="n">ATTEN_RPE</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Relative attention type: </span><span class="si">%s</span><span class="s1"> unsupported&#39;</span> <span class="o">%</span> <span class="n">atten_type</span><span class="p">)</span>

  <span class="c1"># Gets multiheaded attention tpl from self attention config in transformer.</span>
  <span class="n">trans_params_copy</span> <span class="o">=</span> <span class="n">transformer_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span>

  <span class="c1"># If already using relative attention class.</span>
  <span class="k">if</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="ow">in</span> <span class="p">(</span><span class="n">MultiHeadedAttentionRPE</span><span class="p">,</span> <span class="n">MultiHeadedAttentionXL</span><span class="p">,</span>
                       <span class="n">LocalSelfAttentionXL</span><span class="p">):</span>
    <span class="n">atten_tpl</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">rel_pos_emb_dim</span>
    <span class="k">return</span> <span class="n">trans_params_copy</span>

  <span class="k">if</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="n">ATTEN_TRANSFORMER_XL</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">MultiHeadedAttention</span><span class="p">:</span>
      <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">LocalSelfAttention</span><span class="p">:</span>
      <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="p">(</span><span class="n">LocalSelfAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported attention: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">atten_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="n">ATTEN_RPE</span><span class="p">:</span>
    <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionRPE</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>

  <span class="n">rel_atten_tpl</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="n">rel_atten_tpl</span><span class="p">)</span>
  <span class="n">rel_atten_tpl</span><span class="o">.</span><span class="n">rel_pos_emb_dim</span> <span class="o">=</span> <span class="n">rel_pos_emb_dim</span>

  <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">rel_atten_tpl</span>
  <span class="k">return</span> <span class="n">trans_params_copy</span></div>


<div class="viewcode-block" id="ClearRelativeAttentionInTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ClearRelativeAttentionInTransformerLayer">[docs]</a><span class="k">def</span> <span class="nf">ClearRelativeAttentionInTransformerLayer</span><span class="p">(</span><span class="n">transformer_params</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes relative position attention in the transformer layer.</span>

<span class="sd">  Args:</span>
<span class="sd">    transformer_params: A mt_attention_layer.TransformerLayer param.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A mt_attention_layer.TransformerLayer param without relative attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported input transformer layer: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                     <span class="n">transformer_params</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>
  <span class="n">trans_params_copy</span> <span class="o">=</span> <span class="n">transformer_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">attention_tpl</span> <span class="o">=</span> <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span>
  <span class="k">if</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">MultiHeadedAttentionXL</span><span class="p">:</span>
    <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="k">elif</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="p">(</span><span class="n">LocalSelfAttentionXL</span><span class="p">):</span>
    <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">LocalSelfAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported attention params: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">attention_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">)</span>

  <span class="n">new_attention_tpl</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span>
      <span class="n">attention_tpl</span><span class="p">,</span> <span class="n">new_attention_tpl</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;skip_term_b&#39;</span><span class="p">])</span>
  <span class="n">trans_params_copy</span><span class="o">.</span><span class="n">tr_self_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">new_attention_tpl</span>
  <span class="k">return</span> <span class="n">trans_params_copy</span></div>


<div class="viewcode-block" id="TransformerDecoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerDecoderLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transformer decoder layer with multiheaded attention.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerDecoderLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerDecoderLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div></div>


<div class="viewcode-block" id="MultiSourceTransformerDecoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiSourceTransformerDecoderLayer</span><span class="p">(</span><span class="n">MultiSourceTransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multi-source transformer decoder layer with multiheaded attention.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultiSourceTransformerDecoderLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div></div>


<div class="viewcode-block" id="StackedTransformerLayers"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers">[docs]</a><span class="k">class</span> <span class="nc">StackedTransformerLayers</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A stack of Batch-Major Transformer layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="StackedTransformerLayers.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_aux_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mask_self_atten&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, use masked self-attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_layers&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num of layers in this stack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mdl_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Model dimension in Transformer layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;The hidden layer dimension in Transformer layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_atten_heads&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num of attention heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="s1">&#39;Apply dropout at this prob at various places.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;If set, uses unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;transformer_layer_params_tpl&#39;</span><span class="p">,</span> <span class="n">TransformerLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;A template of TransformerLayer.params, can be a list of params &#39;</span>
        <span class="s1">&#39;of length equal to the num_layers or a factor of num_layers.&#39;</span>
        <span class="s1">&#39;For a factor, the params are tiled as [a, a, ..., b, b,...,].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;final_layer_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If true, apply layer normalization to the final output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, each training example may pack multiple sequences.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_fused_layernorm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use fused layernorm. &#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;layernorm_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Template for the &#39;</span>
        <span class="s1">&#39;LayerNorm layers. use_fused_layernorm param above overrides the &#39;</span>
        <span class="s1">&#39;layernorm_tpl.use_fused_layernorm for compatibility.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;splits&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;None or a list of layer indices. If None, all layers &#39;</span>
        <span class="s1">&#39;are placed on the same and only one partition. Else, len(splits) is &#39;</span>
        <span class="s1">&#39;the number of partitions the stack is sliced into. layer_i is placed &#39;</span>
        <span class="s1">&#39;on the kth partition (0-based) where split[k] &lt; i &lt;= split[k+1].&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">splits</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">params</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">splits</span><span class="p">)</span>
      <span class="c1"># Assert p.splits is strictly monotonically increasing.</span>
      <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">splits</span><span class="p">)))</span> <span class="o">==</span> <span class="n">params</span><span class="o">.</span><span class="n">splits</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_atten_heads</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span> <span class="o">&lt;</span> <span class="mf">1.0</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_layers should be divisible by &#39;</span>
                         <span class="s1">&#39;transformer_layer_params_tpl&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_LayerParams</span><span class="p">(</span><span class="n">ii</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Construct ii-th layer params.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">ii</span> <span class="o">//</span> <span class="n">factor</span>
        <span class="n">p_ii</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">p_ii</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transformer_layer_params_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ii</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">has_aux_atten</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">mask_self_atten</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mask_self_atten</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">packed_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_atten_heads</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">atten_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">residual_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">add_unnormalized_input</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">add_unnormalized_input</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">residual_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="n">p_ii</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">relu_dropout_prob</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span>
      <span class="k">return</span> <span class="n">p_ii</span>

    <span class="n">layer_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">_LayerParams</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;x_layers&#39;</span><span class="p">,</span> <span class="n">layer_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">:</span>
      <span class="n">final_ln_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">layernorm_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">mdl_dim</span><span class="p">,</span> <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;final_ln&#39;</span><span class="p">,</span> <span class="n">final_ln_p</span><span class="p">)</span>

<div class="viewcode-block" id="StackedTransformerLayers.GetSplitForLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.GetSplitForLayer">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">GetSplitForLayer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">buckets</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">layer_index</span> <span class="o">&lt;=</span> <span class="n">buckets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;layer_index:</span><span class="si">{</span><span class="n">layer_index</span><span class="si">}</span><span class="s1"> &gt; buckets[-1]:</span><span class="si">{</span><span class="n">buckets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1">#  Return index of the smallest element greater than or equal to layer_index</span>
    <span class="k">return</span> <span class="n">bisect</span><span class="o">.</span><span class="n">bisect_left</span><span class="p">(</span><span class="n">buckets</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="StackedTransformerLayers._GetDeviceOfLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers._GetDeviceOfLayer">[docs]</a>  <span class="k">def</span> <span class="nf">_GetDeviceOfLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the device for a given layer index based on our params.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">splits</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">GetSplitForLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">splits</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">))</span></div>

<div class="viewcode-block" id="StackedTransformerLayers.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">query_vec</span><span class="p">,</span>
            <span class="n">paddings</span><span class="p">,</span>
            <span class="n">aux_vec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">aux_segment_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stacked Transformer layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:      [batch, target_time, dim].</span>
<span class="sd">      paddings:       [batch, target_time].</span>
<span class="sd">      aux_vec:        [batch, source_time, dim].</span>
<span class="sd">      aux_paddings:   [batch, source_time].</span>
<span class="sd">      segment_mask:     [batch, 1, target_time, target_time]</span>
<span class="sd">      aux_segment_mask: [batch, 1, target_time, source_time]</span>

<span class="sd">    Returns:</span>
<span class="sd">      (context, paddings), where the context vector has shape [batch,</span>
<span class="sd">      target_time, dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">x_out</span> <span class="o">=</span> <span class="n">query_vec</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">x_out</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_GetDeviceOfLayer</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span>
          <span class="n">x_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
              <span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
              <span class="n">x_in</span><span class="p">,</span>
              <span class="n">paddings</span><span class="p">,</span>
              <span class="n">aux_vec</span><span class="p">,</span>
              <span class="n">aux_paddings</span><span class="p">,</span>
              <span class="n">segment_mask</span><span class="o">=</span><span class="n">segment_mask</span><span class="p">,</span>
              <span class="n">aux_segment_mask</span><span class="o">=</span><span class="n">aux_segment_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="p">:</span>
      <span class="c1"># Place on the last device.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_GetDeviceOfLayer</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">final_ln</span><span class="p">,</span> <span class="n">x_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="StackedTransformerLayers.InitStates"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.InitStates">[docs]</a>  <span class="k">def</span> <span class="nf">InitStates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x_layers</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">InitStates</span><span class="p">(</span><span class="n">layer_theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_theta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">)</span>
    <span class="p">])</span></div>

<div class="viewcode-block" id="StackedTransformerLayers.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StackedTransformerLayers.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding.</span>
<span class="sd">        cached_states.x_layers is a list corresponding to self.x_layers, where</span>
<span class="sd">        each element is a NestedMap with attention keys and values:</span>

<span class="sd">        - key: [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">        - value: [target_time, target_batch, num_heads, dim_per_head].</span>

<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: The last decoder layer output of shape [target_batch, 1, dim].</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      updated_states.x_layers is a list corresponding to self.x_layers, where</span>
<span class="sd">      each element is a NestedMap with attention keys and values:</span>

<span class="sd">      - key: [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      - value: [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">updated_states</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x_layers</span><span class="o">=</span><span class="p">[])</span>
      <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">query_vec</span>
      <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_theta</span><span class="p">,</span> <span class="n">layer_states</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">x_layers</span><span class="p">,</span>
                                                  <span class="n">cached_states</span><span class="o">.</span><span class="n">x_layers</span><span class="p">):</span>
        <span class="n">decoder_output</span><span class="p">,</span> <span class="n">updated_layer_states</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
            <span class="n">layer_theta</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span> <span class="n">aux_paddings</span><span class="p">,</span> <span class="n">layer_states</span><span class="p">,</span>
            <span class="n">time_step</span><span class="p">,</span> <span class="n">use_short_seq_opt</span><span class="p">)</span>
        <span class="n">updated_states</span><span class="o">.</span><span class="n">x_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updated_layer_states</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_output</span>
    <span class="k">return</span> <span class="n">decoder_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId">[docs]</a><span class="k">class</span> <span class="nc">TransformerFeedForwardLayerWithTaskId</span><span class="p">(</span>
    <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;TransformerFeedForwardLayer with optional task_id input args.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_task_ids&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If set, introduces a second attention layer&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TransformerFeedForwardLayerWithTaskId.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Feed-forward, residual and layer-norm.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: [batch, time, dim].</span>
<span class="sd">      paddings: [batch, time]</span>
<span class="sd">      task_id: optional task_id with shape [batch]</span>

<span class="sd">    Returns:</span>
<span class="sd">      tensor of the same shape with inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_task_ids</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">task_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Must pass task_id if use_task_ids.&#39;</span><span class="p">)</span>
    <span class="n">inputs_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;res_proj_layer&#39;</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_proj_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">res_proj_layer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">expanded_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fflayer_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs_normalized</span><span class="p">,</span> <span class="n">expanded_paddings</span><span class="p">]</span>
    <span class="n">fflayer_args</span> <span class="o">+=</span> <span class="p">[</span><span class="n">task_id</span><span class="p">]</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_task_ids</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">residual_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span>
                                                   <span class="o">*</span><span class="n">fflayer_args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">h</span></div></div>


<span class="c1"># TODO(ankurbpn,huangyp): Remove this layer.</span>
<div class="viewcode-block" id="GPipeTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">GPipeTransformerLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;GPipe compatible transformer layer.</span>

<span class="sd">  DEPRECATED: This layer and its use in GPipeTransformerStack is</span>
<span class="sd">  deprecated. Consider using the new GPipeBatchMajorTransformerStack instead.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GPipeTransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span> <span class="o">=</span> <span class="n">TransformerFeedForwardLayerWithTaskId</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">source_paddings</span><span class="p">,</span>
            <span class="n">target_vecs</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span>
            <span class="n">source_segment_id</span><span class="p">,</span>
            <span class="n">target_segment_id</span><span class="p">,</span>
            <span class="n">transparent_acc</span><span class="p">,</span>
            <span class="n">transparent_acc_helper</span><span class="p">,</span>
            <span class="n">source_task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">target_task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>  <span class="c1"># Decoder FProp</span>
        <span class="n">seg_mask</span> <span class="o">=</span> <span class="n">SegmentMask</span><span class="p">(</span><span class="n">target_segment_id</span><span class="p">,</span> <span class="n">target_segment_id</span><span class="p">)</span>
        <span class="n">aux_seg_mask</span> <span class="o">=</span> <span class="n">SegmentMask</span><span class="p">(</span><span class="n">target_segment_id</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
            <span class="n">target_vecs</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">seg_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span>
            <span class="n">atten_vec</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">source_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">aux_seg_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
                                       <span class="n">target_paddings</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">target_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
                <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">target_segment_id</span><span class="p">,</span> <span class="n">transparent_acc</span><span class="p">,</span>
                <span class="n">transparent_acc_helper</span><span class="p">,</span> <span class="n">source_task_id</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span>
      <span class="c1"># Encoder FProp</span>
      <span class="n">seg_mask</span> <span class="o">=</span> <span class="n">SegmentMask</span><span class="p">(</span><span class="n">source_segment_id</span><span class="p">,</span> <span class="n">source_segment_id</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
          <span class="n">source_vecs</span><span class="p">,</span>
          <span class="kc">None</span><span class="p">,</span>
          <span class="n">source_paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">seg_mask</span><span class="p">)</span>
      <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span>
                                     <span class="n">source_task_id</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="k">return</span> <span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">target_vecs</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
              <span class="n">source_segment_id</span><span class="p">,</span> <span class="n">target_segment_id</span><span class="p">,</span> <span class="n">transparent_acc</span><span class="p">,</span>
              <span class="n">transparent_acc_helper</span><span class="p">,</span> <span class="n">source_task_id</span><span class="p">,</span> <span class="n">target_task_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="n">flops_per_element</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">source_batch</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="n">flops_per_element</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">source_batch</span> <span class="o">*</span> <span class="n">dim</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.SetupDeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.SetupDeterministicDropout">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetupDeterministicDropout</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replaced dropout layers in transformer with deterministic ones.&quot;&quot;&quot;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">residual_dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">params</span></div>

<div class="viewcode-block" id="GPipeTransformerLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeTransformerLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head]. value -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      task_id: [batch_size]: the input task_id meta information.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [target_batch, 1, dim]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      value - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_batch</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># First the self-attention layer.</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Next the cross-attention layer.</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span>
                                          <span class="n">aux_paddings</span><span class="p">)</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">atten_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">task_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cur_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer">[docs]</a><span class="k">class</span> <span class="nc">GPipeBatchMajorTransformerLayer</span><span class="p">(</span><span class="n">TransformerLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;GPipe compatible batch majortransformer layer.</span>

<span class="sd">  To be used with the new GPipeBatchMajorStack.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Layer norm default params. No layernorm if set to None.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_layer_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to layer normalize the output of the layer.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="c1"># Initialize output layer norm.</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_layer_norm</span><span class="p">:</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;output_ln&#39;</span>
      <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">target_vecs</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span> <span class="n">encoder_self_atten_segment_mask</span><span class="p">,</span>
            <span class="n">decoder_self_atten_segment_mask</span><span class="p">,</span> <span class="n">decoder_cross_atten_segment_mask</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_aux_atten</span><span class="p">:</span>  <span class="c1"># Decoder FProp</span>
        <span class="n">sa_mask</span><span class="p">,</span> <span class="n">ca_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
          <span class="c1"># This computation doesn&#39;t behave nicely when outside</span>
          <span class="c1"># recurrent.Recurrent resulting in nans for splits &gt; 1</span>
          <span class="n">min_val</span> <span class="o">=</span> <span class="n">GetDtypeMin</span><span class="p">(</span><span class="n">decoder_self_atten_segment_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
          <span class="c1"># Operator overloading with * produces type-errors when running on</span>
          <span class="c1"># borg with splits &gt; 1.</span>
          <span class="n">sa_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">decoder_self_atten_segment_mask</span><span class="p">)</span>
          <span class="n">ca_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">decoder_cross_atten_segment_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
            <span class="n">target_vecs</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">target_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">sa_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span>
            <span class="n">atten_vec</span><span class="p">,</span>
            <span class="n">source_vecs</span><span class="p">,</span>
            <span class="n">source_paddings</span><span class="p">,</span>
            <span class="n">segment_mask</span><span class="o">=</span><span class="n">ca_mask</span><span class="p">)</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
                                       <span class="n">target_paddings</span><span class="p">)</span>
        <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">target_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_layer_norm</span><span class="p">:</span>
          <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">source_vecs</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
                <span class="n">encoder_self_atten_segment_mask</span><span class="p">,</span>
                <span class="n">decoder_self_atten_segment_mask</span><span class="p">,</span>
                <span class="n">decoder_cross_atten_segment_mask</span><span class="p">)</span>

      <span class="c1"># Encoder FProp</span>
      <span class="n">sa_mask</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="n">GetDtypeMin</span><span class="p">(</span><span class="n">encoder_self_atten_segment_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">sa_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">encoder_self_atten_segment_mask</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span>
          <span class="n">source_vecs</span><span class="p">,</span>
          <span class="kc">None</span><span class="p">,</span>
          <span class="n">source_paddings</span><span class="p">,</span>
          <span class="n">segment_mask</span><span class="o">=</span><span class="n">sa_mask</span><span class="p">)</span>
      <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">)</span>
      <span class="n">atten_vec</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">source_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_layer_norm</span><span class="p">:</span>
        <span class="n">atten_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">)</span>

      <span class="k">return</span> <span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="n">source_paddings</span><span class="p">,</span> <span class="n">target_vecs</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span>
              <span class="n">encoder_self_atten_segment_mask</span><span class="p">,</span> <span class="n">decoder_self_atten_segment_mask</span><span class="p">,</span>
              <span class="n">decoder_cross_atten_segment_mask</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="n">flops_per_element</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">source_batch</span><span class="p">,</span> <span class="n">src_time</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="n">flops_per_element</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">src_time</span> <span class="o">*</span> <span class="n">source_batch</span> <span class="o">*</span> <span class="n">dim</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">)</span></div>

<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer.SetupDeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.SetupDeterministicDropout">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetupDeterministicDropout</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replaced dropout layers in transformer with deterministic ones.&quot;&quot;&quot;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">residual_dropout_tpl</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="n">params</span><span class="o">.</span><span class="n">tr_fflayer_tpl</span><span class="o">.</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">params</span></div>

<div class="viewcode-block" id="GPipeBatchMajorTransformerLayer.ExtendStep"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.ExtendStep">[docs]</a>  <span class="k">def</span> <span class="nf">ExtendStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">theta</span><span class="p">,</span>
                 <span class="n">query_vec</span><span class="p">,</span>
                 <span class="n">aux_vec</span><span class="p">,</span>
                 <span class="n">aux_paddings</span><span class="p">,</span>
                 <span class="n">cached_states</span><span class="p">,</span>
                 <span class="n">time_step</span><span class="p">,</span>
                 <span class="n">use_short_seq_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer decoder layer, extend one step in autoregressive decoding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      query_vec:    [target_batch, 1, dim].</span>
<span class="sd">      aux_vec:      [source_batch, source_time, dim]</span>
<span class="sd">      aux_paddings: [source_batch, source_time]</span>
<span class="sd">      cached_states: A `.NestedMap` object containing tensors which are the</span>
<span class="sd">        results of previous attentions, used for fast decoding. key   -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head]. value -</span>
<span class="sd">        [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      time_step: A scalar, the current decode step, 0-based.</span>
<span class="sd">      use_short_seq_opt: A bool, whether using short sequence optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      cur_output: [target_batch, 1, dim]</span>
<span class="sd">      updated_states: A `.NestedMap` object containing the updated states.</span>
<span class="sd">      key   - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">      value - [target_time, target_batch, num_heads, dim_per_head].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_batch</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">source_batch</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">aux_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># First the self-attention layer.</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">updated_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_atten</span><span class="o">.</span><span class="n">ExtendStep</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">self_atten</span><span class="p">,</span> <span class="n">query_vec</span><span class="p">,</span> <span class="n">cached_states</span><span class="p">,</span> <span class="n">time_step</span><span class="p">,</span>
        <span class="n">use_short_seq_opt</span><span class="p">)</span>

    <span class="c1"># Next the cross-attention layer.</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">atten_vec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">cross_atten</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span> <span class="n">aux_vec</span><span class="p">,</span>
                                          <span class="n">aux_paddings</span><span class="p">)</span>
    <span class="n">atten_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">atten_vec</span><span class="p">,</span> <span class="p">[</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Finally the feed-forward layer.</span>
    <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">fflayer</span><span class="p">,</span> <span class="n">atten_vec</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">target_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">atten_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">output_layer_norm</span><span class="p">:</span>
      <span class="n">cur_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">cur_output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cur_output</span><span class="p">,</span> <span class="n">updated_states</span></div></div>


<div class="viewcode-block" id="ResidualAddLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ResidualAddLayer">[docs]</a><span class="k">class</span> <span class="nc">ResidualAddLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer to add inputs with residual weight.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="ResidualAddLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ResidualAddLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `ResidualAddLayer`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;residual_weight&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Residual weight.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="ResidualAddLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ResidualAddLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return combined inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: weights defined in this layer.</span>
<span class="sd">      x: input tensor.</span>
<span class="sd">      y: input tensor to apply weight to.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Added tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_weight</span> <span class="o">*</span> <span class="n">y</span></div>

<div class="viewcode-block" id="ResidualAddLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.ResidualAddLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="PaddingLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PaddingLayer">[docs]</a><span class="k">class</span> <span class="nc">PaddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer that applies paddings to the inputs.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="PaddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PaddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return combined inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: weights defined in this layer.</span>
<span class="sd">      inputs: input tensor.</span>
<span class="sd">      paddings: paddings tensor, should be of shape tf.shape(inputs)[:-1].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor with paddings applied.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="PaddingLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.PaddingLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">(),</span> <span class="n">paddings</span><span class="o">.</span><span class="n">num_elements</span><span class="p">())</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="StrideLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StrideLayer">[docs]</a><span class="k">class</span> <span class="nc">StrideLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer that does stride.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="StrideLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StrideLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for `StrideLayer`.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;To use every k-th token, set the stride to k. When &#39;</span>
        <span class="s1">&#39;stride == 0, only returns the first token of the input. When &#39;</span>
        <span class="s1">&#39;stride == 1, returns every token in the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;first_n&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;only considers the first N tokens for the &#39;</span>
        <span class="s1">&#39;output. We use [:first_n:stride] to select the output tokens. If &#39;</span>
        <span class="s1">&#39;first_n is None, this flag is a no-op. If stride is positive, the&#39;</span>
        <span class="s1">&#39; output sequence length is &quot;(first_n-1) // stride + 1&quot;. If stride&#39;</span>
        <span class="s1">&#39; is 0, first_n has to be None or 1. first_n ca not be 0. If &#39;</span>
        <span class="s1">&#39;first_n &lt;= stride, only the first token is used.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="StrideLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StrideLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies stride to the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: weights defined in this layer.</span>
<span class="sd">      x: input tensor, [batch, time, ...]. Stride is applied to the time dim.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Strided tensor, with the stride applied to the second dim in x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="n">p</span><span class="o">.</span><span class="n">first_n</span><span class="p">:</span><span class="n">p</span><span class="o">.</span><span class="n">stride</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">::</span><span class="n">p</span><span class="o">.</span><span class="n">stride</span><span class="p">]</span></div>

<div class="viewcode-block" id="StrideLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.StrideLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">x</span><span class="p">,))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">flops</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),))</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">first_n</span><span class="p">:</span>
      <span class="c1"># out_seq_len is 1 if first_n is 1 ~ stride and is 2 if it&#39;s stride+1 ~</span>
      <span class="c1"># 2*stride...</span>
      <span class="n">out_seq_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">first_n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">flops</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_seq_len</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),))</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,))</span>

    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),))</span></div></div>


<span class="c1"># pyformat: disable</span>
<div class="viewcode-block" id="Builder"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder">[docs]</a><span class="k">class</span> <span class="nc">Builder</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">Base</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Builder for self-attention layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Builder.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;model_dim&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Model dim of this layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Number of heads in the atten layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_hidden_dim&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Hidden dim of the feedforward layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_hidden_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Hidden dim of the attention layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;residual_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Dropout prob to the output of each sub-layer before it is added &#39;</span>
             <span class="s1">&#39;to the sub-layer input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_activation_fn&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Activation function in Feedforward layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_residual_weight&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Weight given to F(x) in the residual &#39;</span>
             <span class="s1">&#39;connection: y = x + ff_residual_weight * F(x), in Feedforward &#39;</span>
             <span class="s1">&#39;layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;relu_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the hidden layer of &#39;</span>
             <span class="s1">&#39;feed-forward network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_dropout_prob&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Probability at which we apply dropout to the attention layer&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;selfatten_add_unnormalized_input&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use unnormalized input in the residual add.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;selfatten_enable_value_proj&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether value v is pre-projected before self attention or not.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;conv_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Activation function for convolution layer in Builder.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_splits&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Number of model parallelism splits.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_micro_batches&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Number of spatial partition along the batch dimension. &#39;</span>
             <span class="s1">&#39;When num_micro_batches &gt; 1, the effective batch size of the &#39;</span>
             <span class="s1">&#39;intermediate activation is batch_size // num_micro_batches.&#39;</span>
             <span class="s1">&#39;This allows models to try larger batch size which might improve &#39;</span>
             <span class="s1">&#39;model quality&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;glu_with_tanh&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If the Gated Linear Unit should apply tanh on the activation &#39;</span>
             <span class="s1">&#39;input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;packed_input&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to support packed input&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_per_dim_scale&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Whether using per_dim_scale or scaling by a constant factor.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_fused_layernorm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use fused layernorm. &#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;layernorm_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Template for the &#39;</span>
             <span class="s1">&#39;LayerNorm layers. use_fused_layernorm param above overrides the &#39;</span>
             <span class="s1">&#39;layernorm_tpl.use_fused_layernorm for compatibility.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use bias for projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;norm_layer_tpl&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;If specified, the normalization layer template.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;enable_scaling_code_motion&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Move scalings from the side &#39;</span>
        <span class="s1">&#39;of T^2 to the side of T for better performance. This may result &#39;</span>
        <span class="s1">&#39;in model quality drops when using bf16 for some models due to &#39;</span>
        <span class="s1">&#39;different XLA fusion decisions.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span>

<div class="viewcode-block" id="Builder._Dropout"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Dropout">[docs]</a>  <span class="k">def</span> <span class="nf">_Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a DropoutLayer Params.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">drop_prob</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Add"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Add">[docs]</a>  <span class="k">def</span> <span class="nf">_Add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">residual_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ResidualAddLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                                         <span class="n">residual_weight</span><span class="o">=</span><span class="n">residual_weight</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._DefaultLN"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._DefaultLN">[docs]</a>  <span class="k">def</span> <span class="nf">_DefaultLN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Layer norm with default params.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">layernorm_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">use_fused_layernorm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._ExpandDims"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._ExpandDims">[docs]</a>  <span class="k">def</span> <span class="nf">_ExpandDims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Squeeze"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Squeeze">[docs]</a>  <span class="k">def</span> <span class="nf">_Squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Glu"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Glu">[docs]</a>  <span class="k">def</span> <span class="nf">_Glu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_GLUFn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">act_inputs</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_GatedTanhFn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">act_inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">)</span>

    <span class="n">fn</span> <span class="o">=</span> <span class="n">_GatedTanhFn</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">glu_with_tanh</span> <span class="k">else</span> <span class="n">_GLUFn</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">fn</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span>
                    <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]),</span>
                    <span class="n">fn_flops</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">15</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Pad"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Pad">[docs]</a>  <span class="k">def</span> <span class="nf">_Pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">PaddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._MultiHeadedAtten"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._MultiHeadedAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_MultiHeadedAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a MultiHeadedAttention params.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">num_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="n">atten_p</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">attention_hidden_dim</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">enable_value_proj</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">selfatten_enable_value_proj</span><span class="p">,</span>
        <span class="n">enable_per_dim_scale</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span><span class="p">,</span>
        <span class="n">packed_input</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">enable_scaling_code_motion</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">enable_scaling_code_motion</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span><span class="p">:</span>
      <span class="n">atten_p</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">atten_p</span></div>

<div class="viewcode-block" id="Builder.Feedforward"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Feedforward">[docs]</a>  <span class="k">def</span> <span class="nf">Feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ff_hidden_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">is_causal</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">ff_hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ff_hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span>
    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;after_feedforward&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;feedforward&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_DefaultLN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">),</span>  <span class="c1"># LN with default params.</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear01&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">ff_hidden_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias01&#39;</span><span class="p">,</span> <span class="n">ff_hidden_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Activation</span><span class="p">(</span><span class="s1">&#39;act&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_activation_fn</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;relu_dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relu_dropout_prob</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear02&#39;</span><span class="p">,</span> <span class="n">ff_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias02&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,after_feedforward-&gt;added&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_residual_weight</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;added,i.paddings-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Pad</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)),</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._MaybeSplit"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._MaybeSplit">[docs]</a>  <span class="k">def</span> <span class="nf">_MaybeSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">num_layers</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span>
    <span class="n">layers_per_split</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">cells</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">blocks</span><span class="p">:</span>
      <span class="n">head</span><span class="p">,</span> <span class="n">blocks</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">[:</span><span class="n">layers_per_split</span><span class="p">],</span> <span class="n">blocks</span><span class="p">[</span><span class="n">layers_per_split</span><span class="p">:]</span>
      <span class="n">cells</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="s1">&#39;cell_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cells</span><span class="p">)),</span> <span class="o">*</span><span class="n">head</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">cells</span><span class="p">)</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">num_splits</span>

    <span class="k">return</span> <span class="n">gpipe</span><span class="o">.</span><span class="n">PipeliningLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">cell_tpl</span><span class="o">=</span><span class="n">cells</span><span class="p">,</span>
        <span class="n">nested_map_fprop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_micro_batches</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_micro_batches</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._DepthwiseConv2D"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._DepthwiseConv2D">[docs]</a>  <span class="k">def</span> <span class="nf">_DepthwiseConv2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A depthwise convolution block for lightweight conv.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">conv_builder_params</span> <span class="o">=</span> <span class="n">conv_layers</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">norm_layer_tpl</span><span class="p">:</span>
      <span class="n">conv_builder_params</span><span class="o">.</span><span class="n">norm_layer_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">norm_layer_tpl</span>
    <span class="n">conv_builder</span> <span class="o">=</span> <span class="n">conv_builder_params</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">conv_builder</span><span class="o">.</span><span class="n">DepthwiseConv2D</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">depth_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">filter_shape</span><span class="o">=</span><span class="p">[</span><span class="n">filter_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">conv_activation</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._NormalizedDepthwiseConv2D"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._NormalizedDepthwiseConv2D">[docs]</a>  <span class="k">def</span> <span class="nf">_NormalizedDepthwiseConv2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A depthwise convolution block for lightweight conv.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">conv_builder_params</span> <span class="o">=</span> <span class="n">conv_layers</span><span class="o">.</span><span class="n">Builder</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">conv_builder</span> <span class="o">=</span> <span class="n">conv_builder_params</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">conv_builder</span><span class="o">.</span><span class="n">NormalizedDepthwiseConv2D</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">dropconnect_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">deterministic_dropout</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LConv"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LConv">[docs]</a>  <span class="k">def</span> <span class="nf">LConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">convolution_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;[DEPRECATED] A lightweight convolution block as described in.</span>

<span class="sd">    Use conv_layers_builder.LConv() instead.</span>

<span class="sd">    https://arxiv.org/abs/1901.10430</span>
<span class="sd">    Corresponding PyTorch Implementation (L587):</span>
<span class="sd">    https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/models/lightconv.py</span>


<span class="sd">    This block can be used as an alternative to self-attention block.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the params</span>
<span class="sd">      kernel_size: kernel size used in the conv layer.</span>
<span class="sd">      is_causal: is causal padding or not.</span>
<span class="sd">      convolution_fn: Convolution to apply, default _NormalizedDepthwiseConv2D.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A LightWeightConvLayerBlock layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">convolution_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">convolution_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_NormalizedDepthwiseConv2D&#39;</span><span class="p">)</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;pre_conv&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;pre_conv&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_DefaultLN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Glu</span><span class="p">(</span><span class="s1">&#39;glu&#39;</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_ExpandDims</span><span class="p">(</span><span class="s1">&#39;expand&#39;</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;pre_conv,i.paddings-&gt;post_conv,o.paddings&#39;</span><span class="p">,</span>
         <span class="n">convolution_fn</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;post_conv-&gt;after_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
             <span class="s1">&#39;post_conv&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Squeeze</span><span class="p">(</span><span class="s1">&#39;squeeze&#39;</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">),</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,after_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LconvBlock"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LconvBlock">[docs]</a>  <span class="k">def</span> <span class="nf">LconvBlock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">,</span>
                 <span class="n">convolution_fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A lightweight conv block followed by a feedforward one.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LConv</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lconv&#39;</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
            <span class="n">convolution_fn</span><span class="o">=</span><span class="n">convolution_fn</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Feedforward</span><span class="p">(</span><span class="s1">&#39;ff&#39;</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">))</span></div>

<div class="viewcode-block" id="Builder.Seq"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Seq">[docs]</a>  <span class="k">def</span> <span class="nf">Seq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">subs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of sequential layers.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">subs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.LConvStack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.LConvStack">[docs]</a>  <span class="k">def</span> <span class="nf">LConvStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_sizes</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of LConv layers with kernel size in kernel_sizes.&quot;&quot;&quot;</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kernel_sizes</span><span class="p">):</span>
      <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">LconvBlock</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;block_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
              <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
              <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
              <span class="n">convolution_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">blocks</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._Stride"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._Stride">[docs]</a>  <span class="k">def</span> <span class="nf">_Stride</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">first_n</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Strides the input sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of this layer.</span>
<span class="sd">      stride: To use every k-th token, set the stride to k. When stride == 0,</span>
<span class="sd">        only returns the first token of the input. When stride == 1, returns</span>
<span class="sd">        every token in the input.</span>
<span class="sd">      first_n: only considers the first N tokens for the output. We use</span>
<span class="sd">        [:first_n:stride] to select the output tokens. If first_n is None, this</span>
<span class="sd">        flag is a no-op. If stride is positive, the output sequence length is</span>
<span class="sd">        &quot;(first_n-1) // stride + 1&quot;. If stride is 0, first_n has to be None or</span>
<span class="sd">        1. first_n can&#39;t be 0. If first_n &lt;= stride, only the first token is</span>
<span class="sd">        used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A layer params that does stride.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">StrideLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">first_n</span><span class="o">=</span><span class="n">first_n</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder._StridedAttention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder._StridedAttention">[docs]</a>  <span class="k">def</span> <span class="nf">_StridedAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">first_n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes self attention with optional stride.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of this layer.</span>
<span class="sd">      stride: If omitted, the default is 1: use every token in the query. To use</span>
<span class="sd">        every k-th token, set the stride to k. When set to 0, only use the first</span>
<span class="sd">        token of the query.</span>
<span class="sd">      first_n: only considers the first N tokens for the output. We use</span>
<span class="sd">        [:first_n:stride] to select the output tokens. If first_n is None, this</span>
<span class="sd">        flag is a no-op. If stride is positive, the output sequence length is</span>
<span class="sd">        &quot;(first_n-1) // stride + 1&quot;. If stride is 0, first_n has to be None or</span>
<span class="sd">        1. first_n can&#39;t be 0. If first_n &lt;= stride, only the first token is</span>
<span class="sd">        used.</span>
<span class="sd">      num_heads: the number of heads.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A self attention layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">input_to_add</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;i.vec&#39;</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">selfatten_add_unnormalized_input</span> <span class="k">else</span> <span class="s1">&#39;after_ln&#39;</span><span class="p">)</span>

    <span class="n">attention_inputs</span> <span class="o">=</span> <span class="s1">&#39;strided_query,after_ln,after_ln,i.paddings&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">attention_inputs</span> <span class="o">+=</span> <span class="s1">&#39;,i.segment_mask&#39;</span>

    <span class="k">if</span> <span class="n">num_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;after_ln&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_DefaultLN</span><span class="p">(</span><span class="s1">&#39;LN&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;after_ln-&gt;strided_query&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;query_after_stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">first_n</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">-&gt;after_att,prob&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attention_inputs</span><span class="p">),</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_MultiHeadedAtten</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;after_att-&gt;after_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">-&gt;strided_input&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_to_add</span><span class="p">),</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;before_add&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">first_n</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;strided_input,after_dropout-&gt;o.vec&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Stride</span><span class="p">(</span><span class="s1">&#39;padding_after_Stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">first_n</span><span class="p">)),</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">packed_input</span><span class="p">:</span>
      <span class="n">sub_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;i.segment_mask-&gt;o.segment_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;segment_mask&#39;</span><span class="p">)))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings, segment_mask}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings, segment_mask}</span>
        <span class="o">*</span><span class="n">sub_list</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.TransformerEncoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.TransformerEncoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">TransformerEncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">first_n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">ff_hidden_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;(inputs, paddings) -&gt; (encoded, paddings).</span>

<span class="sd">    Args:</span>
<span class="sd">      name: the string name of the encoder layer params.</span>
<span class="sd">      stride: To use every k-th token, set the stride to k. When stride == 0,</span>
<span class="sd">        only returns the first token of the input. When stride == 1, returns</span>
<span class="sd">        every token in the input.</span>
<span class="sd">      first_n: only considers the first N tokens for the output. We use</span>
<span class="sd">        [:first_n:stride] to select the output tokens. If first_n is None, this</span>
<span class="sd">        flag is a no-op. If stride is positive, the output sequence length is</span>
<span class="sd">        &quot;(first_n-1) // stride + 1&quot;. If stride is 0, first_n has to be None or</span>
<span class="sd">        1. first_n can&#39;t be 0. If first_n &lt;= stride, only the first token is</span>
<span class="sd">        used.</span>
<span class="sd">      ff_hidden_dim: The feed forward layer&#39;s hidden dimension. If specified,</span>
<span class="sd">        this will override p.ff_hidden_dim.</span>
<span class="sd">      num_heads: The number of heads for the multi-head attention module. If</span>
<span class="sd">        specified, this will override p.num_heads.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A transformer encoder layer params that supports optional stride.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">ff_hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ff_hidden_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span>
    <span class="k">if</span> <span class="n">num_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">num_heads</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_heads</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
        <span class="s1">&#39;block&#39;</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_StridedAttention</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                               <span class="n">first_n</span><span class="o">=</span><span class="n">first_n</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Feedforward</span><span class="p">(</span><span class="s1">&#39;ff&#39;</span><span class="p">,</span> <span class="n">ff_hidden_dim</span><span class="o">=</span><span class="n">ff_hidden_dim</span><span class="p">)))</span></div>

<div class="viewcode-block" id="Builder.Stack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.Stack">[docs]</a>  <span class="k">def</span> <span class="nf">Stack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of sequential layers.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">blocks</span><span class="p">)</span></div>

<div class="viewcode-block" id="Builder.TransformerEncoderStack"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.Builder.TransformerEncoderStack">[docs]</a>  <span class="k">def</span> <span class="nf">TransformerEncoderStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a stack of num_layers self-attention layers.&quot;&quot;&quot;</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;iter_</span><span class="si">{:0&gt;3d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span></div></div>
<span class="c1"># pyformat: enable</span>


<div class="viewcode-block" id="LmBuilder"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder">[docs]</a><span class="k">class</span> <span class="nc">LmBuilder</span><span class="p">(</span><span class="n">Builder</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Langange model builder with causal padding.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="LmBuilder.Params"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;xla_num_partitions&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of SPMD partitions.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;Datatype to use.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="LmBuilder._ShardedVar"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._ShardedVar">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedVar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">split_dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">moe_layers</span><span class="o">.</span><span class="n">ShardedVarLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
        <span class="n">split_dimension</span><span class="o">=</span><span class="n">split_dim</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">xla_num_partitions</span><span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder._LinearWeight"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._LinearWeight">[docs]</a>  <span class="k">def</span> <span class="nf">_LinearWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">split_dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span>
                  <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">],</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">((</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">input_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">))],</span>
        <span class="n">split_dim</span><span class="o">=</span><span class="n">split_dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder._Linear"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._Linear">[docs]</a>  <span class="k">def</span> <span class="nf">_Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">split_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LinearWeight</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">split_dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;inputs,w-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
             <span class="s1">&#39;linear&#39;</span><span class="p">,</span>
             <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLI,IO-&gt;BLO&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">))),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder._BiasWeight"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._BiasWeight">[docs]</a>  <span class="k">def</span> <span class="nf">_BiasWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span>
                  <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">dim</span><span class="p">],</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">))],</span>
        <span class="n">split_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder._Bias"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._Bias">[docs]</a>  <span class="k">def</span> <span class="nf">_Bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;b&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_BiasWeight</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;inputs,b-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span>
                                       <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">b</span><span class="p">)),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder.Feedforward"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder.Feedforward">[docs]</a>  <span class="k">def</span> <span class="nf">Feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">ff_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DefaultLN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear01&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">,</span> <span class="n">split_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">ff_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias01&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">))</span>
    <span class="n">ff_list</span> <span class="o">+=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Activation</span><span class="p">(</span><span class="s1">&#39;act&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_activation_fn</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;relu_dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relu_dropout_prob</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Linear</span><span class="p">(</span><span class="s1">&#39;linear02&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">split_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">ff_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Bias</span><span class="p">(</span><span class="s1">&#39;bias02&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">))</span>
    <span class="n">ff_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">))</span>

    <span class="n">sub_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;after_feedforward&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="s1">&#39;feedforward&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">ff_list</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,after_feedforward-&gt;added&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span>
                                                     <span class="n">p</span><span class="o">.</span><span class="n">ff_residual_weight</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;added,i.paddings-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Pad</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)),</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings}</span>
        <span class="o">*</span><span class="n">sub_list</span><span class="p">)</span></div>

<div class="viewcode-block" id="LmBuilder._Attention"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder._Attention">[docs]</a>  <span class="k">def</span> <span class="nf">_Attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes self attention with optional stride.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of this layer.</span>
<span class="sd">      is_causal: If true, add cause per_step padding to the attention layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A self attention layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">tr_atten_p</span> <span class="o">=</span> <span class="n">TransformerAttentionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;transformer_atten&#39;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">attention_hidden_dim</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">is_masked</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">residual_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">residual_dropout_prob</span><span class="p">,</span>
        <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_dropout_prob</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
        <span class="n">add_unnormalized_input</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">selfatten_add_unnormalized_input</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span>
    <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">enable_value_proj</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">selfatten_enable_value_proj</span>
    <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">enable_per_dim_scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_per_dim_scale</span>
    <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">xla_num_partitions</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span><span class="p">:</span>
      <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">tr_atten_p</span><span class="o">.</span><span class="n">atten_p</span><span class="o">.</span><span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DeterministicDropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>  <span class="c1"># input NestedMap with {vec, paddings}</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>  <span class="c1"># output NestedMap with {vec, paddings}</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,i.vec,i.paddings-&gt;o.vec,unused_prob&#39;</span><span class="p">,</span> <span class="n">tr_atten_p</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;i.paddings-&gt;o.paddings&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Id</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)))</span></div>

<div class="viewcode-block" id="LmBuilder.TransformerEncoderLayer"><a class="viewcode-back" href="../../../lingvo.core.batch_major_attention.html#lingvo.core.batch_major_attention.LmBuilder.TransformerEncoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">TransformerEncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;(inputs, paddings) -&gt; (encoded, paddings).</span>

<span class="sd">    Args:</span>
<span class="sd">      name: the string name of the encoder layer params.</span>
<span class="sd">      is_causal: If true, add cause per_step padding to the attention layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A transformer encoder layer params that supports optional stride.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Hack to be compatible with ckpt generated by self._rep</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="s1">&#39;block&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Attention</span><span class="p">(</span><span class="s1">&#39;self_atten&#39;</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">),</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">Feedforward</span><span class="p">(</span><span class="s1">&#39;ff&#39;</span><span class="p">)))</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>