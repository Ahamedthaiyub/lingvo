

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.core.conformer_layer &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.conformer_layer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.conformer_layer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2020 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Conformer layers as in https://arxiv.org/abs/2005.08100.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">lingvo</span> <span class="kn">import</span> <span class="n">compat</span> <span class="k">as</span> <span class="n">tf</span>

<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">activations</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">batch_major_attention</span> <span class="k">as</span> <span class="n">attention_lib</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">bn_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">conv_layers_with_time_padding</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_builder</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span> <span class="k">as</span> <span class="n">hparams_lib</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers_with_attention</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">recurrent</span>


<div class="viewcode-block" id="LConvLayer"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer">[docs]</a><span class="k">class</span> <span class="nc">LConvLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Lightweight conv layer.</span>

<span class="sd">  architecture::</span>

<span class="sd">    input</span>
<span class="sd">    /   \</span>
<span class="sd">    |   ln(.)                   # input_dim</span>
<span class="sd">    |   fflayer(.)              # 2 * input_dim</span>
<span class="sd">    |    |</span>
<span class="sd">    |   glu(.)                  # input_dim</span>
<span class="sd">    |   depthwise_conv_1d(.)</span>
<span class="sd">    |   norm(.)</span>
<span class="sd">    |   act(.)</span>
<span class="sd">    |    |</span>
<span class="sd">    |   fflayer(.)</span>
<span class="sd">    |   dropout(.)</span>
<span class="sd">    \   /</span>
<span class="sd">      +</span>
<span class="sd">      |</span>
<span class="sd">    output</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LConvLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Input and (in fact,) output dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Kernel size of 1d deptwise conv.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;conv_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;SWISH&#39;</span><span class="p">,</span> <span class="s1">&#39;Activation after normalization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;is_causal&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether this is a causal layer.&#39;</span>
        <span class="s1">&#39;If set to true, use &#39;</span>
        <span class="s1">&#39;conv_layers_with_time_padding.CausalDepthwiseConv2DLayer for &#39;</span>
        <span class="s1">&#39;`depthwise_conv_tpl`.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;glu_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Activation in GLU. Check lingvo.core.activations._ACTIVATIONS for &#39;</span>
        <span class="s1">&#39;other options.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="s1">&#39;Dropout probability.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Input layer norm template.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;linear_start_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">FCLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Linear start layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;depthwise_conv_tpl&#39;</span><span class="p">,</span>
        <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">DepthwiseConv2DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Depthwise conv template. For causal layer, use &#39;</span>
        <span class="s1">&#39;conv_layers_with_time_padding.CausalDepthwiseConv2DLayer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;conv_norm_layer_tpl&#39;</span><span class="p">,</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">BatchNormLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Normalization layer after conv.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;linear_end_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">FCLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Linear end layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Residual dropout layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;split_act_gated_linear_start&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Separate act and gated linear start to remove data formatting &#39;</span>
        <span class="s1">&#39;overheads&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">linear_start_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;NONE&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">linear_end_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;NONE&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># SPMD partition related params.</span>
    <span class="c1">#</span>
    <span class="c1"># d - model_dim</span>
    <span class="c1"># f - ff_hidden_dim (here ff_hidden_dim has the same size as model_dim)</span>
    <span class="c1"># h - height</span>
    <span class="c1"># w - width</span>
    <span class="c1"># i - in_channels</span>
    <span class="c1"># m - channel_multiplier</span>
    <span class="c1"># b - batch_size</span>
    <span class="c1"># l - seq_len</span>
    <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="n">hparams_lib</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">wp</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;df&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Mesh split for lconv linear start weight with the shape of &#39;</span>
        <span class="s1">&#39;[model_dim, ff_hidden_dim], the default hidden_dim is the same as &#39;</span>
        <span class="s1">&#39;the model_dim.&#39;</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;hwim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Mesh split for lconv depthwise conv weight with the shape of &#39;</span>
        <span class="s1">&#39;[height, width, in_channels, channel_multiplier]. Width and &#39;</span>
        <span class="s1">&#39;channel_multiplier are both 1 for the common use case.&#39;</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fd&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mesh split for lconv linear end weight with the shape of &#39;</span>
        <span class="s1">&#39;[ff_hidden_dim, model_dim], the default hidden_dim is the same as &#39;</span>
        <span class="s1">&#39;the model_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span> <span class="o">=</span> <span class="n">hparams_lib</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">ap</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span>
    <span class="n">ap</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;blf&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mesh split for lconv linear start activation and lconv &#39;</span>
        <span class="s1">&#39;depthwise conv after normalization with the shape of &#39;</span>
        <span class="s1">&#39;[batch_size, seq_len, ff_hidden_dim], the default hidden_dim is the &#39;</span>
        <span class="s1">&#39;same as model_dim.&#39;</span><span class="p">)</span>
    <span class="n">ap</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;bld&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Mesh split for lconv linear end activation with the shape of &#39;</span>
        <span class="s1">&#39;[batch_size, seq_len, model_dim].&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="LConvLayer.SetCanonicalShardingParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.SetCanonicalShardingParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetCanonicalShardingParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set up canonical SPMD sharding params.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">params</span><span class="o">.</span><span class="n">device_mesh</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="n">wp</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">hwim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># TODO(shibow/rpang): understand the effects of fd sharding, especially why</span>
    <span class="c1"># [-1, -1] performs better when bld is [0, -1, -1].</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">fd</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ap</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span>
    <span class="n">ap</span><span class="o">.</span><span class="n">blf</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ap</span><span class="o">.</span><span class="n">bld</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span></div>

<div class="viewcode-block" id="LConvLayer.CommonParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.CommonParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">CommonParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                   <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">conv_activation</span><span class="o">=</span><span class="s1">&#39;SWISH&#39;</span><span class="p">,</span>
                   <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">conv_activation</span><span class="o">=</span><span class="n">conv_activation</span><span class="p">,</span>
        <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_causal</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">depthwise_conv_tpl</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">CausalDepthwiseConv2DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="LConvLayer.SetFPropDtype"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.SetFPropDtype">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetFPropDtype</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span>
    <span class="k">if</span> <span class="n">fprop_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="c1"># Depthwise conv supports bfloat16 only on TPUs.</span>
      <span class="n">p</span><span class="o">.</span><span class="n">depthwise_conv_tpl</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
      <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">BatchNormLayer</span><span class="p">):</span>
        <span class="c1"># Batch norm does not support bfloat16 on TPUs.</span>
        <span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">ln_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">ln_p</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">split_act_gated_linear_start</span><span class="p">:</span>
      <span class="n">linear_start_act_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">linear_start_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="o">.</span><span class="n">df</span><span class="p">,</span>
          <span class="n">activation_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blf</span><span class="p">)</span>
      <span class="n">linear_start_gated_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">linear_start_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="o">.</span><span class="n">df</span><span class="p">,</span>
          <span class="n">activation_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blf</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;linear_start_act&#39;</span><span class="p">,</span> <span class="n">linear_start_act_p</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;linear_start_gated&#39;</span><span class="p">,</span> <span class="n">linear_start_gated_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">linear_start_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">linear_start_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;linear_start&#39;</span><span class="p">,</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">output_dim</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;linear_start&#39;</span><span class="p">,</span> <span class="n">linear_start_p</span><span class="p">)</span>

    <span class="n">linear_end_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">linear_end_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;linear_end&#39;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
        <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="o">.</span><span class="n">fd</span><span class="p">,</span>
        <span class="n">activation_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">bld</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;linear_end&#39;</span><span class="p">,</span> <span class="n">linear_end_p</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">:</span>
      <span class="n">norm_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;norm_layer&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">norm_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;norm_layer&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">GroupNormLayer</span><span class="p">:</span>
      <span class="n">norm_p</span><span class="o">.</span><span class="n">cumulative</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">is_causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm_p</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">is_causal</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">depthwise_conv_tpl</span><span class="o">.</span><span class="n">cls</span> <span class="o">==</span>
        <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">DepthwiseConv2DLayer</span><span class="p">):</span>
      <span class="c1"># If causal, switch to causal depthwise conv.</span>
      <span class="n">depthwise_conv_p</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">CausalDepthwiseConv2DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>
      <span class="n">hparams_lib</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">depthwise_conv_tpl</span><span class="p">,</span> <span class="n">depthwise_conv_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">depthwise_conv_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">depthwise_conv_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="c1"># 1d depthwise conv with channel_mulitplier = 1</span>
    <span class="n">depthwise_conv_p</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;depthwise_conv&#39;</span><span class="p">,</span>
        <span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">filter_stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;depthwise_conv1d&#39;</span><span class="p">,</span> <span class="n">depthwise_conv_p</span><span class="p">)</span>

    <span class="n">dropout_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">)</span>

<div class="viewcode-block" id="LConvLayer._GLU"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer._GLU">[docs]</a>  <span class="k">def</span> <span class="nf">_GLU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyActivation</span><span class="p">(</span><span class="n">act_inputs</span><span class="p">,</span>
                                 <span class="n">p</span><span class="o">.</span><span class="n">glu_activation</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LConvLayer._ApplyActivation"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer._ApplyActivation">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyActivation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">act_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">act_name</span> <span class="o">==</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span>
    <span class="k">return</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">act_name</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LConvLayer._Normalize"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer._Normalize">[docs]</a>  <span class="k">def</span> <span class="nf">_Normalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of layer params.</span>
<span class="sd">      inputs: [b, t, 1, d].</span>
<span class="sd">      paddings: [b, t].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [b, t, d].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">GroupNormLayer</span><span class="p">):</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">input_rank</span> <span class="o">==</span> <span class="mi">4</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="c1"># [b, t, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># [b, t, 1, d] -&gt; [b, t, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">BatchNormLayer</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s1">&#39;Only bn_layers.{BatchNormLayer,GroupNormLayer}, layers.LayerNorm &#39;</span>
            <span class="s1">&#39;are supported.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LConvLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Builds FProp graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of Tensors, see base class.</span>
<span class="sd">      inputs: A Tensor of shape [batch, seqlen, dim0].</span>
<span class="sd">      paddings: A Tensor of shape [batch, seqlen].</span>

<span class="sd">    Returns:</span>
<span class="sd">      output: A Tensor of shape [batch, seqlen, dim0].</span>
<span class="sd">      out_paddings: A Tensor of shape [batch, seqlen].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>
      <span class="n">unnormalized_inputs</span> <span class="o">=</span> <span class="n">inputs</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ln</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">split_act_gated_linear_start</span><span class="p">:</span>
        <span class="n">act_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start_act</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start_act</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">gated_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start_gated</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start_gated</span><span class="p">,</span>
                                                     <span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GLU</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span><span class="p">)</span>

      <span class="c1"># TODO(jamesqin): inroduce depthwise conv2d with 3d inputs.</span>
      <span class="c1"># [b, t, d] --&gt; [b, t, 1, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">adapted_blf_dims_mapping</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">adapted_blf_dims_mapping</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">adapted_blf_dims_mapping</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                      <span class="n">adapted_blf_dims_mapping</span><span class="p">)</span>
      <span class="n">theta</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="o">.</span><span class="n">hwim</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
        <span class="c1"># Depthwise conv doesn&#39;t support bfloat32 on CPU.</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="p">,</span>
                                                     <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                      <span class="n">adapted_blf_dims_mapping</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Normalize</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                      <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blf</span><span class="p">)</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyActivation</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_activation</span><span class="p">)</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_end</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_end</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

      <span class="n">output</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">unnormalized_inputs</span>
      <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="LConvLayer.zero_state"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;zero_state&#39;</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_causal</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;depthwise_conv1d&#39;</span><span class="p">):</span>
          <span class="n">res</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">conv_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="s1">&#39;zero_state&#39;</span><span class="p">):</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">):</span>
            <span class="n">res</span><span class="o">.</span><span class="n">norm_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If not causal, depthwise_conv1d does not have zero_state().</span>
        <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span></div>

<div class="viewcode-block" id="LConvLayer._NormalizeStep"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer._NormalizeStep">[docs]</a>  <span class="k">def</span> <span class="nf">_NormalizeStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">state1</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="s1">&#39;StreamStep&#39;</span><span class="p">):</span>
      <span class="c1"># TODO(jamesqin): support 3d inputs.</span>
      <span class="c1"># At present it&#39;s guaranteed GroupNorm.</span>
      <span class="k">assert</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">GroupNormLayer</span><span class="p">)</span> <span class="ow">and</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">input_rank</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">norm_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">norm_state</span><span class="p">)</span>
      <span class="c1"># [b, t, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">norm_state</span> <span class="o">=</span> <span class="n">norm_state1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># [b, t, 1, d] -&gt; [b, t, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s1">&#39;Only bn_layers.GroupNormLayer, layers.LayerNorm are supported.&#39;</span><span class="p">)</span>
    <span class="c1"># [b, t, d]</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="LConvLayer.StreamStep"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.LConvLayer.StreamStep">[docs]</a>  <span class="k">def</span> <span class="nf">StreamStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Streams t steps.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of layer params.</span>
<span class="sd">      inputs: [b, t, d].</span>
<span class="sd">      paddings: A 0/1 valued tensor of shape [b, t].</span>
<span class="sd">      state0: A NestedMap of tensors of the same struct as returned by</span>
<span class="sd">        zero_state().</span>

<span class="sd">    Returns:</span>
<span class="sd">      outputs: A NestedMap of tensors consisting:</span>
<span class="sd">      padding: the same as input paddings.</span>
<span class="sd">      state1: A NestedMap of tensors of the same struct as state0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">is_causal</span>

    <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">/StreamStep&#39;</span><span class="p">):</span>
      <span class="n">unnormalized_inputs</span> <span class="o">=</span> <span class="n">inputs</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ln</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">split_act_gated_linear_start</span><span class="p">:</span>
        <span class="n">act_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start_act</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start_act</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">gated_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start_gated</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start_gated</span><span class="p">,</span>
                                                     <span class="n">inputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_start</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_start</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GLU</span><span class="p">(</span><span class="n">gated_inputs</span><span class="p">,</span> <span class="n">act_inputs</span><span class="p">)</span>

      <span class="c1"># TODO(jamesqin): inroduce depthwise conv2d with 3d inputs.</span>
      <span class="c1"># TODO(jamesqin): optimize DepthwiseConv1D.StreamStep()</span>
      <span class="c1"># [b, t, d] --&gt; [b, t, 1, d]</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="c1"># [b, t, 1, d]</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">conv_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">depthwise_conv1d</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">conv_state</span><span class="p">)</span>
      <span class="n">state1</span><span class="o">.</span><span class="n">conv_state</span> <span class="o">=</span> <span class="n">conv_state1</span>
      <span class="c1"># [b, t, d]</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_NormalizeStep</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span>
                                             <span class="n">state1</span><span class="p">)</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyActivation</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_activation</span><span class="p">)</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_end</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear_end</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

      <span class="n">output</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">unnormalized_inputs</span>
      <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state1</span></div></div>


<div class="viewcode-block" id="_AttenCtxIsSet"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer._AttenCtxIsSet">[docs]</a><span class="k">def</span> <span class="nf">_AttenCtxIsSet</span><span class="p">(</span><span class="n">atten_context</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">atten_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">atten_context</span> <span class="o">&gt;=</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="_GShardMoELayerParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer._GShardMoELayerParams">[docs]</a><span class="k">def</span> <span class="nf">_GShardMoELayerParams</span><span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span>
                          <span class="n">per_expert_capacity_dim</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">gshard_builder</span><span class="o">.</span><span class="n">MoEBuilder</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
      <span class="n">num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
      <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
      <span class="n">e_dim</span><span class="o">=</span><span class="n">num_experts</span><span class="p">,</span>
      <span class="n">c_dim</span><span class="o">=</span><span class="n">per_expert_capacity_dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConformerLayer"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer">[docs]</a><span class="k">class</span> <span class="nc">ConformerLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Conformer layer as in https://arxiv.org/abs/2005.08100.</span>

<span class="sd">    Canonical version (with default params.)</span>
<span class="sd">      x = x + 1/2 * FFN(x)</span>
<span class="sd">      x = x + MHSA(x)</span>
<span class="sd">      x = x + Lconv(x)</span>
<span class="sd">      x = x + 1/2 * FFN(x)</span>
<span class="sd">      y = ln(x)</span>

<span class="sd">    Optionally one can change the order of MHSA and conv.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ConformerLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Input dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;is_causal&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If use causal lconv and MHSA layer.&#39;</span>
        <span class="s1">&#39;Notice atten_right_context must be not be infinite(None) if is_causal &#39;</span>
        <span class="s1">&#39;is True. It is important to always set is_causal for streaming case, &#39;</span>
        <span class="s1">&#39;and not expect to infer from atten_{left,right}_context.&#39;</span><span class="p">)</span>
    <span class="c1"># atten layer</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_num_heads&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Num of heads in multi-head self-attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_left_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Local self attention left context.&#39;</span>
        <span class="s1">&#39;If None or &lt; 0, infinite left context. Notice unlike &#39;</span>
        <span class="s1">&#39;atten_right_context, this includes `self` position.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;atten_right_context&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Local self attention right context.&#39;</span>
        <span class="s1">&#39;If None or &lt; 0, infinite right context.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_relative_atten&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;If using relative attention.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;relative_pos_emb_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If use_relative_atten, sets the relative pos embedding dim.&#39;</span>
        <span class="s1">&#39;Default is the same as input_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;layer_order&#39;</span><span class="p">,</span> <span class="s1">&#39;mhsa_before_conv&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Only mhsa, conv, mhsa_before_conv or conv_before_mhsa are &#39;</span>
        <span class="s1">&#39;supported.&#39;</span><span class="p">)</span>

    <span class="c1"># lconv layer</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Kernel size of 1d lightweight conv.&#39;</span><span class="p">)</span>

    <span class="c1"># fflayer</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fflayer_hidden_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Hidden dim of the fflayers (start and end).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fflayer_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;SWISH&#39;</span><span class="p">,</span> <span class="s1">&#39;fflayer activation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;fflayer_residual_weight&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;fflayer residual weight.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_prob&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Signature dropout prob of inner componets.&#39;</span><span class="p">)</span>

    <span class="c1"># tpl</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fflayer_start_tpl&#39;</span><span class="p">,</span>
        <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Layer params for Feed forward layer at the beginning. Supports &#39;</span>
        <span class="s1">&#39;using gshard_builder.MoEBuilder.Params() as well wherein the &#39;</span>
        <span class="s1">&#39;MoE() will be used. If set to None, this layer is excluded.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;trans_atten_tpl&#39;</span><span class="p">,</span>
             <span class="n">attention_lib</span><span class="o">.</span><span class="n">TransformerAttentionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Self attention layer params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;lconv_tpl&#39;</span><span class="p">,</span> <span class="n">LConvLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Convolution module params. If set to None, this layer is excluded.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fflayer_end_tpl&#39;</span><span class="p">,</span>
        <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Layer params for Feed forward layer at the end. Supports using &#39;</span>
        <span class="s1">&#39;gshard_builder.MoEBuilder.Params() as well wherein the MoE() &#39;</span>
        <span class="s1">&#39;will be used.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;final_ln_tpl&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Final layer norm.&#39;</span><span class="p">)</span>
    <span class="c1"># https://b/167460492#comment16</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;remat&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If to rematerialize the layer. If true, &#39;</span>
        <span class="s1">&#39;intermediate tensors are not saved in FProp().&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="ConformerLayer.CommonParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.CommonParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">CommonParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                   <span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">atten_num_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">atten_local_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">atten_left_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">atten_right_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">use_relative_atten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fflayer_hidden_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fflayer_activation</span><span class="o">=</span><span class="s1">&#39;SWISH&#39;</span><span class="p">,</span>
                   <span class="n">fflayer_residual_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">layer_order</span><span class="o">=</span><span class="s1">&#39;mhsa_before_conv&#39;</span><span class="p">,</span>
                   <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                   <span class="n">conv_norm_layer_tpl</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fprop_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">use_moe_in_fflayer_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">use_moe_in_fflayer_end</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">moe_num_partitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">moe_num_experts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">moe_num_groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">moe_per_capacity_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fflayer_start_tpl</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">fflayer_end_tpl</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trans_atten_tpl</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">lconv_tpl</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">fflayer_hidden_dim</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">layer_order</span> <span class="o">!=</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">atten_num_heads</span>
    <span class="k">if</span> <span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa&#39;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">conv_norm_layer_tpl</span><span class="p">,</span> <span class="n">lconv_tpl</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">kernel_size</span>

    <span class="k">if</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span><span class="n">atten_local_context</span><span class="p">):</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span><span class="n">atten_left_context</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span>
          <span class="n">atten_right_context</span>
      <span class="p">),</span> <span class="p">(</span><span class="s1">&#39;atten_local_context and atten_{left,right}_context can not be set&#39;</span>
          <span class="s1">&#39;at the same time.&#39;</span><span class="p">)</span>
      <span class="n">atten_left_context</span> <span class="o">=</span> <span class="n">atten_local_context</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># including self position.</span>
      <span class="n">atten_right_context</span> <span class="o">=</span> <span class="n">atten_local_context</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">atten_num_heads</span><span class="o">=</span><span class="n">atten_num_heads</span><span class="p">,</span>
        <span class="n">atten_left_context</span><span class="o">=</span><span class="n">atten_left_context</span><span class="p">,</span>
        <span class="n">atten_right_context</span><span class="o">=</span><span class="n">atten_right_context</span><span class="p">,</span>
        <span class="n">use_relative_atten</span><span class="o">=</span><span class="n">use_relative_atten</span><span class="p">,</span>
        <span class="n">fflayer_hidden_dim</span><span class="o">=</span><span class="n">fflayer_hidden_dim</span><span class="p">,</span>
        <span class="n">fflayer_activation</span><span class="o">=</span><span class="n">fflayer_activation</span><span class="p">,</span>
        <span class="n">fflayer_residual_weight</span><span class="o">=</span><span class="n">fflayer_residual_weight</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">,</span>
        <span class="n">layer_order</span><span class="o">=</span><span class="n">layer_order</span><span class="p">,</span>
        <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>
    <span class="c1"># Set the two feed forward modules.</span>
    <span class="k">if</span> <span class="n">fflayer_end_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">fflayer_end_tpl</span> <span class="o">=</span> <span class="n">fflayer_end_tpl</span>
    <span class="k">if</span> <span class="n">fflayer_start_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">fflayer_start_tpl</span> <span class="o">=</span> <span class="n">fflayer_start_tpl</span>
    <span class="c1"># Set the MHSA module.</span>
    <span class="k">if</span> <span class="n">trans_atten_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span> <span class="o">=</span> <span class="n">trans_atten_tpl</span>
    <span class="c1"># Set the convolution module.</span>
    <span class="k">if</span> <span class="n">lconv_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">lconv_tpl</span> <span class="o">=</span> <span class="n">lconv_tpl</span>
    <span class="k">if</span> <span class="n">conv_norm_layer_tpl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="o">.</span><span class="n">conv_norm_layer_tpl</span> <span class="o">=</span> <span class="n">conv_norm_layer_tpl</span>
    <span class="k">if</span> <span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">SetFPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_moe_in_fflayer_start</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">SetMoEFFLayerStartParams</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">moe_num_partitions</span><span class="p">,</span> <span class="n">moe_num_experts</span><span class="p">,</span>
                                     <span class="n">moe_num_groups</span><span class="p">,</span> <span class="n">moe_per_capacity_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_moe_in_fflayer_end</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">SetMoEFFLayerEndParams</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">moe_num_partitions</span><span class="p">,</span> <span class="n">moe_num_experts</span><span class="p">,</span>
                                   <span class="n">moe_num_groups</span><span class="p">,</span> <span class="n">moe_per_capacity_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="ConformerLayer.SetFPropDtype"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.SetFPropDtype">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetFPropDtype</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span>
    <span class="k">for</span> <span class="n">sub_p</span> <span class="ow">in</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">fflayer_start_tpl</span><span class="p">,</span>
                  <span class="n">p</span><span class="o">.</span><span class="n">fflayer_end_tpl</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">sub_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sub_p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">SetFPropDtype</span><span class="p">(</span><span class="n">sub_p</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="ConformerLayer.SetMoEFFLayerStartParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.SetMoEFFLayerStartParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetMoEFFLayerStartParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span>
                               <span class="n">num_groups</span><span class="p">,</span> <span class="n">per_expert_capacity_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates params setting MoE as feed-forward layer.&quot;&quot;&quot;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">fflayer_start_tpl</span> <span class="o">=</span> <span class="n">_GShardMoELayerParams</span><span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span>
                                                     <span class="n">num_experts</span><span class="p">,</span>
                                                     <span class="n">per_expert_capacity_dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConformerLayer.SetMoEFFLayerEndParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.SetMoEFFLayerEndParams">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetMoEFFLayerEndParams</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span>
                             <span class="n">per_expert_capacity_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates params setting MoE as feed-forward layer.&quot;&quot;&quot;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">fflayer_end_tpl</span> <span class="o">=</span> <span class="n">_GShardMoELayerParams</span><span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span>
                                                   <span class="n">num_experts</span><span class="p">,</span>
                                                   <span class="n">per_expert_capacity_dim</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s1">&#39;mhsa&#39;</span><span class="p">,</span> <span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;mhsa_before_conv&#39;</span><span class="p">,</span> <span class="s1">&#39;conv_before_mhsa&#39;</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_causal</span><span class="p">:</span>
      <span class="c1"># None is different from 0, the former is &#39;infinite&#39;.</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_right_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;is_causal is not compatible with infinite atten_right_context &#39;</span>
          <span class="s1">&#39;(None).&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa&#39;</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_lconv</span><span class="p">,</span> <span class="s1">&#39;mhsa must not have a lconv block.&#39;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_fflayer_start</span><span class="p">:</span>
      <span class="n">fflayer_start_p</span><span class="p">,</span> <span class="n">is_moe_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ConfigFFLayerOrMoEParams</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">fflayer_start_tpl</span><span class="p">,</span> <span class="s1">&#39;fflayer_start&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">is_moe_layer</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer_start_moe&#39;</span><span class="p">,</span> <span class="n">fflayer_start_p</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer_start&#39;</span><span class="p">,</span> <span class="n">fflayer_start_p</span><span class="p">)</span>

    <span class="n">fflayer_end_p</span><span class="p">,</span> <span class="n">is_moe_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ConfigFFLayerOrMoEParams</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">fflayer_end_tpl</span><span class="p">,</span> <span class="s1">&#39;fflayer_end&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_moe_layer</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer_end_moe&#39;</span><span class="p">,</span> <span class="n">fflayer_end_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fflayer_end&#39;</span><span class="p">,</span> <span class="n">fflayer_end_p</span><span class="p">)</span>

    <span class="c1"># For local MHSA, is_masked is ignored, thus it&#39;s safe to set is_masked</span>
    <span class="c1"># based on p.is_causal, for global and local MHSA cases.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_mhsa</span><span class="p">:</span>
      <span class="n">trans_atten_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">num_heads</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_num_heads</span><span class="p">,</span>
          <span class="n">is_masked</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_causal</span><span class="p">,</span>
          <span class="n">atten_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">,</span>
          <span class="n">residual_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ConfigSelfAttenParams</span><span class="p">(</span><span class="n">trans_atten_p</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;trans_atten&#39;</span><span class="p">,</span> <span class="n">trans_atten_p</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_lconv</span><span class="p">:</span>
      <span class="n">lconv_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">kernel_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
          <span class="n">is_causal</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_causal</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;lconv&#39;</span><span class="p">,</span> <span class="n">lconv_p</span><span class="p">)</span>

    <span class="n">ln_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">final_ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;final_ln&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;final_ln&#39;</span><span class="p">,</span> <span class="n">ln_p</span><span class="p">)</span>

  <span class="c1"># lconv and fflayer_start have the special treatment, which can be absent,</span>
  <span class="c1"># because Transformer doesn&#39;t have those.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_lconv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_mhsa</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="s1">&#39;mhsa&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_order</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_fflayer_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">fflayer_start_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="ConformerLayer._ConfigSelfAttenParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._ConfigSelfAttenParams">[docs]</a>  <span class="k">def</span> <span class="nf">_ConfigSelfAttenParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trans_atten_p</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_pos_emb_dim</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">relative_pos_emb_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="c1"># TODO(jamesqin): add an attention factory in batch_major_attention.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">atten_left_context</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">atten_right_context</span><span class="p">):</span>
      <span class="c1"># No atten context set, each position attends to all positions.</span>
      <span class="n">atten_type</span> <span class="o">=</span> <span class="s1">&#39;global&#39;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_relative_atten</span> <span class="k">else</span> <span class="s1">&#39;global_relative&#39;</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">_AttenCtxIsSet</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">atten_left_context</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_right_context</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># Left context is infinite, right context is 0.</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_relative_atten</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;Relative attention isn</span><span class="se">\&#39;</span><span class="s1">t supported for causal attention.&#39;</span><span class="p">)</span>
      <span class="n">atten_type</span> <span class="o">=</span> <span class="s1">&#39;global_causal&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">atten_type</span> <span class="o">=</span> <span class="s1">&#39;local_relative&#39;</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_relative_atten</span> <span class="k">else</span> <span class="s1">&#39;local&#39;</span>

    <span class="k">if</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="s1">&#39;global_relative&#39;</span><span class="p">:</span>
      <span class="n">atten_tpl</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">attention_lib</span><span class="o">.</span><span class="n">MultiHeadedAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
              <span class="n">rel_pos_emb_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">relative_pos_emb_dim</span><span class="p">))</span>
      <span class="n">hparams_lib</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="n">atten_tpl</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="s1">&#39;local_relative&#39;</span><span class="p">:</span>
      <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">attention_lib</span><span class="o">.</span><span class="n">LocalSelfAttentionXL</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_left_context</span><span class="p">,</span>
          <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_right_context</span><span class="p">,</span>
          <span class="n">rel_pos_emb_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">relative_pos_emb_dim</span><span class="p">)</span>
      <span class="n">hparams_lib</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span>
          <span class="n">atten_tpl</span><span class="p">,</span>
          <span class="n">skip</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;left_context&#39;</span><span class="p">,</span> <span class="s1">&#39;right_context&#39;</span><span class="p">,</span> <span class="s1">&#39;rel_pos_emb_dim&#39;</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">atten_type</span> <span class="o">==</span> <span class="s1">&#39;local&#39;</span><span class="p">:</span>
      <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">attention_lib</span><span class="o">.</span><span class="n">LocalSelfAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">left_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_left_context</span><span class="p">,</span>
          <span class="n">right_context</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">atten_right_context</span><span class="p">)</span>
      <span class="n">hparams_lib</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span>
          <span class="n">atten_tpl</span><span class="p">,</span>
          <span class="n">skip</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;left_context&#39;</span><span class="p">,</span> <span class="s1">&#39;right_context&#39;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># No op for &#39;global&#39; atten</span>
      <span class="k">assert</span> <span class="n">atten_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;global&#39;</span><span class="p">,</span> <span class="s1">&#39;global_causal&#39;</span><span class="p">),</span> <span class="p">(</span>
          <span class="sa">f</span><span class="s1">&#39;Unknown atten_type </span><span class="si">{</span><span class="n">atten_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">attention_lib</span><span class="o">.</span><span class="n">MultiHeadedAttention</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">hparams_lib</span><span class="o">.</span><span class="n">CopyFieldsTo</span><span class="p">(</span><span class="n">trans_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span><span class="p">,</span> <span class="n">atten_tpl</span><span class="p">)</span>
    <span class="n">trans_atten_p</span><span class="o">.</span><span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">atten_tpl</span></div>

<div class="viewcode-block" id="ConformerLayer._ConfigFFLayerOrMoEParams"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._ConfigFFLayerOrMoEParams">[docs]</a>  <span class="k">def</span> <span class="nf">_ConfigFFLayerOrMoEParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fflayer_tpl</span><span class="p">,</span> <span class="n">name_prefix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Configures fflayer_tpl params to create Feed-forward layer or MoE params.</span>

<span class="sd">    Args:</span>
<span class="sd">      fflayer_tpl: Input Feedforward/MoE params to be initialized.</span>
<span class="sd">      name_prefix: Layer name prefix to be added in case of creating MoE layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      fflayer_p: Configured Feedforward/MoE layer params to be initialized.</span>
<span class="sd">      is_moe_layer_p: Whether returned `fflayer_p` params of form subclass:</span>
<span class="sd">      gshard_builder.MoEBuilder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span>
                   <span class="n">layers_with_attention</span><span class="o">.</span><span class="n">TransformerFeedForwardLayer</span><span class="p">)):</span>
      <span class="n">fflayer_p</span> <span class="o">=</span> <span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_hidden_dim</span><span class="p">,</span>
          <span class="n">activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_activation</span><span class="p">,</span>
          <span class="n">residual_weight</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_residual_weight</span><span class="p">,</span>
          <span class="n">residual_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">,</span>
          <span class="n">relu_dropout_prob</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">fflayer_p</span><span class="p">,</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">gshard_builder</span><span class="o">.</span><span class="n">MoEBuilder</span><span class="p">):</span>
      <span class="n">moe_builder_p</span> <span class="o">=</span> <span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">model_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dropout_prob</span><span class="p">,</span>
          <span class="n">moe_hidden_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_hidden_dim</span><span class="p">,</span>
          <span class="n">moe_activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_activation</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">moe_builder_p</span><span class="o">.</span><span class="n">num_devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_devices must be specified for MoEBuilder.&#39;</span><span class="p">)</span>
      <span class="n">is_moe_layer</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">name_prefix</span> <span class="o">+</span> <span class="s1">&#39;_moe&#39;</span>
      <span class="n">moe_p</span> <span class="o">=</span> <span class="n">moe_builder_p</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span><span class="o">.</span><span class="n">MoE</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="n">moe_builder</span> <span class="o">=</span> <span class="n">moe_builder_p</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
      <span class="n">moe_p</span> <span class="o">=</span> <span class="n">moe_builder</span><span class="o">.</span><span class="n">EncoderLayer</span><span class="p">(</span>
          <span class="n">name</span><span class="p">,</span>
          <span class="n">moe_builder</span><span class="o">.</span><span class="n">MoE</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
          <span class="n">residual_weight</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">fflayer_residual_weight</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">moe_p</span><span class="p">,</span> <span class="n">is_moe_layer</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;p.fflayer_tpl must be either &#39;</span>
                       <span class="s1">&#39;TransformerFeedForwardLayer or MoEBuilder.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConformerLayer._SelfAtten"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._SelfAtten">[docs]</a>  <span class="k">def</span> <span class="nf">_SelfAtten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans_atten</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">trans_atten</span><span class="p">,</span>
        <span class="n">query_vec</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">source_vecs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="ConformerLayer._LConv"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._LConv">[docs]</a>  <span class="k">def</span> <span class="nf">_LConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_lconv</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">!=</span> <span class="s1">&#39;mhsa&#39;</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;mhsa does not have a lconv block.&#39;</span><span class="p">)</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lconv</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">lconv</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span></div>

<div class="viewcode-block" id="ConformerLayer._MoeOrFFLayer"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._MoeOrFFLayer">[docs]</a>  <span class="k">def</span> <span class="nf">_MoeOrFFLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">fflayer_name</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;FProp for MoE or Feed forward layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Layer theta: A NestedMap of Tensors.</span>
<span class="sd">      fflayer_name: Child FFLayer name as created in __init__.</span>
<span class="sd">        For example: &#39;fflayer_end&#39;. This assumes the moe_layer if created would</span>
<span class="sd">        have the convention as (`fflayer_name` + `_moe`).</span>
<span class="sd">      in_nmap: Nested Map containing the following:</span>

<span class="sd">        * inputs: A Tensor of shape [batch, seqlen, dim0].</span>
<span class="sd">        * paddings: A Tensor of shape [batch, seqlen].</span>
<span class="sd">        * moe_aux_loss: [None] Optional aux loss if present in input batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">     out_nmap: A NestedMap of output tensors:</span>

<span class="sd">       * features: Tensor of shape [batch, seqlen, dim0].</span>
<span class="sd">       * paddings: A Tensor of shape [batch, seqlen].</span>
<span class="sd">       * aux_loss: [Optional] Scalar tensor. Output moe auxiliary loss with</span>
<span class="sd">         input aux loss added.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out_nmap</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">fflayer_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">fflayer_name</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">fflayer_name</span><span class="p">),</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">paddings</span><span class="p">)</span>
      <span class="n">out_nmap</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span>
      <span class="k">return</span> <span class="n">out_nmap</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">moe_fflayer_name</span> <span class="o">=</span> <span class="n">fflayer_name</span> <span class="o">+</span> <span class="s1">&#39;_moe&#39;</span>
      <span class="k">if</span> <span class="n">moe_fflayer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> child layer not present.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">moe_fflayer_name</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">moe_fflayer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> layer theta not present.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">moe_fflayer_name</span><span class="p">))</span>
      <span class="c1"># 0 - padded positions and 1 - non-padded positions.</span>
      <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="n">segment_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>  <span class="c1"># not used but required by MoE.</span>
      <span class="n">moe_in</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">vec</span><span class="o">=</span><span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">segment_id</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">segment_pos</span><span class="o">=</span><span class="n">segment_pos</span><span class="p">)</span>
      <span class="n">moe_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">moe_fflayer_name</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">moe_fflayer_name</span><span class="p">),</span> <span class="n">moe_in</span><span class="p">)</span>
      <span class="n">out_nmap</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">moe_out</span><span class="o">.</span><span class="n">vec</span>
      <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">moe_out</span><span class="o">.</span><span class="n">aux_loss</span>
      <span class="k">if</span> <span class="s1">&#39;aux_loss&#39;</span> <span class="ow">in</span> <span class="n">in_nmap</span><span class="p">:</span>
        <span class="n">aux_loss</span> <span class="o">+=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">aux_loss</span>
      <span class="c1"># Add &#39;aux_loss&#39; in out_nmap.</span>
      <span class="n">out_nmap</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">aux_loss</span>
      <span class="k">return</span> <span class="n">out_nmap</span></div>

<div class="viewcode-block" id="ConformerLayer._FProp"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer._FProp">[docs]</a>  <span class="k">def</span> <span class="nf">_FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">paddings</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>
      <span class="n">out_nmap</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_fflayer_start</span><span class="p">:</span>
        <span class="n">in_nmap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MoeOrFFLayer</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;fflayer_start&#39;</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SelfAtten</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa_before_conv&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SelfAtten</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;conv_before_mhsa&#39;</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SelfAtten</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">inputs</span>
      <span class="n">in_nmap</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>
      <span class="n">in_nmap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MoeOrFFLayer</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;fflayer_end&#39;</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span>
      <span class="k">if</span> <span class="s1">&#39;aux_loss&#39;</span> <span class="ow">in</span> <span class="n">in_nmap</span><span class="p">:</span>
        <span class="n">out_nmap</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">aux_loss</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">final_ln</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>
      <span class="n">out_nmap</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">inputs</span>
      <span class="n">out_nmap</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>
      <span class="k">return</span> <span class="n">out_nmap</span></div>

<div class="viewcode-block" id="ConformerLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">remat</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">CellFn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">unused_inputs</span><span class="p">):</span>
      <span class="n">out_nmap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">out_nmap</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">state1</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">in_nmap</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])),</span>  <span class="c1"># A dummy input of shape [T, ?].</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">CellFn</span><span class="p">,</span>
        <span class="n">allow_implicit_capture</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">allow_implicit_capture</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">state1</span></div>

<div class="viewcode-block" id="ConformerLayer.zero_state"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.zero_state">[docs]</a>  <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">is_causal</span><span class="p">:</span>
      <span class="n">lconv_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="n">atten_state</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_lconv</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;lconv&#39;</span><span class="p">):</span>
          <span class="n">lconv_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lconv</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_mhsa</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;atten&#39;</span><span class="p">):</span>
          <span class="n">atten_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans_atten</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">lconv_state</span><span class="o">=</span><span class="n">lconv_state</span><span class="p">,</span> <span class="n">atten_state</span><span class="o">=</span><span class="n">atten_state</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span></div>

<div class="viewcode-block" id="ConformerLayer.StreamStep"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ConformerLayer.StreamStep">[docs]</a>  <span class="k">def</span> <span class="nf">StreamStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Streams t steps.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of read-only layer params.</span>
<span class="sd">      inputs: A tensor of shape [b, t, d].</span>
<span class="sd">      paddings: A 0/1 valued tensor of shape [b, t].</span>
<span class="sd">      state0: A NestedMap of tensors of the same struct as returned by</span>
<span class="sd">        zero_state().</span>

<span class="sd">    Returns:</span>
<span class="sd">      outputs:A tensor of shape [b, t, d].</span>
<span class="sd">      padding: the same as input paddings.</span>
<span class="sd">      state1: A NestedMap of tensors of the same struct as state0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">is_causal</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">remat</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">/StreamStep&#39;</span><span class="p">):</span>
      <span class="n">in_nmap</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_fflayer_start</span><span class="p">:</span>
        <span class="n">in_nmap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MoeOrFFLayer</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;fflayer_start&#39;</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">atten_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans_atten</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">trans_atten</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten_state</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">lconv_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lconv</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">lconv</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">lconv_state</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;mhsa_before_conv&#39;</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">atten_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans_atten</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">trans_atten</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten_state</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">lconv_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lconv</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">lconv</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">lconv_state</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">==</span> <span class="s1">&#39;conv_before_mhsa&#39;</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">lconv_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lconv</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">lconv</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">lconv_state</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">atten_state1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans_atten</span><span class="o">.</span><span class="n">StreamStep</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">.</span><span class="n">trans_atten</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state0</span><span class="o">.</span><span class="n">atten_state</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_lconv</span><span class="p">:</span>
        <span class="n">lconv_state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_mhsa</span><span class="p">:</span>
        <span class="n">atten_state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

      <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">inputs</span>
      <span class="n">in_nmap</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>
      <span class="n">in_nmap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MoeOrFFLayer</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;fflayer_end&#39;</span><span class="p">,</span> <span class="n">in_nmap</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">in_nmap</span><span class="o">.</span><span class="n">features</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_ln</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">final_ln</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

      <span class="n">state1</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">lconv_state</span><span class="o">=</span><span class="n">lconv_state1</span><span class="p">,</span> <span class="n">atten_state</span><span class="o">=</span><span class="n">atten_state1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">state1</span></div></div>


<div class="viewcode-block" id="ApplyGshard"><a class="viewcode-back" href="../../../lingvo.core.conformer_layer.html#lingvo.core.conformer_layer.ApplyGshard">[docs]</a><span class="k">def</span> <span class="nf">ApplyGshard</span><span class="p">(</span><span class="n">conformer_tpl</span><span class="p">,</span>
                <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">proj_w_split_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">proj_activation_split_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">atten_dnh_w_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">atten_blnh_activation_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">atten_bld_activation_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lconv_df_w_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lconv_hwim_w_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lconv_fd_w_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lconv_blf_activation_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lconv_bld_activation_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies gshard on conformer params.</span>

<span class="sd">  Args:</span>
<span class="sd">    conformer_tpl: A NestedMap of conformer Params.</span>
<span class="sd">    device_mesh: A numpy.ndarray specifying the device mesh on which the</span>
<span class="sd">      computation is sharded.</span>
<span class="sd">    proj_w_split_list: A list of mesh split specifying how weights are sharded</span>
<span class="sd">      for fflayer.</span>
<span class="sd">    proj_activation_split_list: A list of mesh split specifying how activations</span>
<span class="sd">      are sharded for fflayer.</span>
<span class="sd">    atten_dnh_w_split: Mesh split of the attention projection weight with the</span>
<span class="sd">      shape of [model_dim, num_heads, dim_per_head].</span>
<span class="sd">    atten_blnh_activation_split: Mesh split of the attention activation with</span>
<span class="sd">      shape of [batch, seq_len, num_heads, dim_per_head].</span>
<span class="sd">    atten_bld_activation_split: Mesh split of the attention activation with</span>
<span class="sd">      shape of [batch, seq_len, model_dim].</span>
<span class="sd">    lconv_df_w_split: Mesh split of the weights in lconv with the shape of</span>
<span class="sd">      [model_dim, ff_hidden_dim].</span>
<span class="sd">    lconv_hwim_w_split: Mesh split of the depthwise conv weight in lconv with</span>
<span class="sd">      the shape of [height, width, in_channels, channel_multiplier].</span>
<span class="sd">    lconv_fd_w_split: Mesh split of the weights in lconv with the shape of</span>
<span class="sd">      [ff_hidden_dim, model_dim].</span>
<span class="sd">    lconv_blf_activation_split: Mesh split of the activations in lconv with the</span>
<span class="sd">      shape of [batch, seq_len, ff_hidden_dim].</span>
<span class="sd">    lconv_bld_activation_split: Mesh split of the activations in lconv with the</span>
<span class="sd">      shape of [batch, seq_len, model_dim].</span>

<span class="sd">  Returns:</span>
<span class="sd">    The updated conformer_tpl.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Not all attention class supports gshard. If not, errors would be throw here.</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">device_mesh</span> <span class="o">=</span> <span class="n">device_mesh</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">atten_dnh_w_split</span><span class="p">)</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">proj_tpl</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">atten_dnh_w_split</span><span class="p">)</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">blnh</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">atten_blnh_activation_split</span><span class="p">)</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">trans_atten_tpl</span><span class="o">.</span><span class="n">atten_tpl</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="o">.</span><span class="n">bld</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">atten_bld_activation_split</span><span class="p">)</span>
  <span class="c1"># TODO(jamesqin): support residual_proj xla sharding too.</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">fflayer_start_tpl</span><span class="o">.</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
      <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
      <span class="n">weight_split_dims_mapping_list</span><span class="o">=</span><span class="n">proj_w_split_list</span><span class="p">,</span>
      <span class="n">activation_split_dims_mapping_list</span><span class="o">=</span><span class="n">proj_activation_split_list</span><span class="p">)</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">fflayer_end_tpl</span><span class="o">.</span><span class="n">fflayer_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
      <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">,</span>
      <span class="n">weight_split_dims_mapping_list</span><span class="o">=</span><span class="n">proj_w_split_list</span><span class="p">,</span>
      <span class="n">activation_split_dims_mapping_list</span><span class="o">=</span><span class="n">proj_activation_split_list</span><span class="p">)</span>
  <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
      <span class="n">split_act_gated_linear_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span>
  <span class="n">lconv_w_split</span> <span class="o">=</span> <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span>
  <span class="n">lconv_w_split</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">lconv_df_w_split</span>
  <span class="n">lconv_w_split</span><span class="o">.</span><span class="n">hwim</span> <span class="o">=</span> <span class="n">lconv_hwim_w_split</span>
  <span class="n">lconv_w_split</span><span class="o">.</span><span class="n">fd</span> <span class="o">=</span> <span class="n">lconv_fd_w_split</span>
  <span class="n">lconv_activation_split</span> <span class="o">=</span> <span class="n">conformer_tpl</span><span class="o">.</span><span class="n">lconv_tpl</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span>
  <span class="n">lconv_activation_split</span><span class="o">.</span><span class="n">blf</span> <span class="o">=</span> <span class="n">lconv_blf_activation_split</span>
  <span class="n">lconv_activation_split</span><span class="o">.</span><span class="n">bld</span> <span class="o">=</span> <span class="n">lconv_bld_activation_split</span>
  <span class="k">return</span> <span class="n">conformer_tpl</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>