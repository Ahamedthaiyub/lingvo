<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lingvo.core.gshard_builder &mdash; Lingvo  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>lingvo.core.gshard_builder</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for lingvo.core.gshard_builder</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2020 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;GShard Builder. To be used with xla_sharding + SPMD.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="kn">from</span> <span class="nn">lingvo</span> <span class="kn">import</span> <span class="n">compat</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_model</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">builder</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">builder_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">flat_beam_search_helper</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>


<div class="viewcode-block" id="_ToInt32"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder._ToInt32">[docs]</a><span class="k">def</span> <span class="nf">_ToInt32</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span></div>


<div class="viewcode-block" id="MoEBuilder"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder">[docs]</a><span class="k">class</span> <span class="nc">MoEBuilder</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">Base</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Mixture-of-Experts, Dense and DenseSparse Builder.</span>

<span class="sd">  To be used with xla_sharding + SPMD.</span>

<span class="sd">  MoEBuilder can be used to construct MoE and non-MoE Transformer models.</span>

<span class="sd">  Such models are typically defined by introducing encoder and decoder layer</span>
<span class="sd">  stacks, for example::</span>

<span class="sd">      enc = builder.EncoderLayerStack(&#39;encoder&#39;, [</span>
<span class="sd">          builder.SelfAttention(&#39;self_attention&#39;),</span>
<span class="sd">          builder.MoE(&#39;moe&#39;),</span>
<span class="sd">          builder.SelfAttention(&#39;self_attention&#39;),</span>
<span class="sd">          builder.DenseReluDense(&#39;dense_relu_dense&#39;), ], 3)</span>

<span class="sd">      dec = builder.DecoderLayerStack(&#39;decoder&#39;, [</span>
<span class="sd">          builder.DecSelfAttention(&#39;dec_self_attention&#39;),</span>
<span class="sd">          builder.DecEncAttention(&#39;dec_enc_attention&#39;),</span>
<span class="sd">          builder.MoE(&#39;moe&#39;, decoder=True),</span>
<span class="sd">          builder.DecSelfAttention(&#39;dec_self_attention&#39;),</span>
<span class="sd">          builder.DecEncAttention(&#39;dec_enc_attention&#39;),</span>
<span class="sd">          builder.DenseReluDense(&#39;dense_relu_dense&#39;, decoder=True), ], 3)</span>

<span class="sd">  Each layer (e.g. builder.SelfAttention) is ultimately wrapped with</span>
<span class="sd">  builder.EncoderLayer or builder.DecoderLayer. These wrappers introduce</span>
<span class="sd">  Transformer residual connections and layer norm as well.</span>

<span class="sd">  Naturally supports input packing, where multiple segments are packed in a</span>
<span class="sd">  single vec row (e.g. packing 2 segments in a single row)::</span>

<span class="sd">      vec      [  4,   3,  24]</span>
<span class="sd">      segment_id  [  1,   1,   2] (0 would indicate padding)</span>
<span class="sd">      segment_pos [  0,   1,   0] (0 for first token in the segment etc)</span>

<span class="sd">  by adding Attention bias to Attention logits before applying tf.nn.softmax,</span>
<span class="sd">  bias calculated as follows::</span>

<span class="sd">      SelfAttention</span>
<span class="sd">        segment_id  [  1,   1,   2]</span>
<span class="sd">      =&gt;</span>
<span class="sd">        bias       [[  0,   0,  -X],</span>
<span class="sd">                    [  0,   0,  -X],</span>
<span class="sd">                    [ -X,  -X,   0]], where X is a large number.</span>

<span class="sd">  Segments can only attend to itself::</span>

<span class="sd">      DecSelfAttention</span>
<span class="sd">        segment_id  [  1,   1,   2]</span>
<span class="sd">        segment_pos [  0,   1,   0]</span>
<span class="sd">      =&gt;</span>
<span class="sd">        bias       [[  0,  -X,  -X],</span>
<span class="sd">                    [  0,   0,  -X],</span>
<span class="sd">                    [ -X,  -X,   0]], where X is a large number.</span>

<span class="sd">  Segments can only attend to itself, and pos &#39;i&#39; can only attend to &lt;= &#39;i&#39;</span>
<span class="sd">  subsegment::</span>

<span class="sd">      DecEncAttention</span>
<span class="sd">        segment_id  [  1,   1,   2]</span>
<span class="sd">        encoder_segment_id  [  1,   2]</span>
<span class="sd">      =&gt;</span>
<span class="sd">        bias       [[  0,  -X],</span>
<span class="sd">                    [  0,  -X],</span>
<span class="sd">                    [ -X,   0]], where X is a large number.</span>

<span class="sd">  Encoder layers must share same Graph input_endpoints, output_endpoints,</span>
<span class="sd">  Builder.{MoE,DenseReluDense,SelfAttention},</span>
<span class="sd">  so do Decoder layers (with decoder=true set where appropriate),</span>
<span class="sd">  Builder.{MoE,DenseReluDense,DecSelfAttention,DecEncAttention},</span>
<span class="sd">  so we can universally wrap them with Builder.{Encoder,Decoder}Layer and</span>
<span class="sd">  further stack with Builder.{Encoder,Decoder}LayerStack. To be moved from</span>
<span class="sd">  XlaShardingBuilder.</span>

<span class="sd">  TODO(lepikhin): enable MoE-Attention.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MoEBuilder.Params"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_devices&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;The number of cores to split weights and computation over.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_groups&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The number of groups. Set to None to use num_devices.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;layer_norm_epsilon&#39;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span>
             <span class="s1">&#39;Epsilon for layer norm numerical stability.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;Datatype to use.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;model_dim&#39;</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="s1">&#39;Model dimension that applies to embedding &#39;</span>
        <span class="s1">&#39;layers and all Transformer layers.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dropout_rate&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;Universal dropout rate that applies to inputs, residual, &#39;</span>
        <span class="s1">&#39;and other Transformer layers.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;noise_shape_broadcast_dims&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;A list of dimension where the noise shape is broadcasted. For &#39;</span>
        <span class="s1">&#39;example, noise_shape = [n, h, w, 1] when &#39;</span>
        <span class="s1">&#39;noise_shape_broadcast_dims=[-1] &#39;</span><span class="p">)</span>

    <span class="c1"># attention params</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_num_heads&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Attention number of heads.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;attention_num_memory_heads&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Attention number of memory heads. We only support &#39;</span>
        <span class="s1">&#39;attention_num_memory_heads of 1 or None (default).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_key_value_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Shared dimensionality for Attention keys, values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Attention dropout probability.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;moe_dropout_rate&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;MoE dropout probability.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;attention_combine_dims&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Attention optimization. &#39;</span>
        <span class="s1">&#39;The heads and key/value dimensions are combined in the variables &#39;</span>
        <span class="s1">&#39;and the computation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_combine_qkv&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Attention optimization. &#39;</span>
             <span class="s1">&#39;Combine qkv matmul.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ff_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;DenseReluDense hidden dim.&#39;</span><span class="p">)</span>

    <span class="c1"># MoE params</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;e_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;E dimension. Number of experts.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;c_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;C dimension. Per-expert capacity.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;moe_hidden_dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mixture-of-Experts hidden dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;moe_activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span> <span class="s1">&#39;MoE activation function.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;second_expert_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span>
             <span class="s1">&#39;Mixture-of-Experts dispatch policy.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;second_expert_threshold&#39;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
             <span class="s1">&#39;Mixture-of-Experts second-best gate threshold.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;legacy_mtf_behavior&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s1">&#39;Mixture-of-Experts legacy mtf behavior. No renormalization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;Label smoothing.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;capacity_factor&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Capacity factor. Overrides c_dim.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;gating_func&#39;</span><span class="p">,</span> <span class="s1">&#39;top_2&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Gating function. Can be one of [&quot;top_2&quot;, &quot;token_shuffle&quot;, &quot;optimal_transport&quot;].&#39;</span>
    <span class="p">)</span>

    <span class="c1"># Used in DecSelfAttentionRelativeBias:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;relative_attention_type&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Attention type. None is default. Alternative is &quot;bias&quot;.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;relative_attention_num_buckets&#39;</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span>
             <span class="s1">&#39;Relative attention num buckets.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;relative_attention_max_distance&#39;</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span>
             <span class="s1">&#39;Max relative distance (outer bucket boundary).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;relative_attention_use_universal_1d_position&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Relative attention could rely on fake 1d position tensor, &#39;</span>
        <span class="s1">&#39;since only the relative difference matters and extra large &#39;</span>
        <span class="s1">&#39;negative logit bias term is added for attention across segments &#39;</span>
        <span class="s1">&#39;anyway. Set to True to enable the hack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;inflate_universal_relative_bias_to_match_batch_dimension&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, inflate the relative_bias tensor to match the batch &#39;</span>
        <span class="s1">&#39;dimension of the BLM logits. When BLM is partitioned along the batch &#39;</span>
        <span class="s1">&#39;dimension, this avoids the all-reduce for the relative_bias &#39;</span>
        <span class="s1">&#39;activation gradients, but performs the all-reduce for relative_bias &#39;</span>
        <span class="s1">&#39;weights instead. This may cause computation overhead when batch_size&#39;</span>
        <span class="s1">&#39;is much larger than num_batch_partitions, so it should only be used&#39;</span>
        <span class="s1">&#39;when batch_size is not too large compared to num_batch_partitions. &#39;</span>
        <span class="s1">&#39;This flag is ignored if relative_attention_use_universal_1d_position &#39;</span>
        <span class="s1">&#39;is set to False. Please see b/173612674#comment2 for more details.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;attention_extra_logit&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Extra logit for attention softmax.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;attention_logits_dtype&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Using float32 for attention logits with fprop_dtype=bfloat16 &#39;</span>
        <span class="s1">&#39;generally makes training giant models more stable.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;mask_dtype&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Using bfloat16 for fprop_dtype could be problematic for &#39;</span>
        <span class="s1">&#39;mask tensors, mask_dtype is a special dtype for such tensors.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;gating_logits_dtype&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Using bfloat16 for fprop_dtype could be problematic for &#39;</span>
        <span class="s1">&#39;gating logits, gating_logits_dtype is a special dtype for such &#39;</span>
        <span class="s1">&#39;tensors.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;conv_vars_reshape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Boolean, whether or not to &#39;</span>
        <span class="s1">&#39;change the shape of conv variables. For checkpoint backward &#39;</span>
        <span class="s1">&#39;compatibility only, deprecated soon. If True, the variable shape &#39;</span>
        <span class="s1">&#39;of _LNConv will be based on model_dim_reshape_segment&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_fused_depthwise_conv_autoregressive&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If True, use CausalDepthwiseConv for &#39;</span>
        <span class="s1">&#39;DepthwiseConvAutoregressive.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_no_scale&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Override Builder._LN with Builder._LNNoScale.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;model_dim_reshape_segments&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Size of N when reshaping model dimension M to Nm&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_xla_dynamic_update_slice&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;internal optimization&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="MoEBuilder.SetFPropDtype"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SetFPropDtype">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">SetFPropDtype</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
    <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span>
    <span class="k">if</span> <span class="n">fprop_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_model_dim_reshape_segments</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span>
    <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span><span class="p">]</span>

<div class="viewcode-block" id="MoEBuilder._AdjustMSplit"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._AdjustMSplit">[docs]</a>  <span class="k">def</span> <span class="nf">_AdjustMSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">m_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adjusts split annotation according to model_dim_reshape_segments.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">split</span>
    <span class="n">new_split</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">:</span>
      <span class="n">new_split</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">m_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_split</span></div>

<div class="viewcode-block" id="MoEBuilder._AdjustMSplitByName"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._AdjustMSplitByName">[docs]</a>  <span class="k">def</span> <span class="nf">_AdjustMSplitByName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_split_name</span><span class="p">):</span>
    <span class="n">split</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">p_split_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">m_axis</span> <span class="o">=</span> <span class="n">p_split_name</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m_axis</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">m_axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">split</span></div>

<div class="viewcode-block" id="MoEBuilder._Dropout"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Dropout">[docs]</a>  <span class="k">def</span> <span class="nf">_Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">noise_shape_broadcast_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">noise_shape_broadcast_dims</span> <span class="ow">or</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">noise_shape_broadcast_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._OneHotEncode"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._OneHotEncode">[docs]</a>  <span class="k">def</span> <span class="nf">_OneHotEncode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">fprop_dtype</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder._Var"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Var">[docs]</a>  <span class="k">def</span> <span class="nf">_Var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">shared_var_collection_suffix</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">VarLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">fprop_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
        <span class="n">shared_var_collection_suffix</span><span class="o">=</span><span class="n">shared_var_collection_suffix</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ShardedVar"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ShardedVar">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedVar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a layer of variables potentially sharded in a device mesh.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      weights: list of (name, gshard_layers.ShardedWeightParams).</span>
<span class="sd">      device_mesh: device mesh used in mesh_split. If None, the variables will</span>
<span class="sd">        not be sharded</span>

<span class="sd">    Returns:</span>
<span class="sd">      A layer of variables sharded according to device_mesh and the weights&#39;</span>
<span class="sd">      sharding specification in ShardedWeightParams.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">device_mesh</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedVarLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">fprop_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">,</span>
          <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ShardedVarOn1DDeviceArray"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ShardedVarOn1DDeviceArray">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedVarOn1DDeviceArray</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Variables sharded along dimension 0.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      weights: list of (name, py_utils.WeightParams).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A layer of variables sharded on dimension 0 across all devices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sharded_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
      <span class="n">dims_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">sharded_weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span>
                              <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                                  <span class="n">shape</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                  <span class="n">init</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">init</span><span class="p">,</span>
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                  <span class="n">collections</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">collections</span><span class="p">,</span>
                                  <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">dims_mapping</span><span class="p">)))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="n">sharded_weights</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_devices</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder._EmbeddingWeight"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._EmbeddingWeight">[docs]</a>  <span class="k">def</span> <span class="nf">_EmbeddingWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">name</span><span class="p">,</span>
                       <span class="n">vocab_dim</span><span class="p">,</span>
                       <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">w_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;embedding&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">vocab_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">w_mesh_split</span><span class="p">))],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.SharedEmbSoftmax"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SharedEmbSoftmax">[docs]</a>  <span class="k">def</span> <span class="nf">SharedEmbSoftmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">name</span><span class="p">,</span>
                       <span class="n">vocab_size</span><span class="p">,</span>
                       <span class="n">max_len</span><span class="p">,</span>
                       <span class="n">logits_abs_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">z_loss_coef</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                       <span class="n">use_tgt_labels_size_as_loss_denominator</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">SharedEmbeddingSoftmaxLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">max_len</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span>
        <span class="n">logits_abs_max</span><span class="o">=</span><span class="n">logits_abs_max</span><span class="p">,</span>
        <span class="n">z_loss_coef</span><span class="o">=</span><span class="n">z_loss_coef</span><span class="p">,</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
        <span class="n">label_smoothing</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">,</span>
        <span class="n">use_tgt_labels_size_as_loss_denominator</span><span class="o">=</span><span class="n">use_tgt_labels_size_as_loss_denominator</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.Embedding"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Embedding">[docs]</a>  <span class="k">def</span> <span class="nf">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;emb&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EmbeddingWeight</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;ids-&gt;ids_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;ids_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;ids_split-&gt;one_hot_ids&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_OneHotEncode</span><span class="p">(</span><span class="s1">&#39;one_hot_ids&#39;</span><span class="p">,</span>
                                                      <span class="n">vocab_dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;one_hot_ids-&gt;one_hot_ids_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;one_hot_ids_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;emb,one_hot_ids_split-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;einsum&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;VH,BLV-&gt;BLH&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">))))</span></div>

<div class="viewcode-block" id="MoEBuilder.SoftmaxWeight"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SoftmaxWeight">[docs]</a>  <span class="k">def</span> <span class="nf">SoftmaxWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;softmax_weight&#39;</span><span class="p">,</span>
                  <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">]))])</span></div>

<div class="viewcode-block" id="MoEBuilder.Mask"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Mask">[docs]</a>  <span class="k">def</span> <span class="nf">Mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">_apply_padding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_id</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
      <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_apply_padding</span><span class="p">,</span> <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.EncoderLayer"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.EncoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">EncoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">residual_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns params for lambda x: x + residual_weight * DropOut(layer(LN(x))).&quot;&quot;&quot;</span>
    <span class="n">layer_input_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>
    <span class="n">layer_inputs</span> <span class="o">=</span> <span class="s1">&#39;x,&#39;</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;i.&#39;</span> <span class="o">+</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">layer_input_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,i.segment_id-&gt;input_masked&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;input_masked-&gt;x&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="n">layer_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;y,o.aux_loss&#39;</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;y-&gt;y_dropout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;y_dropout&#39;</span><span class="p">,</span>
                                       <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_masked,y_dropout-&gt;o.vec&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="n">residual_weight</span><span class="o">=</span><span class="n">residual_weight</span><span class="p">)),</span>
    <span class="p">)</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_EncoderLayerInMapKeys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;vec&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_pos&#39;</span><span class="p">]</span>

  <span class="c1"># We avoid Builder._Seq and Builder._Rep to improve theta / checkpoint</span>
  <span class="c1"># readability and reduce layer nesting.</span>
<div class="viewcode-block" id="MoEBuilder.EncoderLayerStack"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.EncoderLayerStack">[docs]</a>  <span class="k">def</span> <span class="nf">EncoderLayerStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">name</span><span class="p">,</span>
                        <span class="n">sub_layers</span><span class="p">,</span>
                        <span class="n">num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">use_repeat_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">spmd_pipeline_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">spmd_pipeline_microbatches</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clean EncoderLayerStack with minimal layer nesting.</span>

<span class="sd">    E.g::</span>

<span class="sd">      encoder/</span>
<span class="sd">        layer_000/</span>
<span class="sd">          ln/w/</span>
<span class="sd">            scale</span>
<span class="sd">          self_attention/w/</span>
<span class="sd">            wq</span>
<span class="sd">            wk</span>
<span class="sd">            wv</span>
<span class="sd">            wo</span>
<span class="sd">        layer_001/</span>
<span class="sd">          ...</span>

<span class="sd">    will be constructed with::</span>

<span class="sd">      builder.EncoderLayerStack(&#39;encoder&#39;, [</span>
<span class="sd">          builder.SelfAttention(&#39;self_attention&#39;),</span>
<span class="sd">          ...], ...)</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Name of this layer</span>
<span class="sd">      sub_layers: Sublayers of the encoder layer.</span>
<span class="sd">      num: Number of encoder layers.</span>
<span class="sd">      use_repeat_layer: bool, whether to wrap num layers into a RepeatLayer.</span>
<span class="sd">      spmd_pipeline_stages: If &gt; 1, use SPMD-shardable pipelining with this many</span>
<span class="sd">        pipeline stages.</span>
<span class="sd">      spmd_pipeline_microbatches: The number of microbatches when SPMD-shardable</span>
<span class="sd">        pipelining is used.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LayerStack</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">sub_layers</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">use_repeat_layer</span><span class="p">,</span>
                            <span class="n">spmd_pipeline_stages</span><span class="p">,</span> <span class="n">spmd_pipeline_microbatches</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span><span class="p">,</span>
                            <span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">p</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder.DecoderLayer"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">DecoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">name</span><span class="p">,</span>
                   <span class="n">layer</span><span class="p">,</span>
                   <span class="n">conv_kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span>
                   <span class="n">norm_policy</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A decoder layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Layer name.</span>
<span class="sd">      layer: Layer logic added to the compute graph.</span>
<span class="sd">      conv_kernel_size: The width of the kernel when convolution is added to</span>
<span class="sd">        layer norm.</span>
<span class="sd">      norm_type: String that describes the normalization type. Currently only</span>
<span class="sd">        supports &#39;ln&#39;.</span>
<span class="sd">      norm_policy: String that describes the policy for applying normalzation.</span>
<span class="sd">        Currently only supports &#39;pre&#39; for pre-transformation normalzation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Compute graph of decoder layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">conv_kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">norm_type</span> <span class="o">!=</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only ln supports conv. </span><span class="si">%s</span><span class="s1"> does not support conv.&#39;</span> <span class="o">%</span>
                         <span class="n">norm_type</span><span class="p">)</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNConv</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;true_ln&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_TrueLN</span><span class="p">(</span><span class="s1">&#39;true_ln&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;pn&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PN</span><span class="p">(</span><span class="s1">&#39;pn&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Norm type </span><span class="si">%s</span><span class="s1"> not supported.&#39;</span> <span class="o">%</span> <span class="n">norm_type</span><span class="p">)</span>
    <span class="n">layer_input_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="n">layer_inputs</span> <span class="o">=</span> <span class="s1">&#39;x,&#39;</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;i.&#39;</span> <span class="o">+</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">layer_input_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
    <span class="k">if</span> <span class="n">norm_policy</span> <span class="o">==</span> <span class="s1">&#39;pre&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
          <span class="n">name</span><span class="p">,</span>
          <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>
          <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>
          <span class="p">(</span><span class="s1">&#39;i.vec,i.segment_id-&gt;input_masked&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;input_masked-&gt;x&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">),</span>
          <span class="p">(</span><span class="n">layer_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;y,o.aux_loss&#39;</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span>
          <span class="p">(</span><span class="s1">&#39;y-&gt;y_dropout&#39;</span><span class="p">,</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;y_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;input_masked,y_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
      <span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_policy</span> <span class="o">==</span> <span class="s1">&#39;primer&#39;</span><span class="p">:</span>
      <span class="n">indx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:])</span>
      <span class="k">if</span> <span class="n">indx</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>
            <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>
            <span class="p">(</span><span class="s1">&#39;i.vec,i.segment_id-&gt;input_masked&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;input_masked-&gt;x&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">),</span>
            <span class="p">(</span><span class="n">layer_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;y,o.aux_loss&#39;</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;y-&gt;y_dropout&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;y_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
            <span class="p">(</span><span class="s1">&#39;input_masked,y_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
        <span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>
            <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>
            <span class="p">(</span><span class="s1">&#39;i.vec,i.segment_id-&gt;x&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
            <span class="p">(</span><span class="n">layer_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;y,o.aux_loss&#39;</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;y-&gt;y_norm&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;y_norm-&gt;y_dropout&#39;</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;y_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
            <span class="p">(</span><span class="s1">&#39;x,y_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)),</span>
        <span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unsupported norm policy: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">norm_policy</span><span class="p">)</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_DecoderLayerInMapKeys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="s1">&#39;vec&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_pos&#39;</span><span class="p">,</span> <span class="s1">&#39;encoder_output&#39;</span><span class="p">,</span>
        <span class="s1">&#39;encoder_segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;encoder_segment_pos&#39;</span>
    <span class="p">]</span>

<div class="viewcode-block" id="MoEBuilder.DecoderLayerStack"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecoderLayerStack">[docs]</a>  <span class="k">def</span> <span class="nf">DecoderLayerStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">name</span><span class="p">,</span>
                        <span class="n">sub_layers</span><span class="p">,</span>
                        <span class="n">num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">conv_kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span>
                        <span class="n">norm_policy</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">,</span>
                        <span class="n">use_repeat_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">spmd_pipeline_stages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">spmd_pipeline_microbatches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">start_layer_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">has_final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clean DecoderLayerStack, similar to EncoderLayerStack.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_DecoderLayer</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">DecoderLayer</span><span class="p">(</span>
          <span class="n">n</span><span class="p">,</span>
          <span class="n">p</span><span class="p">,</span>
          <span class="n">conv_kernel_size</span><span class="o">=</span><span class="n">conv_kernel_size</span><span class="p">,</span>
          <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span>
          <span class="n">norm_policy</span><span class="o">=</span><span class="n">norm_policy</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LayerStack</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">sub_layers</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">use_repeat_layer</span><span class="p">,</span>
                            <span class="n">spmd_pipeline_stages</span><span class="p">,</span> <span class="n">spmd_pipeline_microbatches</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="n">_DecoderLayer</span><span class="p">,</span>
                            <span class="n">start_layer_id</span><span class="p">,</span> <span class="n">has_final_layer</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.Repeat"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Repeat">[docs]</a>  <span class="k">def</span> <span class="nf">Repeat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">per_layer_vars</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper to call builder_layers.RepeatLayer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">builder_layers</span><span class="o">.</span><span class="n">RepeatLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span>
        <span class="n">repeat</span><span class="o">=</span><span class="n">repeat</span><span class="p">,</span>
        <span class="n">per_layer_vars</span><span class="o">=</span><span class="n">per_layer_vars</span><span class="p">,</span>
        <span class="n">unroll</span><span class="o">=</span><span class="s1">&#39;eval_only&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.ShardablePipeline"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.ShardablePipeline">[docs]</a>  <span class="k">def</span> <span class="nf">ShardablePipeline</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">stages</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper to call gshard_layers.LayerwiseShardablePipelinedLayer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">LayerwiseShardablePipelinedLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="n">stages</span><span class="p">,</span> <span class="n">single_stage_body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._LayerStack"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._LayerStack">[docs]</a>  <span class="k">def</span> <span class="nf">_LayerStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">name</span><span class="p">,</span>
                  <span class="n">sub_layers</span><span class="p">,</span>
                  <span class="n">num</span><span class="p">,</span>
                  <span class="n">use_repeat_layer</span><span class="p">,</span>
                  <span class="n">spmd_pipeline_stages</span><span class="p">,</span>
                  <span class="n">spmd_pipeline_microbatches</span><span class="p">,</span>
                  <span class="n">imap_keys</span><span class="p">,</span>
                  <span class="n">layer_fn</span><span class="p">,</span>
                  <span class="n">start_layer_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">has_final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># TODO(yuanzx): Consider refactor this into a layer.</span>
    <span class="k">assert</span> <span class="s1">&#39;segment_id&#39;</span> <span class="ow">in</span> <span class="n">imap_keys</span>
    <span class="k">if</span> <span class="n">use_repeat_layer</span><span class="p">:</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">deterministic_dropout</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">:</span>
      <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="p">(</span><span class="s1">&#39;i.&#39;</span> <span class="o">+</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;-&gt;&#39;</span> <span class="o">+</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span><span class="p">)))</span>

    <span class="n">stack</span> <span class="o">+=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">imap_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_split-&gt;x_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">start_layer_id</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.aux_loss-&gt;loss_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">start_layer_id</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">start_layer_id</span><span class="p">)),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_SubLayersBlock</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
      <span class="n">map_inputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,&#39;</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="k">return</span> <span class="p">[((</span><span class="n">map_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;imap_</span><span class="si">%03d</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">),</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">_CreateNestedMap</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;imap_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="n">imap_keys</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;imap_</span><span class="si">%03d</span><span class="s1">-&gt;omap_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">),</span>
               <span class="n">layer_fn</span><span class="p">(</span><span class="s1">&#39;layer_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">l</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;omap_</span><span class="si">%03d</span><span class="s1">.vec-&gt;x_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;vec_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">,omap_</span><span class="si">%03d</span><span class="s1">.aux_loss-&gt;loss_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))]</span>

    <span class="n">i</span> <span class="o">=</span> <span class="n">start_layer_id</span>
    <span class="k">assert</span> <span class="n">num</span> <span class="o">%</span> <span class="n">spmd_pipeline_stages</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">layers_per_stage</span> <span class="o">=</span> <span class="n">num</span> <span class="o">//</span> <span class="n">spmd_pipeline_stages</span>
    <span class="n">main_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">use_repeat_layer</span><span class="p">:</span>
      <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sub_layers</span><span class="p">:</span>
        <span class="n">blocks</span> <span class="o">+=</span> <span class="n">_SubLayersBlock</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">body_inputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,loss_</span><span class="si">%03d</span><span class="s1">,&#39;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="n">start_layer_id</span><span class="p">,</span> <span class="n">start_layer_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
              <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">body_outputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,loss_</span><span class="si">%03d</span><span class="s1">,&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">body_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="s1">&#39;blocks_body&#39;</span><span class="p">,</span> <span class="n">body_inputs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">),</span>
                           <span class="n">body_outputs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">),</span> <span class="o">*</span><span class="n">blocks</span><span class="p">)</span>
      <span class="n">repeat_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Repeat</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;blocks&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body_p</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">layers_per_stage</span><span class="p">)</span>
      <span class="n">main_stack</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">(</span><span class="n">body_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;&#39;</span> <span class="o">+</span> <span class="n">body_outputs</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_split&#39;</span><span class="p">,</span> <span class="s1">&#39;_split_out&#39;</span><span class="p">),</span>
           <span class="n">repeat_p</span><span class="p">)</span>
      <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers_per_stage</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sub_layers</span><span class="p">:</span>
          <span class="c1"># x_i, loss_i =&gt; x_{i+1}, loss_{i+1}</span>
          <span class="n">main_stack</span> <span class="o">+=</span> <span class="n">_SubLayersBlock</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
          <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">spmd_pipeline_stages</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># TODO(yuanzx): Consider refactor this into a layer.</span>

      <span class="c1"># Reshape each input into microbatches.</span>
      <span class="k">def</span> <span class="nf">_ToMicroBatches</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">_ReshapeToMicroBatches</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
          <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">spmd_pipeline_microbatches</span> <span class="o">==</span> <span class="mi">0</span>
          <span class="c1"># First reshape to [microbatch_size, spmd_pipeline_microbatches, ..]</span>
          <span class="c1"># then transpose to [spmd_pipeline_microbatches, microbatch_size, ...]</span>
          <span class="c1"># because we want each microbatch to be sharded the same way as the</span>
          <span class="c1"># original batch dimension.</span>
          <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span>
              <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">spmd_pipeline_microbatches</span><span class="p">,</span>
              <span class="n">spmd_pipeline_microbatches</span>
          <span class="p">]</span>
          <span class="n">new_shape</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
          <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)))</span>
          <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
          <span class="n">perm</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">),</span> <span class="n">perm</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;-&gt;&#39;</span> <span class="o">+</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_m&#39;</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_microbatched&#39;</span><span class="p">,</span> <span class="n">_ReshapeToMicroBatches</span><span class="p">))</span>

      <span class="n">stack</span> <span class="o">+=</span> <span class="p">[</span>
          <span class="n">_ToMicroBatches</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;x_000&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
      <span class="p">]</span>

      <span class="c1"># Pipelining requires each input/output to have a num_microbatches</span>
      <span class="c1"># dimension, but loss is a scalar. We pad it to the shape</span>
      <span class="c1"># [spmd_pipeline_microbatches] to compute per-microbatch loss, then sum</span>
      <span class="c1"># them together after the pipeline.</span>
      <span class="k">def</span> <span class="nf">_PadLoss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">spmd_pipeline_microbatches</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]])</span>

      <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="p">(</span><span class="s1">&#39;loss_000-&gt;loss_000_m&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;loss_padded_microbatched&#39;</span><span class="p">,</span>
                                            <span class="n">_PadLoss</span><span class="p">)))</span>
      <span class="n">body_inputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,loss_</span><span class="si">%03d</span><span class="s1">,&#39;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="n">start_layer_id</span><span class="p">,</span> <span class="n">start_layer_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
              <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">body_outputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,loss_</span><span class="si">%03d</span><span class="s1">,&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">body_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="s1">&#39;pipeline_body&#39;</span><span class="p">,</span> <span class="n">body_inputs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">),</span>
                           <span class="n">body_outputs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">),</span> <span class="o">*</span><span class="n">main_stack</span><span class="p">)</span>
      <span class="n">pipeline_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ShardablePipeline</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;pipeline&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body_p</span><span class="p">,</span> <span class="n">stages</span><span class="o">=</span><span class="n">spmd_pipeline_stages</span><span class="p">)</span>
      <span class="n">pipeline_inputs</span> <span class="o">=</span> <span class="s1">&#39;x_000_m,loss_000_m,&#39;</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split_m&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">pipeline_outputs</span> <span class="o">=</span> <span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">_m,loss_</span><span class="si">%03d</span><span class="s1">_m,&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split_out_m&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="n">main_stack</span> <span class="o">=</span> <span class="p">[(</span><span class="n">pipeline_inputs</span> <span class="o">+</span> <span class="s1">&#39;-&gt;&#39;</span> <span class="o">+</span> <span class="n">pipeline_outputs</span><span class="p">,</span> <span class="n">pipeline_p</span><span class="p">)]</span>

      <span class="c1"># Reshape outputs to the original shape without microbatches.</span>
      <span class="k">def</span> <span class="nf">_ToBatches</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">_ReshapeToBatches</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
          <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
          <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
          <span class="n">perm</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_m-&gt;&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_unmicrobatched&#39;</span><span class="p">,</span> <span class="n">_ReshapeToBatches</span><span class="p">))</span>

      <span class="n">main_stack</span> <span class="o">+=</span> <span class="p">[</span>
          <span class="n">_ToBatches</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">([</span><span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span>
                                  <span class="p">[</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_split_out&#39;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">imap_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
      <span class="p">]</span>
      <span class="c1"># Sum the per-microbatch losses.</span>
      <span class="n">main_stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">_m-&gt;loss_</span><span class="si">%03d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;loss_combined&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">)))</span>

    <span class="n">stack</span> <span class="o">+=</span> <span class="n">main_stack</span>
    <span class="k">if</span> <span class="n">has_final_layer</span><span class="p">:</span>
      <span class="n">stack</span> <span class="o">+=</span> <span class="p">[</span>
          <span class="p">((</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">-&gt;o.aux_loss&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;output_loss&#39;</span><span class="p">)),</span>
          <span class="p">((</span><span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">-&gt;y_norm&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;final_layer_norm&#39;</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;y_norm-&gt;y_dropout&#39;</span><span class="p">,</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;outputs_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;y_dropout,segment_id_split-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
      <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">stack</span> <span class="o">+=</span> <span class="p">[</span>
          <span class="p">((</span><span class="s1">&#39;loss_</span><span class="si">%03d</span><span class="s1">-&gt;o.aux_loss&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;output_loss&#39;</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;x_</span><span class="si">%03d</span><span class="s1">,segment_id_split-&gt;o.vec&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
      <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">stack</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._DenseReluDenseWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._DenseReluDenseWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_DenseReluDenseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">name</span><span class="p">,</span>
                             <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">wi_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">wo_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wi_mesh_split</span><span class="p">)),</span>
                 <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wo_mesh_split</span><span class="p">))],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.DenseReluDense"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DenseReluDense">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDense</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>
    <span class="c1"># Note that dropout is used here, but not in the MoE layer by default.</span>

    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;gelu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;sqr_relu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Activation </span><span class="si">%s</span><span class="s1"> not supported.&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">input_endpoints</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DenseReluDenseWeights</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wi,vec-&gt;h&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">wi</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;MH,BLM-&gt;BLH&#39;</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">vec</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;h-&gt;h_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h_</span><span class="si">%s</span><span class="s1">-&gt;h_dropout&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wo,h_dropout-&gt;outputs_pre_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
             <span class="s1">&#39;wo&#39;</span><span class="p">,</span>
             <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">wo</span><span class="p">,</span> <span class="n">h_dropout</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;HM,BLH-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">h_dropout</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._DenseReluDenseWeightsGatedGELU"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._DenseReluDenseWeightsGatedGELU">[docs]</a>  <span class="k">def</span> <span class="nf">_DenseReluDenseWeightsGatedGELU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                      <span class="n">name</span><span class="p">,</span>
                                      <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">wi_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">wo_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Gated GELU.  There are two separate linear transformations applied in</span>
    <span class="c1"># parallel to the inputs.  You take the gelu of one of them and then</span>
    <span class="c1"># multiply the two componentwise.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wi_0&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wi_mesh_split</span><span class="p">)),</span>
                 <span class="p">(</span><span class="s1">&#39;wi_1&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wi_mesh_split</span><span class="p">)),</span>
                 <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wo_mesh_split</span><span class="p">))],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.DenseReluDenseGated"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DenseReluDenseGated">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDenseGated</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>

    <span class="k">def</span> <span class="nf">_Impl</span><span class="p">(</span><span class="n">wi_0</span><span class="p">,</span> <span class="n">wi_1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
          <span class="n">activation_fn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;MH,BLM-&gt;BLH&#39;</span><span class="p">,</span> <span class="n">wi_0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)),</span>
          <span class="c1"># linear / pass-through</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;MH,BLM-&gt;BLH&#39;</span><span class="p">,</span> <span class="n">wi_1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">input_endpoints</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi_0,wi_1,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DenseReluDenseWeightsGatedGELU</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wi_0,wi_1,vec-&gt;h&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_Impl</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h-&gt;h_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wo,h_dropout-&gt;outputs_pre_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
             <span class="s1">&#39;wo&#39;</span><span class="p">,</span>
             <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">wo</span><span class="p">,</span> <span class="n">h_dropout</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;HM,BLH-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">h_dropout</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.DenseReluDenseGatedGELU"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DenseReluDenseGatedGELU">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDenseGatedGELU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">DenseReluDenseGated</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.DenseReluDenseGatedSILU"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DenseReluDenseGatedSILU">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDenseGatedSILU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">DenseReluDenseGated</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.MoE"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.MoE">[docs]</a>  <span class="k">def</span> <span class="nf">MoE</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns layer params to compute (outputs, scalar_aux_loss).&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">input_endpoints</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;vec-&gt;input_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;input_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id-&gt;segment_id_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;segment_id_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedFeedForwardNetworksWeights</span><span class="p">(</span><span class="n">name</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_split,segment_id_split,wi,wo-&gt;outputs_pre_split,aux_loss&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedMoEPositionWiseFeedForwardNetworks</span><span class="p">(</span><span class="s1">&#39;ffw&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">)))</span></div>

  <span class="c1"># Multi-headed attention Tensors:</span>
  <span class="c1"># q: BLHD [batch, length,        heads, key_value]</span>
  <span class="c1"># k: BMHD [batch, memory_length, heads, key_value]</span>
  <span class="c1"># v: BMHD [batch, memory_length, heads, key_value]</span>
  <span class="c1">#</span>
  <span class="c1"># logits:  BLHM</span>
  <span class="c1"># bias:    BLM</span>
  <span class="c1">#</span>
  <span class="c1"># weights: BLHM [batch, length, heads, memory_length]</span>
  <span class="c1">#</span>
  <span class="c1"># outputs: BLHD [batch, length, heads, key_value]</span>
<div class="viewcode-block" id="MoEBuilder.Attention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Attention">[docs]</a>  <span class="k">def</span> <span class="nf">Attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention with multiple attention heads.</span>

<span class="sd">    Keys, values share same dimensionality</span>
<span class="sd">    params.self.params.attention_key_value_dim.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer</span>

<span class="sd">    Returns:</span>
<span class="sd">      The Attention layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_AddBiasF32</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
      <span class="c1"># logits: BLHM [batch, length, heads, memory_length]</span>
      <span class="c1"># bias: BLHM [batch, length, heads, memory_length]</span>
      <span class="c1">#       (in case of attention with relative bias) OR</span>
      <span class="c1">#</span>
      <span class="c1">#       BLM  [batch, length, memory_length]</span>
      <span class="c1">#       (default masking bias with very negative logits).</span>
      <span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Expanding the &#39;heads&#39; dimension</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">bias</span>
      <span class="k">return</span> <span class="n">retval</span>

    <span class="k">def</span> <span class="nf">_ReduceLogsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="n">max_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

      <span class="n">extra_logit</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_extra_logit</span>
      <span class="k">if</span> <span class="n">extra_logit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">extra_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">extra_logit</span><span class="p">,</span> <span class="n">max_logit</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">max_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_logit</span><span class="p">,</span> <span class="n">extra_logit</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">-=</span> <span class="n">max_logit</span>
      <span class="n">exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">extra_logit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sum_exp_x</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">extra_logit</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sum_exp_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">max_logit</span>

    <span class="k">def</span> <span class="nf">_LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">_ReduceLogsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_LogitsFnF32</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
      <span class="c1"># logits.dtype == tf.float32 leads to better training stability</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHD,BMHD-&gt;BLHM&#39;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_SoftmaxF32</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="c1"># expecting x.dtype == tf.float32</span>
      <span class="c1">#</span>
      <span class="c1"># TODO(lepikhin): consider</span>
      <span class="c1"># if p.attention_extra_logit is None:</span>
      <span class="c1">#   return tf.nn.softmax(x)</span>
      <span class="n">softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">_LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
      <span class="n">softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">softmax</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;_q&#39;</span><span class="p">,</span> <span class="s1">&#39;_k&#39;</span><span class="p">,</span> <span class="s1">&#39;_v&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;_q-&gt;q&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;_q&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;_k-&gt;k&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;_k&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;_v-&gt;v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;_v&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k-&gt;l&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_LogitsFnF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;l,bias-&gt;logits&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_AddBiasF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;logits-&gt;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">_SoftmaxF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;w-&gt;weights&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">attention_dropout_prob</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;weights,v-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
             <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
             <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHM,BMHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">))),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ComputeAttenOutputs"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ComputeAttenOutputs">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeAttenOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">wo</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
      <span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">wo</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;HDM,BLHD-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._EncNotVisible"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._EncNotVisible">[docs]</a>  <span class="k">def</span> <span class="nf">_EncNotVisible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;a, b are encoder_segment_id, decoder_segment_id Tensors.&quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># NotVisible == (a != b) || !((a!=0) || (b != 0))</span>
    <span class="c1">#            == (a != b) || ((a==0) &amp;&amp; (b == 0))</span>
    <span class="c1"># ignore segment_id == 0.</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder.SelfAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SelfAttention">[docs]</a>  <span class="k">def</span> <span class="nf">SelfAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">name</span><span class="p">,</span>
                    <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TransformerEncoder SelfAttention.&quot;&quot;&quot;</span>
    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id-&gt;bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span>
                  <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncNotVisible</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e+09</span><span class="p">),</span>
                  <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;q,k,v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k,v,bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder.DecEncAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecEncAttention">[docs]</a>  <span class="k">def</span> <span class="nf">DecEncAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">name</span><span class="p">,</span>
                      <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer Decoder-Encoder Attention.&quot;&quot;&quot;</span>
    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id,encoder_segment_id-&gt;bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="o">-</span><span class="mf">1e+09</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncNotVisible</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq-&gt;q&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKV</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;encoder_output,wk-&gt;k&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKV</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;encoder_output,wv-&gt;v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKV</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k,v,bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder._DecNotVisible"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._DecNotVisible">[docs]</a>  <span class="k">def</span> <span class="nf">_DecNotVisible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_id</span><span class="p">,</span> <span class="n">segment_pos</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Causal padding with segment_id and segment_pos.&quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="c1"># position (~row) is less that memory position(~column)</span>
    <span class="n">causal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">segment_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">causal</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder._DecComputeBiasGraphEdge"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._DecComputeBiasGraphEdge">[docs]</a>  <span class="k">def</span> <span class="nf">_DecComputeBiasGraphEdge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns an edge of GraphLayer to compute attenion bias for Decoders.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="s1">&#39;segment_id,segment_pos-&gt;qq_bias&#39;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
                <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecNotVisible</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e+09</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder.DecSelfAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecSelfAttention">[docs]</a>  <span class="k">def</span> <span class="nf">DecSelfAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">name</span><span class="p">,</span>
                       <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TransformerDecoder SelfAttention.</span>

<span class="sd">    Note that attention bias (see _DecNotvisible) ensures that current position</span>
<span class="sd">    (~row) is less that memory position(~column).</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      device_mesh: device_mesh for sharding (if specified)</span>
<span class="sd">      w_qkv_mhd_mesh_split: mesh split for qkv weigthts (if specified)</span>
<span class="sd">      wo_hdm_mesh_split: mesh split for output weights (if specified)</span>

<span class="sd">    Returns:</span>
<span class="sd">      layer params for TransformerDecoder SelfAttention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;q,k,v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DecComputeBiasGraphEdge</span><span class="p">(),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;bias_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,bias_full-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder.DecMultiDconvHeadAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecMultiDconvHeadAttention">[docs]</a>  <span class="k">def</span> <span class="nf">DecMultiDconvHeadAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                 <span class="n">name</span><span class="p">,</span>
                                 <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TransformerDecoder DecMultiDconvHeadAttention.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      device_mesh: device_mesh for sharding (if specified)</span>
<span class="sd">      w_qkv_mhd_mesh_split: mesh split for qkv weigthts (if specified)</span>
<span class="sd">      wo_hdm_mesh_split: mesh split for output weights (if specified)</span>

<span class="sd">    Returns:</span>
<span class="sd">      layer params for TransformerDecoder DecMultiDconvHeadAttention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;pre_q,pre_k,pre_v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;pre_q-&gt;q&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;q_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;pre_k-&gt;k&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;k_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;pre_v-&gt;v&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;v_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DecComputeBiasGraphEdge</span><span class="p">(),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;bias_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,bias_full-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder._RelativePositionBucket"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._RelativePositionBucket">[docs]</a>  <span class="k">def</span> <span class="nf">_RelativePositionBucket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">relative_position</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span>
    <span class="n">max_distance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">relative_attention_max_distance</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="o">-</span><span class="n">relative_position</span>
    <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
      <span class="n">num_buckets</span> <span class="o">//=</span> <span class="mi">2</span>
      <span class="n">ret</span> <span class="o">+=</span> <span class="n">_ToInt32</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">*</span> <span class="n">num_buckets</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">n</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># now n is in the range [0, inf)</span>
    <span class="n">max_exact</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">is_small</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">max_exact</span><span class="p">)</span>
    <span class="c1"># should be component-wise tf.math.log</span>
    <span class="n">val_if_large</span> <span class="o">=</span> <span class="n">max_exact</span> <span class="o">+</span> <span class="n">_ToInt32</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span> <span class="o">/</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_distance</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_buckets</span> <span class="o">-</span> <span class="n">max_exact</span><span class="p">))</span>
    <span class="n">val_if_large</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">val_if_large</span><span class="p">,</span> <span class="n">num_buckets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_small</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">val_if_large</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>

  <span class="c1"># When training query_segment_pos = key_segment_pos, of shape [batch, time].</span>
  <span class="c1"># When decoding query_segment_pos is [batch, beam_size]</span>
  <span class="c1"># but key_segment_pos is [batch, memory_size] (because of k_pos StateLayer).</span>
<div class="viewcode-block" id="MoEBuilder._AddRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._AddRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">_AddRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">bias</span><span class="p">,</span>
                       <span class="n">query_segment_pos</span><span class="p">,</span>
                       <span class="n">key_segment_pos</span><span class="p">,</span>
                       <span class="n">relative_bias_weights</span><span class="p">,</span>
                       <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_use_universal_1d_position</span><span class="p">:</span>
      <span class="k">assert</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">key_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span>
          <span class="n">query_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="p">(</span><span class="n">key_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                          <span class="n">query_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">len_dim</span> <span class="o">=</span> <span class="n">key_segment_pos</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">key_segment_pos</span> <span class="o">=</span> <span class="n">query_segment_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">len_dim</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Relative position is defined in such a way that when query is in the</span>
    <span class="c1"># future relative to the key, the value of relative position is negative.</span>
    <span class="n">relative_position</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">key_segment_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_segment_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">relative_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RelativePositionBucket</span><span class="p">(</span><span class="n">relative_position</span><span class="p">,</span>
                                                   <span class="n">bidirectional</span><span class="p">)</span>

    <span class="n">relative_bucket_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">relative_bucket</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="c1"># relative_bucket_one_hot:</span>
    <span class="c1"># ..LJX - [batch?, length, memory_length, num_buckets]</span>
    <span class="c1">#</span>
    <span class="c1"># relative_bias_weights:</span>
    <span class="c1"># HX - [num_heads, num_buckets]</span>
    <span class="c1">#</span>
    <span class="c1"># relative_bias_inc:</span>
    <span class="c1"># [batch?, length, heads, memory_length]</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">relative_attention_use_universal_1d_position</span> <span class="ow">and</span>
        <span class="n">p</span><span class="o">.</span><span class="n">inflate_universal_relative_bias_to_match_batch_dimension</span><span class="p">):</span>
      <span class="k">assert</span> <span class="n">relative_bucket_one_hot</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">4</span>
      <span class="n">relative_bucket_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">relative_bucket_one_hot</span><span class="p">,</span>
                                        <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="n">relative_bias_inc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;HX,...LJX-&gt;...LHJ&#39;</span><span class="p">,</span> <span class="n">relative_bias_weights</span><span class="p">,</span>
                                  <span class="n">relative_bucket_one_hot</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">relative_bias_inc</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">inflate_universal_relative_bias_to_match_batch_dimension</span><span class="p">):</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_use_universal_1d_position</span>
      <span class="n">relative_bias_inc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">relative_bias_inc</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Eventually we add bias to BLHM [batch, length, heads, memory_length]</span>
    <span class="c1"># logits tensor, so we make &#39;heads&#39; dim next to last.</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">relative_bias_inc</span></div>

<div class="viewcode-block" id="MoEBuilder._EncoderAddRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._EncoderAddRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">_EncoderAddRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">segment_pos</span><span class="p">,</span> <span class="n">relative_bias_weights</span><span class="p">):</span>
    <span class="n">query_segment_pos</span><span class="p">,</span> <span class="n">key_segment_pos</span> <span class="o">=</span> <span class="n">segment_pos</span><span class="p">,</span> <span class="n">segment_pos</span>
    <span class="n">bidirectional</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Encoder attention bias is always bidirectional.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AddRelativeBias</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">query_segment_pos</span><span class="p">,</span> <span class="n">key_segment_pos</span><span class="p">,</span>
                                 <span class="n">relative_bias_weights</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.SelfAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SelfAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">SelfAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">name</span><span class="p">,</span>
                                <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TransformerEncoder SelfAttention with relative Attention Bias.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias_shared&#39;</span><span class="p">:</span>
      <span class="c1"># Collection name is used as a unique ID to retrieve the shared variable.</span>
      <span class="c1">#</span>
      <span class="c1"># This name must be different for SelfAttentionRelativeBias (Encoder), and</span>
      <span class="c1"># must have a suffix matching shared_var_collection_suffix, e.g.</span>
      <span class="c1"># &#39;shared_var&#39;.</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_self_attention_shared_var&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span>

    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;relative_bias_weights&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id-&gt;segment_bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span>
                  <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncNotVisible</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e+09</span><span class="p">),</span>
                  <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;segment_bias,segment_pos,relative_bias_weights-&gt;bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;relative_bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_EncoderAddRelativeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;q,k,v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k,v,bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder.DecSelfAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecSelfAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">DecSelfAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">name</span><span class="p">,</span>
                                   <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DecSelfAttention with relative Attention Bias.</span>

<span class="sd">    Note that attention bias (see _DecNotVisible) ensures that current position</span>
<span class="sd">    (~row) is less that memory position(~column).</span>

<span class="sd">    In addition to masking bias we use per-head per-relative position bucket</span>
<span class="sd">    relative_bias_weights tensor (see _RelativeAttentionBiasWeights) of shape</span>
<span class="sd">    [num heads, num relative position buckets]</span>
<span class="sd">    (e.g. [128, 32] for Meena 64B).</span>

<span class="sd">    We compute relative position bucket for every position pair, relative_bucket</span>
<span class="sd">    tensor of shape [batch, length, length] and do</span>
<span class="sd">    tf.gather(relative_bias_weights, relative_bucket, axis=1)</span>
<span class="sd">    to compute per position-pair bias.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      device_mesh: device_mesh for sharding (if specified)</span>
<span class="sd">      w_qkv_mhd_mesh_split: mesh split for qkv weigthts (if specified)</span>
<span class="sd">      wo_hdm_mesh_split: mesh split for output weights (if specified)</span>

<span class="sd">    Returns:</span>
<span class="sd">      The layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias_shared&#39;</span><span class="p">:</span>
      <span class="c1"># Collection name is used as a unique ID to retrieve the shared variable.</span>
      <span class="c1">#</span>
      <span class="c1"># This name must be different for SelfAttentionRelativeBias (Encoder), and</span>
      <span class="c1"># must have a suffix matching shared_var_collection_suffix, e.g.</span>
      <span class="c1"># &#39;shared_var&#39;.</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_dec_self_attention_shared_var&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span>

    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;relative_bias_weights&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;q,k,v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_pos-&gt;key_segment_pos&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;seg_pos&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DecComputeBiasGraphEdge</span><span class="p">(),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;qk_bias&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;qk_bias,segment_pos,key_segment_pos,relative_bias_weights-&gt;qhk_bias&#39;</span><span class="p">,</span>
         <span class="c1"># Decoder _AddRelativeBias always has bidirectional=False.</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;relative_bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AddRelativeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,qhk_bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder.DecMultiDconvHeadAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DecMultiDconvHeadAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">DecMultiDconvHeadAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                             <span class="n">name</span><span class="p">,</span>
                                             <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                             <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                             <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Primer Multi-Dconv-Head Attention with relative attention bias.</span>

<span class="sd">    This follows the same logic as DecSelfAttentionRelativeBias(), with the</span>
<span class="sd">    Primer multi-head depthwise convolutions.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: name of the layer.</span>
<span class="sd">      device_mesh: device_mesh for sharding (if specified).</span>
<span class="sd">      w_qkv_mhd_mesh_split: mesh split for qkv weigthts (if specified).</span>
<span class="sd">      wo_hdm_mesh_split: mesh split for output weights (if specified).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The layer params.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias_shared&#39;</span><span class="p">:</span>
      <span class="c1"># Collection name is used as a unique ID to retrieve the shared variable.</span>
      <span class="c1">#</span>
      <span class="c1"># This name must be different for SelfAttentionRelativeBias (Encoder), and</span>
      <span class="c1"># must have a suffix matching shared_var_collection_suffix, e.g.</span>
      <span class="c1"># &#39;shared_var&#39;.</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_dec_self_attention_shared_var&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span>

    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="c1"># pyformat: disable</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;outputs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;aux_loss&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span>
            <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">,</span> <span class="n">wo_hdm_mesh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;relative_bias_weights&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv-&gt;pre_q,pre_k,pre_v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVCombine</span><span class="p">(</span><span class="s1">&#39;qkv&#39;</span><span class="p">)),</span>
        <span class="c1"># Note: This does not use shared Q and K representations.</span>
        <span class="p">(</span><span class="s1">&#39;pre_q-&gt;q&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;q_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;pre_k-&gt;k&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;k_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;pre_v-&gt;v&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;v_dconv&#39;</span><span class="p">,</span>
                                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">model_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
                                                      <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_pos-&gt;key_segment_pos&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;seg_pos&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DecComputeBiasGraphEdge</span><span class="p">(),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;qk_bias&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;qk_bias,segment_pos,key_segment_pos,relative_bias_weights-&gt;qhk_bias&#39;</span><span class="p">,</span>
         <span class="c1"># Decoder _AddRelativeBias always has bidirectional=False.</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;relative_bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AddRelativeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,qhk_bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o,wo-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ComputeAttenOutputs</span><span class="p">)))</span></div>
    <span class="c1"># pyformat: enable</span>

<div class="viewcode-block" id="MoEBuilder._RelativeAttentionBiasWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._RelativeAttentionBiasWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper for &#39;-&gt;rb&#39; Graph edge.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">+=</span> <span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">shared_var_collection_suffix</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">collections</span><span class="p">:</span>
      <span class="n">shared_var_collection_suffix</span> <span class="o">=</span> <span class="s1">&#39;shared_var&#39;</span>
    <span class="n">rb_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">rb_tpl</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_num_buckets</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">rb_stddev</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">rb_tpl</span><span class="p">)],</span>
        <span class="n">shared_var_collection_suffix</span><span class="o">=</span><span class="n">shared_var_collection_suffix</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._zero_aux_loss"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._zero_aux_loss">[docs]</a>  <span class="k">def</span> <span class="nf">_zero_aux_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder.CausalDepthwiseConv"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.CausalDepthwiseConv">[docs]</a>  <span class="k">def</span> <span class="nf">CausalDepthwiseConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">model_dims</span> <span class="o">=</span> <span class="n">model_dims</span> <span class="ow">or</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span>
    <span class="n">model_dims</span> <span class="o">=</span> <span class="n">model_dims</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_vars_reshape</span> <span class="k">else</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">model_dims</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">CausalDepthwiseConv1DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="n">model_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.DepthwiseConvAutoregressive"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.DepthwiseConvAutoregressive">[docs]</a>  <span class="k">def</span> <span class="nf">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Depthwise convolution for autoregressive models.</span>

<span class="sd">    Same implementation as mesh_tensorflow/</span>
<span class="sd">    transformer/transformer.sublayer_depthwise_conv_autoregressive</span>

<span class="sd">    Given an input x of shape [B, L, M] and kernel_size K, there are K variables</span>
<span class="sd">    W[k] each with shape [M] so they represent the conv kernel [K, M].</span>

<span class="sd">    The output</span>
<span class="sd">      Y[:, t, :] = \sum_k W[k] * X[:, t - k, :]</span>
<span class="sd">      Y = W[0] * X + W[1] * Shift(X, 1) + W[2] * Shift(X, 2), ...</span>
<span class="sd">      where Shift(X, d) function rolls X forward in the time dimension by d.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Name of the layer.</span>
<span class="sd">      kernel_size: an integer.</span>
<span class="sd">      model_dims: Overridden model dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A layer params that computes DepthwiseConvAutoregressive.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_fused_depthwise_conv_autoregressive</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">CausalDepthwiseConv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="p">)</span>

    <span class="n">var_shape</span> <span class="o">=</span> <span class="n">model_dims</span> <span class="ow">or</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_GetScaleVar</span><span class="p">(</span><span class="n">shift_distance</span><span class="p">):</span>
      <span class="n">init_const</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="k">if</span> <span class="n">shift_distance</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">kernel_size</span>
      <span class="n">scale_var_weight_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">init_const</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">var_shape</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_vars_reshape</span> <span class="k">else</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)])</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">shift_distance</span><span class="p">,</span>
          <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale_var_weight_params</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">_Shift</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Shift x to right by 1 in time dim and pad with zeros.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]),</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Y = W[0] * X + W[1] * Shift(X, 1) + W[2] * Shift(X, 2) + ...</span>
    <span class="c1"># Iteratively:</span>
    <span class="c1"># Y_1 = W[0] * X_0</span>
    <span class="c1"># Y_2 = Y_1 + W[1] * X_1 = Y_1 + W[1] * _Shift(X_0)</span>
    <span class="c1"># ...</span>
    <span class="c1"># Y_{d+1} = Y_d + W[d] * X_d = Y_d + W[d] * _Shift(X_{d-1})</span>
    <span class="n">sub_params</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;-&gt;w_0&#39;</span><span class="p">,</span> <span class="n">_GetScaleVar</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span>
                  <span class="p">(</span><span class="s1">&#39;x_0,w_0-&gt;y_1&#39;</span><span class="p">,</span>
                   <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;mul0&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">)))]</span>

    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
      <span class="n">sub_params</span> <span class="o">+=</span> <span class="p">[</span>
          <span class="p">(</span><span class="s1">&#39;x_</span><span class="si">%d</span><span class="s1">-&gt;x_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;shift_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">d</span><span class="p">,</span> <span class="n">_Shift</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;-&gt;w_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">d</span><span class="p">,</span> <span class="n">_GetScaleVar</span><span class="p">(</span><span class="n">d</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;y_</span><span class="si">%d</span><span class="s1">,x_</span><span class="si">%d</span><span class="s1">,w_</span><span class="si">%d</span><span class="s1">-&gt;y_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;scale_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">d</span><span class="p">,</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">)))</span>
      <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x_0&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;y_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">kernel_size</span><span class="p">],</span> <span class="o">*</span><span class="n">sub_params</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._LNNoScale"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._LNNoScale">[docs]</a>  <span class="k">def</span> <span class="nf">_LNNoScale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">_RmsNormNoScale</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_norm_epsilon</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
      <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_RmsNormNoScale</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._LN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._LN">[docs]</a>  <span class="k">def</span> <span class="nf">_LN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overriding with bias-less layer norm.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ln_no_scale</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNNoScale</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNInternal</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._LNConv"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._LNConv">[docs]</a>  <span class="k">def</span> <span class="nf">_LNConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Seq</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNNoScale</span><span class="p">(</span><span class="s1">&#39;ln_no_scale&#39;</span><span class="p">),</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder._LNInternal"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._LNInternal">[docs]</a>  <span class="k">def</span> <span class="nf">_LNInternal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Internal implementation of _LN with optional reshape of the weight.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">LN</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_norm_epsilon</span>
      <span class="c1"># BLm Tensor (m=1, reduced model_dim) or BLnm where model dim is split to</span>
      <span class="c1"># two dims.</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
      <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ln_weight_reshape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="n">ln_weight_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;x_norm&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;scale&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">ln_weight_params</span><span class="p">)])),</span>
        <span class="p">(</span><span class="s1">&#39;x,scale-&gt;x_norm&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">LN</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder._TrueLN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._TrueLN">[docs]</a>  <span class="k">def</span> <span class="nf">_TrueLN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True LN normalization.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">LN</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_norm_epsilon</span>
      <span class="c1"># BLm Tensor (m=1, reduced model_dim) or BLnm where model dim is split to</span>
      <span class="c1"># two dims.</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
      <span class="n">squared_mean_center</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span>
          <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)))</span>
      <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">squared_mean_center</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ln_weight_reshape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">shift</span>

    <span class="n">ln_scale_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>
    <span class="n">ln_shift_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;x_norm&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;scale&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_scale&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">ln_scale_params</span><span class="p">)])),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;shift&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_shift&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="n">ln_shift_params</span><span class="p">)])),</span>
        <span class="p">(</span><span class="s1">&#39;x,scale,shift-&gt;x_norm&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;true_ln&#39;</span><span class="p">,</span> <span class="n">LN</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder._PN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._PN">[docs]</a>  <span class="k">def</span> <span class="nf">_PN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Primer normalization.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">PN</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">layer_norm_epsilon</span>
      <span class="c1"># BLm Tensor (m=1, reduced model_dim) or BLnm where model dim is split to</span>
      <span class="c1"># two dims.</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
      <span class="n">temp</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span>
      <span class="n">mock_variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ln_weight_reshape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">mock_variance</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">shift</span>

    <span class="n">pn_scale_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>
    <span class="n">pn_shift_params</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;x_norm&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;scale&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_scale&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">pn_scale_params</span><span class="p">)])),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;shift&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_shift&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="n">pn_shift_params</span><span class="p">)])),</span>
        <span class="p">(</span><span class="s1">&#39;x,scale,shift-&gt;x_norm&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;pn&#39;</span><span class="p">,</span> <span class="n">PN</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder.Split"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.Split">[docs]</a>  <span class="k">def</span> <span class="nf">Split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets sharding attribute for the Tensor. Split across dim=0.&quot;&quot;&quot;</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;gshard_utils.Split is deprecated. &#39;</span>
                       <span class="s1">&#39;Please use gshard_utils.MeshSplit with specific &#39;</span>
                       <span class="s1">&#39;device_mesh and device_mesh_shape set in the Builder.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_devices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_devices</span><span class="p">))</span></div>

<div class="viewcode-block" id="MoEBuilder._Add"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Add">[docs]</a>  <span class="k">def</span> <span class="nf">_Add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">residual_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">residual_weight</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">fn_out</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._Identity"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Identity">[docs]</a>  <span class="k">def</span> <span class="nf">_Identity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply identity transformation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">IdentityLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._AttentionWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._AttentionWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_AttentionWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">name</span><span class="p">,</span>
                        <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">w_qkv_mhd_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">wo_hdm_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper for &#39;-&gt;wq,wk,wv,wo&#39; Graph edge.&quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
    <span class="n">hd_dims</span> <span class="o">=</span> <span class="p">([</span><span class="n">h</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span>
               <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span> <span class="k">else</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
    <span class="n">kv_hd_dims</span> <span class="o">=</span> <span class="p">([</span><span class="n">h</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span>
                  <span class="k">else</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
    <span class="n">q_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">w_qkv_mhd_mesh_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_qkv_mesh_split</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># hd can not be both sharded. Use negative indices in case there is an</span>
        <span class="c1"># additional leading pipeline stage dimension.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">w_qkv_mhd_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span>
            <span class="s1">&#39;hd can not be both sharded </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">)</span>
        <span class="n">w_qkv_mesh_split</span> <span class="o">=</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
            <span class="n">w_qkv_mhd_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span>
            <span class="nb">max</span><span class="p">(</span><span class="n">w_qkv_mhd_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_qkv_mhd_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="p">]</span>

      <span class="k">if</span> <span class="n">wo_hdm_mesh_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wo_mesh_split</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># wo_hdm_mesh_split is almost always set via</span>
        <span class="c1"># DenseBuilder._attention_output_hdm_w_split, e.g.</span>
        <span class="c1">#   [p.mhd_w_split[-2], p.mhd_w_split[-1], p.mhd_w_split[-3]]</span>
        <span class="c1">#</span>
        <span class="c1"># TODO(lepikhin): this logic needs to be explicit, e.g. via</span>
        <span class="c1"># SetSplitsForCombinedAttentionDims utility function.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">wo_hdm_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span>
                <span class="n">wo_hdm_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;hd can not be both sharded </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                                             <span class="n">wo_hdm_mesh_split</span><span class="p">)</span>
        <span class="n">wo_mesh_split</span> <span class="o">=</span> <span class="n">wo_hdm_mesh_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
            <span class="nb">max</span><span class="p">(</span><span class="n">wo_hdm_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">wo_hdm_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span>
            <span class="n">wo_hdm_mesh_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">w_qkv_mesh_split</span> <span class="o">=</span> <span class="n">w_qkv_mhd_mesh_split</span>
      <span class="n">wo_mesh_split</span> <span class="o">=</span> <span class="n">wo_hdm_mesh_split</span>

    <span class="n">wq_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">hd_dims</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">q_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">w_qkv_mesh_split</span><span class="p">)</span>
    <span class="n">kv_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">wkv_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">kv_hd_dims</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">kv_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">w_qkv_mesh_split</span><span class="p">)</span>
    <span class="n">o_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">wo_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">hd_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">o_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">wo_mesh_split</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wq&#39;</span><span class="p">,</span> <span class="n">wq_tpl</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;wk&#39;</span><span class="p">,</span> <span class="n">wkv_tpl</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;wv&#39;</span><span class="p">,</span> <span class="n">wkv_tpl</span><span class="p">),</span>
                 <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="n">wo_tpl</span><span class="p">)],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ComputeQKV"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ComputeQKV">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ComputeQKVCombine"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ComputeQKVCombine">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKVCombine</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_GetW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
          <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">w</span>

      <span class="n">wq</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wq</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wk</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wk</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wv</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="p">[</span><span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">]</span>

      <span class="k">if</span> <span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">and</span>
           <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span><span class="p">)</span> <span class="ow">or</span>
          <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_qkv</span><span class="p">):</span>
        <span class="c1"># Combined tf.einsum is not possible, falling back to individual</span>
        <span class="c1"># einsum ops.</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wc</span><span class="p">]</span>

      <span class="n">wc</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wc</span><span class="p">]</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">wc</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">[</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,KMHD-&gt;KBLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._Top2GatingWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Top2GatingWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_Top2GatingWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">init_scale</span> <span class="o">=</span> <span class="n">stddev</span> <span class="o">*</span> <span class="mf">3.</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Var</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span>
                  <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">],</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">init_scale</span><span class="p">),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">))])</span></div>

<div class="viewcode-block" id="MoEBuilder._ComputeGating"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ComputeGating">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeGating</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ComputeGating</span><span class="p">(</span>
          <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="p">,</span>
          <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
          <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
          <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
          <span class="n">experts_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span>
          <span class="n">expert_capacity_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">c_dim</span><span class="p">,</span>
          <span class="n">model_dim_reshape_segments</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">,</span>
          <span class="n">local_dispatch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">fprop_dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
          <span class="n">mask_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">mask_dtype</span><span class="p">,</span>
          <span class="n">gating_logits_dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">gating_logits_dtype</span><span class="p">,</span>
          <span class="c1"># We rely on sharding propagation here, Top2Gating is done</span>
          <span class="c1"># independently for each group and inputs are typically sharded by</span>
          <span class="c1"># group dimension.</span>
          <span class="n">gating_func</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">gating_func</span><span class="p">,</span>
          <span class="n">use_xla_sharding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">second_expert_policy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">second_expert_policy</span><span class="p">,</span>
          <span class="n">second_expert_threshold</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">second_expert_threshold</span><span class="p">,</span>
          <span class="n">legacy_mtf_behavior</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">legacy_mtf_behavior</span><span class="p">,</span>
          <span class="n">capacity_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">capacity_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ShardedFeedForwardNetworksWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ShardedFeedForwardNetworksWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedFeedForwardNetworksWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the sharded weights for the two layer feedforward nets.&quot;&quot;&quot;</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;Deprecated. DenseBuilder should universally be used for MoE, &#39;</span>
        <span class="s1">&#39;Dense and Hybrid models.&#39;</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">emh_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">]</span>
    <span class="c1"># See VarianceScalingInitializer in py_utils</span>
    <span class="c1">#   scale        ~ 1.0</span>
    <span class="c1">#   reduced_dims ~ params.input_dim</span>
    <span class="c1">#   mode         ~ &#39;fan_in&#39;</span>
    <span class="c1">#</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wi_kernel_param_init_scale</span> <span class="o">=</span> <span class="n">stddev</span> <span class="o">*</span> <span class="mf">3.</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wi_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">emh_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">wi_kernel_param_init_scale</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># EHM Tensor (output transformation after RELU)</span>
    <span class="n">ehm_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span>
    <span class="c1"># See VarianceScalingInitializer in py_utils</span>
    <span class="c1">#   scale        ~ 1.0</span>
    <span class="c1">#   reduced_dims ~ params.moe_hidden_dim</span>
    <span class="c1">#   mode         ~ &#39;fan_in&#39;</span>
    <span class="c1">#</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wo_kernel_param_init_scale</span> <span class="o">=</span> <span class="n">stddev</span> <span class="o">*</span> <span class="mf">3.</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wo_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">ehm_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">wo_kernel_param_init_scale</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVarOn1DDeviceArray</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="n">wi_pc</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="n">wo_pc</span><span class="p">)])</span></div>

<div class="viewcode-block" id="MoEBuilder._FeedForwardNetworksApplyGating"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._FeedForwardNetworksApplyGating">[docs]</a>  <span class="k">def</span> <span class="nf">_FeedForwardNetworksApplyGating</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Split API is deprecated. &#39;</span>
                         <span class="s1">&#39;Use device_mesh and MeshSplit.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">gating</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">reshaped_inputs</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">wo</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">FeedForwardNetworksApplyGating</span><span class="p">(</span>
          <span class="n">gating</span><span class="p">,</span>
          <span class="n">inputs</span><span class="p">,</span>
          <span class="n">reshaped_inputs</span><span class="p">,</span>
          <span class="n">wi</span><span class="p">,</span>
          <span class="n">wo</span><span class="p">,</span>
          <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
          <span class="n">num_groups</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_groups</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">moe_dropout_rate</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
          <span class="n">model_dim_reshape_segments</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">,</span>
          <span class="n">gsm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;blm_split&#39;</span><span class="p">),</span>
          <span class="n">egcm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;egcm_split&#39;</span><span class="p">),</span>
          <span class="n">gecm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gecm_split&#39;</span><span class="p">),</span>
          <span class="n">gsec_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gsec_split&#39;</span><span class="p">),</span>
          <span class="n">gecs_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gecs_split&#39;</span><span class="p">),</span>
          <span class="n">gec_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gec_split&#39;</span><span class="p">),</span>
          <span class="n">eah_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;eah_split&#39;</span><span class="p">),</span>
          <span class="n">eam_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;eam_split&#39;</span><span class="p">),</span>
          <span class="n">gating_func</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">gating_func</span><span class="p">,</span>
          <span class="n">activation_name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">moe_activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._ShardedMoEPositionWiseFeedForwardNetworks"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._ShardedMoEPositionWiseFeedForwardNetworks">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedMoEPositionWiseFeedForwardNetworks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple MoE FFN with xla_sharding.&quot;&quot;&quot;</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;Deprecated. DenseBuilder should universally be used for MoE, &#39;</span>
        <span class="s1">&#39;Dense and Hybrid models.&#39;</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_groups</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span>

    <span class="n">reshape_input</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ReshapeInputLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span> <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;inputs,segment_id-&gt;reshaped_inputs, paddings&#39;</span><span class="p">,</span> <span class="n">reshape_input</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;gw&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Top2GatingWeights</span><span class="p">(</span><span class="s1">&#39;top_2_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gw,reshaped_inputs,paddings-&gt;gating&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeGating</span><span class="p">(</span><span class="s1">&#39;compute_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gating,inputs,reshaped_inputs,wi,wo-&gt;outputs,aux_loss&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_FeedForwardNetworksApplyGating</span><span class="p">(</span><span class="s1">&#39;process_gating&#39;</span><span class="p">)))</span></div>

<div class="viewcode-block" id="MoEBuilder._AttentionState"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._AttentionState">[docs]</a>  <span class="k">def</span> <span class="nf">_AttentionState</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="ow">or</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">MultiHeadAttentionStateLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">use_xla_dynamic_update_slice</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_xla_dynamic_update_slice</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._Override"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Override">[docs]</a>  <span class="k">def</span> <span class="nf">_Override</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">OverrideLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span> <span class="ow">or</span> <span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder._Softmax"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder._Softmax">[docs]</a>  <span class="k">def</span> <span class="nf">_Softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dec_outs</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_MaybeSplit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">)</span>

    <span class="n">dec_outs</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,VM-&gt;BLV&#39;</span><span class="p">,</span> <span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">dec_outs</span><span class="p">),</span> <span class="n">w</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">label_smoothing</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">label_smoothing</span>
    <span class="n">off_value</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">vocab_dim</span>
    <span class="n">on_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">label_smoothing</span> <span class="o">+</span> <span class="n">off_value</span>
    <span class="n">soft_targets</span> <span class="o">=</span> <span class="n">_MaybeSplit</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
            <span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="n">on_value</span><span class="o">=</span><span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="o">=</span><span class="n">off_value</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_MaybeSplit</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">soft_targets</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>
    <span class="n">non_padding</span> <span class="o">=</span> <span class="n">_MaybeSplit</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)))</span>
    <span class="n">per_token_loss</span> <span class="o">=</span> <span class="n">_MaybeSplit</span><span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">)</span>
    <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">non_padding</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">per_example_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_token_loss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">loss_denom</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoEBuilder.SmoothedSoftmax"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.MoEBuilder.SmoothedSoftmax">[docs]</a>  <span class="k">def</span> <span class="nf">SmoothedSoftmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Softmax layer with optional label smoothing.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span> <span class="p">(</span><span class="s1">&#39;-&gt;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EmbeddingWeight</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.dec_outs,i.tgt,w-&gt;o&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
             <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
             <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">))</span>
        <span class="p">))</span></div></div>


<div class="viewcode-block" id="DenseBuilder"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder">[docs]</a><span class="k">class</span> <span class="nc">DenseBuilder</span><span class="p">(</span><span class="n">MoEBuilder</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Desnse layrs with GShard annotations.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="DenseBuilder.Params"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="c1"># p.Delete(&#39;e_dim&#39;)</span>
    <span class="c1"># p.Delete(&#39;c_dim&#39;)</span>
    <span class="c1"># p.Delete(&#39;moe_hidden_dim&#39;)</span>
    <span class="c1"># p.Delete(&#39;second_expert_policy&#39;)</span>
    <span class="c1"># p.Delete(&#39;second_expert_threshold&#39;)</span>
    <span class="c1"># p.Delete(&#39;capacity_factor&#39;)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;device_mesh_shape&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Device mesh shape.&#39;</span><span class="p">)</span>

    <span class="c1"># Weight sharding configs.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;emb_w_split&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mesh split for embedding weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mhd_w_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for attention MHD weight&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;kv_mhd_w_split&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mesh split for K/V MHD weight&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mh_wi_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for dense MH weight&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hm_wo_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;Mesh split for dense HM weight&#39;</span><span class="p">)</span>

    <span class="c1"># Activation sharding configs.</span>
    <span class="c1"># one_hot_ids_split split should be inferred by XLA.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;one_hot_ids_split&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Mesh split for one_hot_ids.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;emb_out_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for embedding outputs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;qkv_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for Q, K, V&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;blm_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for BLM.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;blh_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for BLH.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;egcm_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for EGCM.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gecm_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for GECM.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gsec_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for GSEC.&#39;</span><span class="p">)</span>
    <span class="c1"># dispatch_tensor_split for token shuffle gating.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gecs_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for GECS.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gec_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for GEC.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;eah_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for EAH.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;eam_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for EAM.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;emh_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for EMH.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ehm_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for EHM.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;logits_split&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Mesh split for logits.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;experimental_fix_split_dims_mapping&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Mesh split dims mapping could require a fix for special cases.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;atten_logit_cap&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Atten logit cap.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="DenseBuilder._ReshapedModelDims"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ReshapedModelDims">[docs]</a>  <span class="k">def</span> <span class="nf">_ReshapedModelDims</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the dimensions that M is reshaped into.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span><span class="p">]</span>
    <span class="n">remaining_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_dim</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">remaining_dim</span> <span class="o">%</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="n">remaining_dim</span> <span class="o">=</span> <span class="n">remaining_dim</span> <span class="o">//</span> <span class="n">d</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="o">+</span> <span class="p">[</span><span class="n">remaining_dim</span><span class="p">]</span></div>

<div class="viewcode-block" id="DenseBuilder._ReshapeM"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ReshapeM">[docs]</a>  <span class="k">def</span> <span class="nf">_ReshapeM</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reshapes tensor x according to model_dim_reshape_segments.&quot;&quot;&quot;</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m_dim</span><span class="p">])</span>
      <span class="n">new_shape</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span>
      <span class="n">new_shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">m_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._EinsumWithModelDim"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._EinsumWithModelDim">[docs]</a>  <span class="k">def</span> <span class="nf">_EinsumWithModelDim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">equation</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Einsum with adjusted equation according to model_dim_reshape_segments.</span>

<span class="sd">    It changes each dimension named &#39;M&#39; in the equation into two dimensions &#39;NM&#39;</span>
<span class="sd">    if model_dim_reshape_segments is set in the params. Therefore the original</span>
<span class="sd">    equation should not have &#39;N&#39;, and only use &#39;M&#39; when it is expected to be</span>
<span class="sd">    reshaped.</span>

<span class="sd">    Args:</span>
<span class="sd">      equation: a string describing the contraction, in the same format as</span>
<span class="sd">        numpy.einsum.</span>
<span class="sd">      x: First input to einsum.</span>
<span class="sd">      y: second input to einsum.</span>

<span class="sd">    Returns:</span>
<span class="sd">      tf.einsum(maybe_modified_equation, x, y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DepthwiseConvAutoregressive"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DepthwiseConvAutoregressive">[docs]</a>  <span class="k">def</span> <span class="nf">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model_dims</span> <span class="o">=</span> <span class="n">model_dims</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.EinsumWithModelDim"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.EinsumWithModelDim">[docs]</a>  <span class="k">def</span> <span class="nf">EinsumWithModelDim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">equation</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">_Fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Fn</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._LN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._LN">[docs]</a>  <span class="k">def</span> <span class="nf">_LN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overriding _LN to consider model_dim_reshape_segments.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">ln_no_scale</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;attempting to call self._LNInternal &#39;</span>
                                         <span class="s1">&#39;instead of self._LNNoScale&#39;</span><span class="p">)</span>
    <span class="n">ln_weight_reshape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNInternal</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._PN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._PN">[docs]</a>  <span class="k">def</span> <span class="nf">_PN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overriding _PN to consider model_dim_reshape_segments.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_PN</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="n">pn_weight_reshape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_PN</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">pn_weight_reshape</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._TrueLN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._TrueLN">[docs]</a>  <span class="k">def</span> <span class="nf">_TrueLN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overriding _TrueLN to consider model_dim_reshape_segments.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_TrueLN</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="n">ln_weight_reshape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_TrueLN</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ln_weight_reshape</span><span class="p">)</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_device_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span>
    <span class="k">if</span> <span class="n">device_mesh</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
      <span class="n">num_devices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span><span class="p">)</span>
      <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span> <span class="o">==</span> <span class="nb">list</span><span class="p">(</span>
          <span class="n">device_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh_shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">device_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">device_mesh</span>

<div class="viewcode-block" id="DenseBuilder._MeshSplit"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._MeshSplit">[docs]</a>  <span class="k">def</span> <span class="nf">_MeshSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tensor_split_dims_mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">experimental_fix_split_dims_mapping</span><span class="p">:</span>
      <span class="c1"># TODO(lepikhin,yuanxz): fix me.</span>
      <span class="c1"># Required for split by attention head dim which could be 1 in special</span>
      <span class="c1"># cases.</span>
      <span class="n">tensor_split_dims_mapping</span> <span class="o">=</span> <span class="n">tensor_split_dims_mapping</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor_split_dims_mapping</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">continue</span>

        <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="n">dim</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Fixing bad tensor_split_dims_mapping </span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
                          <span class="n">tensor_split_dims_mapping</span><span class="p">)</span>
          <span class="n">tensor_split_dims_mapping</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
          <span class="k">continue</span>
        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">d</span> <span class="o">%</span> <span class="n">m</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                              <span class="n">tensor_split_dims_mapping</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                  <span class="n">tensor_split_dims_mapping</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.MeshSplit"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.MeshSplit">[docs]</a>  <span class="k">def</span> <span class="nf">MeshSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">))</span></div>

<div class="viewcode-block" id="DenseBuilder.MoE"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.MoE">[docs]</a>  <span class="k">def</span> <span class="nf">MoE</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns layer params to compute (outputs, scalar_aux_loss).&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">input_endpoints</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;vec-&gt;input_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;input_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id-&gt;segment_id_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;segment_id_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedFeedForwardNetworksWeights</span><span class="p">(</span><span class="n">name</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_split,segment_id_split,wi,wo-&gt;outputs_pre_split,aux_loss&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedMoEPositionWiseFeedForwardNetworks</span><span class="p">(</span><span class="s1">&#39;ffw&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span></div>

<div class="viewcode-block" id="DenseBuilder._ShardedFeedForwardNetworksWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ShardedFeedForwardNetworksWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedFeedForwardNetworksWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">model_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the sharded weights for the two layer feedforward nets.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">device_mesh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span>
    <span class="k">if</span> <span class="n">model_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">model_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span>
    <span class="n">emh_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">]</span>
    <span class="c1"># See VarianceScalingInitializer in py_utils</span>
    <span class="c1">#   scale        ~ 1.0</span>
    <span class="c1">#   reduced_dims ~ params.input_dim</span>
    <span class="c1">#   mode         ~ &#39;fan_in&#39;</span>
    <span class="c1">#</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wi_kernel_param_init_scale</span> <span class="o">=</span> <span class="n">stddev</span> <span class="o">*</span> <span class="mf">3.</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wi_pc</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">emh_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">wi_kernel_param_init_scale</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">emh_split</span><span class="p">)</span>

    <span class="c1"># EHM Tensor (output transformation after RELU)</span>
    <span class="n">ehm_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">,</span> <span class="n">model_dim</span><span class="p">]</span>
    <span class="c1"># See VarianceScalingInitializer in py_utils</span>
    <span class="c1">#   scale        ~ 1.0</span>
    <span class="c1">#   reduced_dims ~ params.moe_hidden_dim</span>
    <span class="c1">#   mode         ~ &#39;fan_in&#39;</span>
    <span class="c1">#</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wo_kernel_param_init_scale</span> <span class="o">=</span> <span class="n">stddev</span> <span class="o">*</span> <span class="mf">3.</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">wo_pc</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">ehm_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">wo_kernel_param_init_scale</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">ehm_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="n">wi_pc</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="n">wo_pc</span><span class="p">)],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._ShardedMoEPositionWiseFeedForwardNetworks"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ShardedMoEPositionWiseFeedForwardNetworks">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedMoEPositionWiseFeedForwardNetworks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple MoE FFN with xla_sharding.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_groups</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span>

    <span class="n">reshape_input</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ReshapeInputLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
        <span class="n">model_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">(),</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;inputs,segment_id-&gt;reshaped_inputs, paddings&#39;</span><span class="p">,</span> <span class="n">reshape_input</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;gw&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Top2GatingWeights</span><span class="p">(</span><span class="s1">&#39;top_2_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gw-&gt;gw_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_gw&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi-&gt;wi_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_wi&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wo-&gt;wo_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_wo&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;gw_reshaped,reshaped_inputs,paddings-&gt;gating&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeGating</span><span class="p">(</span><span class="s1">&#39;compute_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gating,inputs,reshaped_inputs,wi_reshaped,wo_reshaped&#39;</span>
         <span class="s1">&#39;-&gt;outputs,aux_loss&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_FeedForwardNetworksApplyGating</span><span class="p">(</span><span class="s1">&#39;process_gating&#39;</span><span class="p">)))</span></div>

<div class="viewcode-block" id="DenseBuilder.Embedding"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.Embedding">[docs]</a>  <span class="k">def</span> <span class="nf">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;emb_orig&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_EmbeddingWeight</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                               <span class="n">p</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;emb_orig-&gt;emb&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_emb_w&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;ids-&gt;one_hot_ids&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_OneHotEncode</span><span class="p">(</span><span class="s1">&#39;one_hot_ids&#39;</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;one_hot_ids-&gt;one_hot_ids_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;one_hot_ids_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">one_hot_ids_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;emb,one_hot_ids_split-&gt;outputs_pre_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;einsum&#39;</span><span class="p">,</span> <span class="s1">&#39;VM,BLV-&gt;BLM&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;out_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">emb_out_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_attention_output_hdm_w_split</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># wo: hdm</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="c1"># Use negative indices in case there is an additional pipeline stage</span>
    <span class="c1"># dimension.</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
        <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="p">]</span>

<div class="viewcode-block" id="DenseBuilder.SelfAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.SelfAttention">[docs]</a>  <span class="k">def</span> <span class="nf">SelfAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">SelfAttention</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DecEncAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DecEncAttention">[docs]</a>  <span class="k">def</span> <span class="nf">DecEncAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">DecEncAttention</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DecSelfAttention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DecSelfAttention">[docs]</a>  <span class="k">def</span> <span class="nf">DecSelfAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">DecSelfAttention</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.SelfAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.SelfAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">SelfAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">SelfAttentionRelativeBias</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DecSelfAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DecSelfAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">DecSelfAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">DecSelfAttentionRelativeBias</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DecMultiDconvHeadAttentionRelativeBias"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DecMultiDconvHeadAttentionRelativeBias">[docs]</a>  <span class="k">def</span> <span class="nf">DecMultiDconvHeadAttentionRelativeBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">DecMultiDconvHeadAttentionRelativeBias</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.ParallelDecSelfAttentionRelativeBiasFFN"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.ParallelDecSelfAttentionRelativeBiasFFN">[docs]</a>  <span class="k">def</span> <span class="nf">ParallelDecSelfAttentionRelativeBiasFFN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                              <span class="n">name</span><span class="p">,</span>
                                              <span class="n">activation_fn</span><span class="p">,</span>
                                              <span class="n">conv_kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                              <span class="n">hidden_dim_reshape_segments</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs DecSelfAttentionRelativeBias and FFNWithConv in parallel.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias_shared&#39;</span><span class="p">:</span>
      <span class="c1"># Collection name is used as a unique ID to retrieve the shared variable.</span>
      <span class="c1">#</span>
      <span class="c1"># This name must be different for SelfAttentionRelativeBias (Encoder), and</span>
      <span class="c1"># must have a suffix matching shared_var_collection_suffix, e.g.</span>
      <span class="c1"># &#39;shared_var&#39;.</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_dec_self_attention_shared_var&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span>

    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_Output</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">wo1</span><span class="p">,</span> <span class="n">wo2</span><span class="p">):</span>
      <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeCombinedOutputs</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">wo1</span><span class="p">,</span> <span class="n">wo2</span><span class="p">,</span>
                                          <span class="n">hidden_dim_reshape_segments</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">conv_kernel_size</span><span class="p">:</span>
      <span class="n">d</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
      <span class="n">model_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="n">model_dims</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>

    <span class="n">graph_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="n">graph_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">]</span>
    <span class="n">sub_layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wq,wk,wv,wo1&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionWeights</span><span class="p">(</span><span class="s1">&#39;w_atten&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi_0,wi_1,wo2&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_DenseReluDenseWeightsGatedGELU</span><span class="p">(</span><span class="s1">&#39;w_fflayer&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                              <span class="n">p</span><span class="o">.</span><span class="n">mh_wi_split</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hm_wo_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;relative_bias_weights&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;vec,wq,wk,wv,wi_0,wi_1-&gt;q,k,v,h1,h2&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVH</span><span class="p">(</span><span class="s1">&#39;qkvh&#39;</span><span class="p">,</span> <span class="n">hidden_dim_reshape_segments</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_pos-&gt;key_segment_pos&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;seg_pos&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_DecComputeBiasGraphEdge</span><span class="p">(),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;qk_bias&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="c1"># Decoder _AddRelativeBias always has bidirectional=False.</span>
        <span class="p">(</span><span class="s1">&#39;qk_bias,segment_pos,key_segment_pos,relative_bias_weights-&gt;qhk_bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;relative_bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AddRelativeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,qhk_bias-&gt;o&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h1-&gt;h1_act&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;h1_act&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h1_act-&gt;h1_conv&#39;</span><span class="p">,</span> <span class="n">optional_conv_layer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;o,h1_conv,h2,wo1,wo2-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_Output</span><span class="p">))</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">graph_inputs</span><span class="p">,</span> <span class="n">graph_outputs</span><span class="p">,</span> <span class="o">*</span><span class="n">sub_layers</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._CapLogits"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._CapLogits">[docs]</a>  <span class="k">def</span> <span class="nf">_CapLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;When enabled, cap logits by p.atten_logit_cap with tanh.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_logit_cap</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">atten_logit_cap</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">logits</span>
    <span class="n">cap</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">atten_logit_cap</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Note that since this caps the negative side as well, caller</span>
    <span class="c1"># must defer the pad-with-very-negative-logits logic to after</span>
    <span class="c1"># this function returns.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">cap</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">cap</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="DenseBuilder.Attention"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.Attention">[docs]</a>  <span class="k">def</span> <span class="nf">Attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Attention with multiple attention heads.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_AddBiasF32</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
      <span class="c1"># logits: BLHM [batch, length, heads, memory_length]</span>
      <span class="c1"># expecting logits.dtype == tf.float32</span>
      <span class="c1">#</span>
      <span class="c1"># bias: BLHM [batch, length, heads, memory_length]</span>
      <span class="c1">#       (in case of attention with relative bias) OR</span>
      <span class="c1">#</span>
      <span class="c1">#       BLM  [batch, length, memory_length]</span>
      <span class="c1">#       (default masking bias with very negative logits).</span>
      <span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Expanding the &#39;heads&#39; dimension</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">bias</span>
      <span class="k">return</span> <span class="n">retval</span>

    <span class="k">def</span> <span class="nf">_ReduceLogsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="n">max_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">extra_logit</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_extra_logit</span>
      <span class="k">if</span> <span class="n">extra_logit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">extra_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">extra_logit</span><span class="p">,</span> <span class="n">max_logit</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">max_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_logit</span><span class="p">,</span> <span class="n">extra_logit</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">-=</span> <span class="n">max_logit</span>
      <span class="n">exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">extra_logit</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sum_exp_x</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">extra_logit</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sum_exp_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">max_logit</span>

    <span class="k">def</span> <span class="nf">_LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">_ReduceLogsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_SoftmaxF32</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="c1"># expecting x.dtype == tf.float32</span>
      <span class="c1">#</span>
      <span class="c1"># TODO(lepikhin): consider</span>
      <span class="c1"># if p.attention_extra_logit is None:</span>
      <span class="c1">#   return tf.nn.softmax(x)</span>
      <span class="n">softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">_LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
      <span class="n">softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">softmax</span>

    <span class="c1"># p.attention_num_memory_heads == 1 special case is simple, we omit the</span>
    <span class="c1"># &quot;heads&quot; dimension H from the projection matrices wk and wv.</span>
    <span class="c1">#</span>
    <span class="c1"># Then remove the heads dimension H of wk, vv, k, or v in any einsum</span>
    <span class="c1"># formula in which it appears.</span>
    <span class="k">def</span> <span class="nf">_LogitsFnF32</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
      <span class="c1"># logits.dtype == tf.float32 leads to better training stability</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_logits_dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHD,BMD-&gt;BLHM&#39;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHD,BMHD-&gt;BLHM&#39;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;attention_logits&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CapLogits</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_OutputsFn</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHM,BMD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLHM,BMHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;attention_output&#39;</span><span class="p">)</span>

    <span class="n">kv_split</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;_q&#39;</span><span class="p">,</span> <span class="s1">&#39;_k&#39;</span><span class="p">,</span> <span class="s1">&#39;_v&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;_q-&gt;q&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_q&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;_k-&gt;k&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_k&#39;</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;_v-&gt;v&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_v&#39;</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k-&gt;l&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_LogitsFnF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;l,bias-&gt;logits&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_AddBiasF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;logits-&gt;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">_SoftmaxF32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;w-&gt;weights&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">attention_dropout_prob</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;weights-&gt;weights_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_wsplit&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;weights_split,v-&gt;outputs_unsplitted&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_OutputsFn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_unsplitted-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_o&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">)))</span></div>

<div class="viewcode-block" id="DenseBuilder._ComputeQKV"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ComputeQKV">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">)</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._ComputeQKVCombine"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ComputeQKVCombine">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKVCombine</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_GetW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
          <span class="n">combined_split</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
          <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">combined_split</span><span class="p">)</span>
          <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>
        <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

      <span class="n">wq</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wq</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wk</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wk</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wv</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="p">[</span><span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">]</span>

      <span class="k">if</span> <span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">and</span>
           <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span><span class="p">)</span> <span class="ow">or</span>
          <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_qkv</span><span class="p">):</span>
        <span class="c1"># Combined tf.einsum is not possible, falling back to individual</span>
        <span class="c1"># einsum ops.</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wc</span><span class="p">]</span>

      <span class="n">wc</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="p">[</span><span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">]]</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">wc</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="p">[</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,KMHD-&gt;KBLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._ComputeQKVH"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ComputeQKVH">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKVH</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">hidden_dim_reshape_segments</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">,</span> <span class="n">wi0</span><span class="p">,</span> <span class="n">wi1</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_GetW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
              <span class="n">w</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span> <span class="o">+</span>
              <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">])</span>

      <span class="n">wq</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wq</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wk</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wk</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wv</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">)</span>
      <span class="n">wi0</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wi0</span><span class="p">)</span>
      <span class="n">wi1</span> <span class="o">=</span> <span class="n">_GetW</span><span class="p">(</span><span class="n">wi1</span><span class="p">)</span>

      <span class="n">wc</span> <span class="o">=</span> <span class="p">[</span><span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">,</span> <span class="n">wi0</span><span class="p">,</span> <span class="n">wi1</span><span class="p">]</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">and</span>
          <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span><span class="p">):</span>
        <span class="c1"># Combined tf.einsum is not possible, falling back to individual</span>
        <span class="c1"># einsum ops.</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wk</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wv</span><span class="p">)</span>
        <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">wq</span><span class="p">,</span> <span class="n">wi0</span><span class="p">,</span> <span class="n">wi1</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">wq</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">wi0</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MSHD-&gt;BLSHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span>
            <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">splits</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="p">,</span>
                       <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">)</span>

        <span class="c1"># Only one head so there is nothing to shard the H in BLHD k,v tensors.</span>
        <span class="n">kv_split</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">wq</span><span class="p">,</span> <span class="n">wk</span><span class="p">,</span> <span class="n">wv</span><span class="p">,</span> <span class="n">wi0</span><span class="p">,</span> <span class="n">wi1</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">wq</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="n">wi0</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">2</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MSHD-&gt;BLSHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span> <span class="n">splits</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">_MeshSplitQKV</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="k">return</span> <span class="p">[</span><span class="n">_MeshSplitQKV</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span> <span class="o">+</span> <span class="n">ret</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._ComputeAttenOutputs"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ComputeAttenOutputs">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeAttenOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">wo</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">hdm_split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_output_hdm_w_split</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_combine_dims</span><span class="p">:</span>
      <span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">wo</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">])</span>
    <span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">wo</span><span class="p">,</span> <span class="n">hdm_split</span><span class="p">)</span>
    <span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">wo</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;HDM,BLHD-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">o</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span></div>

<div class="viewcode-block" id="DenseBuilder._ComputeCombinedOutputs"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ComputeCombinedOutputs">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeCombinedOutputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                              <span class="n">o1</span><span class="p">,</span>
                              <span class="n">o2</span><span class="p">,</span>
                              <span class="n">wo1</span><span class="p">,</span>
                              <span class="n">wo2</span><span class="p">,</span>
                              <span class="n">hidden_dim_reshape_segments</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">split_h_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span>

    <span class="n">wo1</span><span class="p">,</span> <span class="n">wo2</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">split_h_shape</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="p">[</span><span class="n">wo1</span><span class="p">,</span> <span class="n">wo2</span><span class="p">]</span>
    <span class="p">]</span>

    <span class="n">o1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o1</span><span class="p">,</span> <span class="n">o1</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">split_h_shape</span><span class="p">)</span>
    <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">o1</span><span class="p">,</span> <span class="n">o2</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">wo1</span><span class="p">,</span> <span class="n">wo2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;SHDM,BLSHD-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.0</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DenseReluDense"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DenseReluDense">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDense</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>

    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;gelu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;sqr_relu&#39;</span><span class="p">:</span>
      <span class="n">activation_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Activation </span><span class="si">%s</span><span class="s1"> not supported.&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Note that dropout is used here, but not in the MoE layer by default.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">input_endpoints</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi,wo&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_DenseReluDenseWeights</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">mh_wi_split</span><span class="p">,</span>
                                     <span class="n">p</span><span class="o">.</span><span class="n">hm_wo_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wi-&gt;wi_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi_reshape&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wo-&gt;wo_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wo_reshape&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi_reshaped,vec-&gt;h&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="s1">&#39;MH,BLM-&gt;BLH&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h-&gt;h_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_h_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h_split-&gt;h_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h_</span><span class="si">%s</span><span class="s1">-&gt;h_dropout&#39;</span> <span class="o">%</span> <span class="n">activation</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wo_reshaped,h_dropout-&gt;outputs_pre_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="s1">&#39;HM,BLH-&gt;BLM&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder.DenseReluDenseGated"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.DenseReluDenseGated">[docs]</a>  <span class="k">def</span> <span class="nf">DenseReluDenseGated</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>

    <span class="k">def</span> <span class="nf">_Impl</span><span class="p">(</span><span class="n">wi_0</span><span class="p">,</span> <span class="n">wi_1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="n">wi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">wi_0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">wi_1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">o1</span><span class="p">,</span> <span class="n">o2</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;KMH,BLM-&gt;KBLH&#39;</span><span class="p">,</span> <span class="n">wi</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">]</span>
      <span class="c1"># To match historic behavior use approximate=True with tf.nn.gelu</span>
      <span class="c1"># activation.</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">activation_fn</span><span class="p">(</span><span class="n">o1</span><span class="p">),</span> <span class="n">o2</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">input_endpoints</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi_0,wi_1,wo&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_DenseReluDenseWeightsGatedGELU</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
                                              <span class="n">p</span><span class="o">.</span><span class="n">mh_wi_split</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hm_wo_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wi_0-&gt;wi_0_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi0_reshape&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi_1-&gt;wi_1_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi1_reshape&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wo-&gt;wo_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wo_reshape&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi_0_reshaped,wi_1_reshaped,vec-&gt;h&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;wi&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_Impl</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h-&gt;h_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;_h_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h_split-&gt;h_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;wo_reshaped,h_dropout-&gt;outputs_pre_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="s1">&#39;HM,BLH-&gt;BLM&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zero_aux_loss</span><span class="p">(</span><span class="s1">&#39;aux_loss&#39;</span><span class="p">)),</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._MoEWeightsGatedGELU"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._MoEWeightsGatedGELU">[docs]</a>  <span class="k">def</span> <span class="nf">_MoEWeightsGatedGELU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">name</span><span class="p">,</span>
                           <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">wi_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">wo_mesh_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Gated GELU.  There are two separate linear transformations applied in</span>
    <span class="c1"># parallel to the inputs.  You take the gelu of one of them and then</span>
    <span class="c1"># multiply the two componentwise.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;wi_0&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">emh_split</span><span class="p">)),</span>
                 <span class="p">(</span><span class="s1">&#39;wi_1&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">emh_split</span><span class="p">)),</span>
                 <span class="p">(</span><span class="s1">&#39;wo&#39;</span><span class="p">,</span>
                  <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
                      <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
                          <span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)),</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">e_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_hidden_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
                      <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">ehm_split</span><span class="p">))],</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="DenseBuilder._ShardedMoEGLU"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder._ShardedMoEGLU">[docs]</a>  <span class="k">def</span> <span class="nf">_ShardedMoEGLU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple MoE GLU with xla_sharding.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_groups</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_groups</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span>

    <span class="n">reshape_input</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ReshapeInputLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
        <span class="n">model_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_GLUApplyGating</span><span class="p">(</span><span class="n">gating</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">reshaped_inputs</span><span class="p">,</span> <span class="n">wi_0</span><span class="p">,</span> <span class="n">wi_1</span><span class="p">,</span> <span class="n">wo</span><span class="p">):</span>
      <span class="n">wi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">wi_0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">wi_1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">FeedForwardNetworksApplyGating</span><span class="p">(</span>
          <span class="n">gating</span><span class="p">,</span>
          <span class="n">inputs</span><span class="p">,</span>
          <span class="n">reshaped_inputs</span><span class="p">,</span>
          <span class="n">wi</span><span class="p">,</span>
          <span class="n">wo</span><span class="p">,</span>
          <span class="n">num_devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
          <span class="n">num_groups</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_groups</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span>
          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">moe_dropout_rate</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">,</span>
          <span class="n">model_dim_reshape_segments</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_dim_reshape_segments</span><span class="p">,</span>
          <span class="n">gsm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;blm_split&#39;</span><span class="p">),</span>
          <span class="n">egcm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;egcm_split&#39;</span><span class="p">),</span>
          <span class="n">gecm_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gecm_split&#39;</span><span class="p">),</span>
          <span class="n">gsec_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gsec_split&#39;</span><span class="p">),</span>
          <span class="n">gecs_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gecs_split&#39;</span><span class="p">),</span>
          <span class="n">gec_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;gec_split&#39;</span><span class="p">),</span>
          <span class="n">eah_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;eah_split&#39;</span><span class="p">),</span>
          <span class="n">eam_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplitByName</span><span class="p">(</span><span class="s1">&#39;eam_split&#39;</span><span class="p">),</span>
          <span class="n">use_glu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">activation_name</span><span class="o">=</span><span class="s1">&#39;GELU_APPROXIMATE&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;wi_0&#39;</span><span class="p">,</span> <span class="s1">&#39;wi_1&#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;inputs,segment_id-&gt;reshaped_inputs, paddings&#39;</span><span class="p">,</span> <span class="n">reshape_input</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;gw&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Top2GatingWeights</span><span class="p">(</span><span class="s1">&#39;top_2_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gw-&gt;gw_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_gw&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi_0-&gt;wi0_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_wi0&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wi_1-&gt;wi1_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_wi1&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;wo-&gt;wo_reshaped&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;reshape_wo&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapeM</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;gw_reshaped,reshaped_inputs,paddings-&gt;gating&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeGating</span><span class="p">(</span><span class="s1">&#39;compute_gating&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;gating,inputs,reshaped_inputs,wi0_reshaped,wi1_reshaped,wo_reshaped&#39;</span>
         <span class="s1">&#39;-&gt;outputs,aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;process_gating&#39;</span><span class="p">,</span> <span class="n">_GLUApplyGating</span><span class="p">)))</span></div>

<div class="viewcode-block" id="DenseBuilder.MoEGated"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.DenseBuilder.MoEGated">[docs]</a>  <span class="k">def</span> <span class="nf">MoEGated</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a MoE layer with GLU experts.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">decoder</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_endpoints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EncoderLayerInMapKeys</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">input_endpoints</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="s1">&#39;aux_loss&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;vec-&gt;input_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;input_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;segment_id-&gt;segment_id_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;segment_id_split&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;wi_0,wi_1,wo&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MoEWeightsGatedGELU</span><span class="p">(</span><span class="n">name</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_split,segment_id_split,wi_0,wi_1,wo&#39;</span>
         <span class="s1">&#39;-&gt;outputs_pre_split,aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedMoEGLU</span><span class="p">(</span><span class="s1">&#39;ffw&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;outputs_pre_split-&gt;outputs&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s1">&#39;outputs_split&#39;</span><span class="p">)))</span></div></div>


<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode">[docs]</a><span class="k">class</span> <span class="nc">RecurrentDenseBuilderParallelDecode</span><span class="p">(</span><span class="n">DenseBuilder</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Same as RecurrentDenseBuilder but with micro variables.</span>

<span class="sd">  Projection variables created under this builder will be replaced by multiple</span>
<span class="sd">  micro variables, each with shape [model_dim, proj_weight_hdim, d_kv].</span>
<span class="sd">  For example, the original wi_o with shape [1024, 4096] was replaced with</span>
<span class="sd">  8 micro variables each with  shape [1024, 4, 128] when proj_weight_hdim = 4</span>
<span class="sd">  and d_kv = 128.</span>

<span class="sd">  Using smaller micro variables reduced the total size of variables created by</span>
<span class="sd">  RepeatLayer. For example, when the number of layers is 120, model_dim = 16k,</span>
<span class="sd">  ff_dim = 64k, d_kv = 128, then RepeatLayer will create variables with size</span>
<span class="sd">  120 * 16k * 64k * 4Bytes = 491GB, exceeding the host memoery limit.</span>
<span class="sd">  By setting proj_weight_hdim = 64, RepeatLayer will create 8x micro variables,</span>
<span class="sd">  each with shape [L, 16k, 64, 128] only. Using leasting loading placer those</span>
<span class="sd">  micro variables will be placed at different hosts.</span>

<span class="sd">  To disable micro variables, set proj_weight_hdim to None.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode.Params"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;proj_weight_hdim&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s1">&#39;weight hd_dims = [proj_weight_hdim, attention_key_value_dim]. &#39;</span>
        <span class="s1">&#39;Use None to disable micro variables.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_num_input_proj_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="n">num_wq</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>
    <span class="n">kv_h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
    <span class="k">if</span> <span class="n">kv_h</span> <span class="o">&lt;</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span><span class="p">:</span>
      <span class="n">num_wkv</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">num_wkv</span> <span class="o">=</span> <span class="n">kv_h</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>
    <span class="n">num_wi</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>
    <span class="k">return</span> <span class="n">num_wq</span><span class="p">,</span> <span class="n">num_wkv</span><span class="p">,</span> <span class="n">num_wi</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_num_output_proj_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="n">num_atten</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>
    <span class="n">num_wo</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>
    <span class="k">return</span> <span class="n">num_atten</span><span class="p">,</span> <span class="n">num_wo</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_ffw_var_h_dim_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_atten_q_var_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_atten_kv_var_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">kv_h</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">kv_h</span> <span class="o">&lt;</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">kv_h</span>
    <span class="k">assert</span> <span class="n">kv_h</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">proj_weight_hdim</span>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode._DecoderLayerInputProjWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode._DecoderLayerInputProjWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_DecoderLayerInputProjWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="c1"># Create multiple input projection weights, each of which has shape</span>
    <span class="c1"># [model_dim, proj_weight_hdim, d_kv].</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">input_w_split</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">2</span><span class="p">])]</span>
    <span class="n">kv_w_split</span> <span class="o">=</span> <span class="n">input_w_split</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">kv_mhd_w_split</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span>
        <span class="n">p</span><span class="o">.</span><span class="n">kv_mhd_w_split</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">kv_mhd_w_split</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">kv_mhd_w_split</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="p">]</span>

    <span class="n">q_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">wq_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_atten_q_var_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
        <span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">q_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">input_w_split</span><span class="p">)</span>

    <span class="n">kv_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">wkv_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span>
            <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_atten_kv_var_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
        <span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">kv_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">kv_w_split</span><span class="p">)</span>

    <span class="n">ffw_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ffw_var_h_dim_size</span><span class="p">],</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">input_w_split</span><span class="p">)</span>

    <span class="n">num_wq</span><span class="p">,</span> <span class="n">num_wkv</span><span class="p">,</span> <span class="n">num_wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_input_proj_weights</span>

    <span class="n">weights_params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wq</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;wq_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">wq_tpl</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wkv</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;wk_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">wkv_tpl</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wkv</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;wv_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">wkv_tpl</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wi</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;wi0_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">ffw_tpl</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wi</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;wi1_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">ffw_tpl</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights_params</span><span class="p">,</span> <span class="n">device_mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode._DecoderLayerOutputProjWeights"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode._DecoderLayerOutputProjWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_DecoderLayerOutputProjWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">out_w_split</span> <span class="o">=</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">p</span><span class="o">.</span><span class="n">mhd_w_split</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

    <span class="n">atten_stddev</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
    <span class="n">atten_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_atten_q_var_heads</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span>
        <span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">atten_stddev</span><span class="p">),</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">out_w_split</span><span class="p">)</span>
    <span class="n">ffw_tpl</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">ShardedWeightParams</span><span class="p">(</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(((</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ffw_var_h_dim_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">model_dim</span><span class="p">],</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">out_w_split</span><span class="p">)</span>
    <span class="n">num_atten</span><span class="p">,</span> <span class="n">num_wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_output_proj_weights</span>
    <span class="n">weights_params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_atten</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;atten_wo_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">atten_tpl</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_wo</span><span class="p">):</span>
      <span class="n">weights_params</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;ffw_wo_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">ffw_tpl</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ShardedVar</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights_params</span><span class="p">,</span> <span class="n">device_mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_mesh</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode._ComputeQKVH"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode._ComputeQKVH">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeQKVH</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">hidden_dim_reshape_segments</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">d_kv</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>

    <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">ws</span><span class="p">):</span>
      <span class="n">num_wq</span><span class="p">,</span> <span class="n">num_wkv</span><span class="p">,</span> <span class="n">num_wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_input_proj_weights</span>

      <span class="k">def</span> <span class="nf">_ReshapeW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
              <span class="n">w</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">()</span> <span class="o">+</span>
              <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">d_kv</span><span class="p">])</span>

      <span class="n">wqs</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[:</span><span class="n">num_wq</span><span class="p">]</span>
      <span class="n">wks</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="n">num_wq</span><span class="p">:</span><span class="n">num_wq</span> <span class="o">+</span> <span class="n">num_wkv</span><span class="p">]</span>
      <span class="n">wvs</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="n">num_wq</span> <span class="o">+</span> <span class="n">num_wkv</span><span class="p">:</span><span class="n">num_wq</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_wkv</span><span class="p">]</span>
      <span class="n">wi0s</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="n">num_wq</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_wkv</span><span class="p">:</span><span class="n">num_wq</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_wkv</span> <span class="o">+</span> <span class="n">num_wi</span><span class="p">]</span>
      <span class="n">wi1s</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="n">num_wq</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_wkv</span> <span class="o">+</span> <span class="n">num_wi</span><span class="p">:]</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">and</span>
          <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span><span class="p">):</span>
        <span class="c1"># Combined tf.einsum is not possible, falling back to individual</span>
        <span class="c1"># einsum ops.</span>
        <span class="n">h_kv</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span>
        <span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">_ReshapeW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h_kv</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wks</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">_ReshapeW</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h_kv</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wvs</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wk</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MHD-&gt;BLHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wv</span><span class="p">)</span>
        <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">_ReshapeW</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wqs</span> <span class="o">+</span> <span class="n">wi0s</span> <span class="o">+</span> <span class="n">wi1s</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span>
                 <span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">d_kv</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MSHD-&gt;BLSHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span>
            <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">splits</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">f2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">f2</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">q</span><span class="p">,</span>
                       <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">)</span>

        <span class="c1"># Only one head so there is nothing to shard the H in BLHD k,v tensors.</span>
        <span class="n">kv_split</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">qkv_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">kv_split</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span>
      <span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">_ReshapeW</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">ws</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span>
               <span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">d_kv</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;BLM,MSHD-&gt;BLSHD&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wc</span><span class="p">),</span> <span class="n">splits</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">_MeshSplitQKV</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

      <span class="k">return</span> <span class="p">[</span><span class="n">_MeshSplitQKV</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span> <span class="o">+</span> <span class="n">ret</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_Compute</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode.DecoderLayer"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode.DecoderLayer">[docs]</a>  <span class="k">def</span> <span class="nf">DecoderLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">name</span><span class="p">,</span>
                   <span class="n">activation_fn</span><span class="p">,</span>
                   <span class="n">conv_kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">hidden_dim_reshape_segments</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                   <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span>
                   <span class="n">norm_policy</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">collections</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias_shared&#39;</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_dec_self_attention_shared_var&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span> <span class="o">==</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_attention_type</span>

    <span class="k">def</span> <span class="nf">_ComputeBias</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="n">segment_pos</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecNotVisible</span><span class="p">(</span><span class="n">segment_id</span><span class="p">,</span> <span class="n">segment_pos</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e+09</span><span class="p">)</span>

    <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_memory_heads</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_num_heads</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">conv_kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNConv</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="n">conv_kernel_size</span><span class="p">)</span>
      <span class="n">d</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ff_dim</span> <span class="o">//</span> <span class="n">hidden_dim_reshape_segments</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span>
      <span class="n">model_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DepthwiseConvAutoregressive</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_kernel_size</span><span class="p">,</span> <span class="n">model_dims</span><span class="o">=</span><span class="n">model_dims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;ln&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LN</span><span class="p">(</span><span class="s1">&#39;ln&#39;</span><span class="p">)</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;true_ln&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_TrueLN</span><span class="p">(</span><span class="s1">&#39;true_ln&#39;</span><span class="p">)</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;pn&#39;</span><span class="p">:</span>
      <span class="n">norm_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PN</span><span class="p">(</span><span class="s1">&#39;pn&#39;</span><span class="p">)</span>
      <span class="n">optional_conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Norm type </span><span class="si">%s</span><span class="s1"> not supported&#39;</span> <span class="o">%</span> <span class="n">norm_type</span><span class="p">)</span>
    <span class="n">num_q</span><span class="p">,</span> <span class="n">num_kv</span><span class="p">,</span> <span class="n">num_wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_input_proj_weights</span>
    <span class="n">num_weights</span> <span class="o">=</span> <span class="n">num_q</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_kv</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_wi</span>
    <span class="n">wi_str</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;wi_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_weights</span><span class="p">)])</span>
    <span class="n">num_atten</span><span class="p">,</span> <span class="n">num_wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_output_proj_weights</span>
    <span class="n">num_weights</span> <span class="o">=</span> <span class="n">num_atten</span> <span class="o">+</span> <span class="n">num_wo</span>
    <span class="n">wo_str</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;wo_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_weights</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">_Outputs</span><span class="p">(</span><span class="n">o_atten</span><span class="p">,</span> <span class="n">o_ffw</span><span class="p">,</span> <span class="o">*</span><span class="n">ws</span><span class="p">):</span>
      <span class="n">h_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">attention_key_value_dim</span><span class="p">]</span>
      <span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h_shape</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReshapedModelDims</span><span class="p">())</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">ws</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

      <span class="n">o_atten</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o_atten</span><span class="p">,</span> <span class="n">o_atten</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_shape</span><span class="p">)</span>
      <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MeshSplit</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">o_atten</span><span class="p">,</span> <span class="n">o_ffw</span><span class="p">],</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">blh_split</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EinsumWithModelDim</span><span class="p">(</span><span class="s1">&#39;SHDM,BLSHD-&gt;BLM&#39;</span><span class="p">,</span> <span class="n">wo</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.0</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">norm_policy</span> <span class="o">!=</span> <span class="s1">&#39;pre&#39;</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Normalization policy </span><span class="si">%s</span><span class="s1"> not supported&#39;</span> <span class="o">%</span> <span class="n">norm_policy</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span>
        <span class="p">(</span><span class="s1">&#39;i.vec,i.segment_id-&gt;input_masked&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_id-&gt;o.segment_id&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;segment_id&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_pos-&gt;o.segment_pos&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;segment_pos&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_masked-&gt;x&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">wi_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInputProjWeights</span><span class="p">(</span><span class="s1">&#39;get_w_in&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;relative_bias_ws&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_RelativeAttentionBiasWeights</span><span class="p">(</span><span class="s1">&#39;wrb&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;x,</span><span class="si">%s</span><span class="s1">-&gt;q,k,v,h1,h2&#39;</span> <span class="o">%</span> <span class="n">wi_str</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeQKVH</span><span class="p">(</span><span class="s1">&#39;qkvh&#39;</span><span class="p">,</span> <span class="n">hidden_dim_reshape_segments</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;k-&gt;k_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;k_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;v-&gt;v_full&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;v_state&#39;</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_pos-&gt;key_segment_pos&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_AttentionState</span><span class="p">(</span><span class="s1">&#39;seg_pos&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_id,i.segment_pos-&gt;qq_bias&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span>
            <span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">_ComputeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;qq_bias-&gt;qk_bias&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Override</span><span class="p">(</span><span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">)),</span>
        <span class="c1"># Decoder _AddRelativeBias always has bidirectional=False.</span>
        <span class="p">(</span><span class="s1">&#39;qk_bias,i.segment_pos,key_segment_pos,relative_bias_ws-&gt;qhk_bias&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;relative_bias&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_AddRelativeBias</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;q,k_full,v_full,qhk_bias-&gt;o_atten&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="s1">&#39;attention&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h1-&gt;h1_act&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;h1_act&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;h1_act-&gt;h1_conv&#39;</span><span class="p">,</span> <span class="n">optional_conv_layer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;h1_conv,h2-&gt;o_ffw&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;h_ffw&#39;</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;-&gt;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">wo_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerOutputProjWeights</span><span class="p">(</span><span class="s1">&#39;get_w_out&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;o_atten,o_ffw,</span><span class="si">%s</span><span class="s1">-&gt;y&#39;</span> <span class="o">%</span> <span class="n">wo_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Fn</span><span class="p">(</span><span class="s1">&#39;compute_output&#39;</span><span class="p">,</span> <span class="n">_Outputs</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;y-&gt;y_dropout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;y_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_masked,y_dropout-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Add</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)))</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_DecoderLayerInMapKeys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;vec&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_id&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_pos&#39;</span><span class="p">]</span>

<div class="viewcode-block" id="RecurrentDenseBuilderParallelDecode.DecoderLayerStack"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.RecurrentDenseBuilderParallelDecode.DecoderLayerStack">[docs]</a>  <span class="k">def</span> <span class="nf">DecoderLayerStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">name</span><span class="p">,</span>
                        <span class="n">sub_layers</span><span class="p">,</span>
                        <span class="n">num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">conv_kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">norm_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DecoderLayerStack with self attention and feedforward in parallel.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">norm_policy</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">deterministic_dropout</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;i.vec-&gt;inputs_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;inputs_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">,</span> <span class="mi">2</span><span class="p">))),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_id-&gt;segment_id_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;segment_id_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">[:</span><span class="mi">2</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;i.segment_pos-&gt;segment_pos_split&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;segment_pos_split&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">blm_split</span><span class="p">[:</span><span class="mi">2</span><span class="p">])),</span>
        <span class="p">(</span><span class="s1">&#39;inputs_split-&gt;input_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;input_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;input_dropout,segment_id_split,segment_pos_split-&gt;sub_layer_imap&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_CreateNestedMap</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;map&#39;</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_DecoderLayerInMapKeys</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;sub_layer_imap-&gt;sub_layer_omap&#39;</span><span class="p">,</span> <span class="n">sub_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
        <span class="p">(</span><span class="s1">&#39;sub_layer_omap.vec-&gt;y_norm&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LNNoScale</span><span class="p">(</span><span class="s1">&#39;final_layer_norm&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;y_norm-&gt;y_dropout&#39;</span><span class="p">,</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="s1">&#39;outputs_dropout&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;i.aux_loss-&gt;o.aux_loss&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Identity</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;y_dropout,segment_id_split-&gt;o.vec&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Mask</span><span class="p">()),</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">stack</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="UniTransformer"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer">[docs]</a><span class="k">class</span> <span class="nc">UniTransformer</span><span class="p">(</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseTask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;LM TransformerModel with z-loss.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="UniTransformer.Params"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;debug&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If true, outfeed additional per-example tensor.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;builder&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;GShard Builder.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Vocabulary size&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;max_length&#39;</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span>
        <span class="s1">&#39;Max sequence length. Second pos_emb Tensor dim is set to &#39;</span> <span class="o">+</span>
        <span class="s1">&#39;max_length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Batch size. Unused.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_transformer_layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Number of blocks in builder.{Decoder,Encoder}LayerStack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;loss_denominator&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;If positive, ignore the value of &#39;</span>
        <span class="s1">&#39;use_tgt_labels_size_as_loss_denominator and set the denominator &#39;</span>
        <span class="s1">&#39;directly.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_tgt_labels_size_as_loss_denominator&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;False to use total number of non-padding tokens instead of &#39;</span>
        <span class="s1">&#39;fixed tgt_labels tensor size.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;aux_loss_coef&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;Multiplier for GShard aux_loss.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;enable_tpu_summary&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Enables GetTpuSummaryTensors().&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;Label smoothing.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;logits_abs_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Logits clipping.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;z_loss&#39;</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="s1">&#39;if z_loss is nonzero, we add a loss equal to &#39;</span>
        <span class="s1">&#39;z_loss * tf.math.square(tf.math.reduce_logsumexp(logits, -1))&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;positional_embedding&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Positional embs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gated_gelu&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;FFN gated GELU. &#39;</span>
             <span class="s1">&#39;Deprecated. Use gated_ffn_activation=gelu.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;moe_gated_gelu&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Use gated GELU for the MoE layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gated_ffn_activation&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Transformer gated FFN activation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;parallel_ffn&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to make ffn and attention parallel.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;hidden_dim_reshape_segments&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s1">&#39;Size of S when reshaping hidden dimension H to Sh. Only used when&#39;</span>
        <span class="s1">&#39; parallel_ffn is true currently.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;conv_kernel_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Optional 1D depthwise convolutional kernel.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_per_layer_vars_for_recurrent&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Create per-layer variables for RecurrentDenseBuilderParallelDecode, &#39;</span>
        <span class="s1">&#39;instead of  combined variables [num_layers, ...].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_repeat_layer&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use RepeatLayer to wrap the layer stack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_spmd_pipeline_stages&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;If &gt; 1, SPMD-shardable pipelining is used with this many stages.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_spmd_pipeline_microbatches&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Number of microbatches when num_spmd_pipeline_stages &gt; 1.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;moe&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True for Mixture-of-Experts, False for canonical Transformer model, &#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="s1">&#39;Transformer non-gated FFN activation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;norm_type&#39;</span><span class="p">,</span> <span class="s1">&#39;ln&#39;</span><span class="p">,</span> <span class="s1">&#39;Type of normalization. Options are: &#39;</span>
             <span class="s1">&#39;[ln, pn, true_ln].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;norm_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;pre&#39;</span><span class="p">,</span> <span class="s1">&#39;Policy for applying normalization. &#39;</span>
        <span class="s1">&#39;Options are: [pre, primer].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;multi_dconv_head_att&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s2">&quot;Whether or not to use Primer&#39;s Mutli-Dconv-Head attention.&quot;</span><span class="p">)</span>
    <span class="c1"># TODO(krikun): add a separate params class for decoder options</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;decoder_max_steps&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;Max decoder iterations for inference.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;decoder_beam_size&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Beam size for beam search decoding.&#39;</span><span class="p">)</span>
    <span class="c1"># In common vocab:</span>
    <span class="c1"># 0 =&gt; &lt;pad&gt;</span>
    <span class="c1"># 1 =&gt; &lt;/s&gt;</span>
    <span class="c1"># 2 =&gt; &lt;unk&gt;</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;decoder_eos_id&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&lt;/s&gt; is in model SPM&#39;</span><span class="p">)</span>
    <span class="c1"># bos_id is not used in prefix decoding</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;decoder_bos_id&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;&lt;s&gt; is in model SPM&#39;</span><span class="p">)</span>
    <span class="c1"># start_layer_id, has_embedding_layer and has_final_layer  are usedful to</span>
    <span class="c1"># construct a sub model during inference, each sub model only contains part</span>
    <span class="c1"># of layers from the model. The uni-transformer model is composed of an</span>
    <span class="c1"># embedding layer, following by num_sub_layer * num_transformer_layer</span>
    <span class="c1"># transformer layers and final softmax feedforward layer.</span>
    <span class="c1"># For example, if num_transformer_layer = 16 and num_sub_layer = 2, and we</span>
    <span class="c1"># want to split the model into 3 sub models with num_transformer_layer = 4</span>
    <span class="c1"># in each sub model:</span>
    <span class="c1">#   1. To configure the 1st sub model, we need to have</span>
    <span class="c1"># num_transformer_layer = 4, start_layer_id = 0, has_embedding_layer = True,</span>
    <span class="c1"># has_final_layer = False;</span>
    <span class="c1">#   2. To configure the 2nd sub model, we need to have</span>
    <span class="c1"># num_transformer_layer = 4, start_layer_id = 1 * num_transformer_layer *</span>
    <span class="c1"># num_sub_layer = 8, has_embedding_layer = False, has_final_layer = False;</span>
    <span class="c1">#   3. To configure the 3rd sub model, we need to have</span>
    <span class="c1"># num_transformer_layer = 4, start_layer_id = 2 * num_transformer_layer *</span>
    <span class="c1"># num_sub_layer = 16, has_embedding_layer = False, has_final_layer = True;</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;start_layer_id&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Start layer id.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_embedding_layer&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Model has embedding layer or not.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_final_layer&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;The model has the final layer such as &#39;</span>
             <span class="s1">&#39;feedforward net.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_repeat_layer</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">deterministic_dropout</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>

    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span><span class="p">):</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span> <span class="o">==</span> <span class="s1">&#39;silu&#39;</span><span class="p">:</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_gelu</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span> <span class="o">==</span> <span class="s1">&#39;gelu&#39;</span><span class="p">:</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_embedding_layer</span><span class="p">:</span>
      <span class="n">dec_emb</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="s1">&#39;dec_emb&#39;</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dec_emb&#39;</span><span class="p">,</span> <span class="n">dec_emb</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
        <span class="n">dec_pos_emb</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="s1">&#39;dec_pos_emb&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dec_pos_emb&#39;</span><span class="p">,</span> <span class="n">dec_pos_emb</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="c1"># Make sure training won&#39;t enable partially constructed model code</span>
      <span class="c1"># path.</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">start_layer_id</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">has_embedding_layer</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">has_final_layer</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">parallel_ffn</span><span class="p">:</span>  <span class="c1"># Only works with RecurrentDenseBuilderParallelDecode.</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span>
      <span class="k">assert</span> <span class="n">gated_ffn_activation</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">RecurrentDenseBuilderParallelDecode</span><span class="p">)</span>
      <span class="c1"># Make sure multi stage inference won&#39;t jump into this path.</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">start_layer_id</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">has_embedding_layer</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">has_final_layer</span>
      <span class="n">decoder_sub_layers</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">b</span><span class="o">.</span><span class="n">Repeat</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;blocks&#39;</span><span class="p">,</span>
              <span class="n">body</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">DecoderLayer</span><span class="p">(</span>
                  <span class="s1">&#39;block&#39;</span><span class="p">,</span>
                  <span class="n">gated_ffn_activation</span><span class="p">,</span>
                  <span class="n">p</span><span class="o">.</span><span class="n">conv_kernel_size</span><span class="p">,</span>
                  <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim_reshape_segments</span><span class="p">,</span>
                  <span class="n">norm_type</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
                  <span class="n">norm_policy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_policy</span><span class="p">),</span>
              <span class="n">repeat</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span><span class="p">,</span>
              <span class="n">per_layer_vars</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_per_layer_vars_for_recurrent</span><span class="p">)</span>
      <span class="p">]</span>
      <span class="n">dec</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecoderLayerStack</span><span class="p">(</span>
          <span class="s1">&#39;decoder&#39;</span><span class="p">,</span>
          <span class="n">decoder_sub_layers</span><span class="p">,</span>
          <span class="mi">1</span><span class="p">,</span>
          <span class="n">conv_kernel_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">conv_kernel_size</span><span class="p">,</span>
          <span class="n">norm_type</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
          <span class="n">norm_policy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_policy</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">multi_dconv_head_att</span><span class="p">:</span>
          <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecMultiDconvHeadAttention</span><span class="p">(</span><span class="s1">&#39;multi_dconv_head_att&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecSelfAttention</span><span class="p">(</span><span class="s1">&#39;dec_self_attention&#39;</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">multi_dconv_head_att</span><span class="p">:</span>
        <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecMultiDconvHeadAttentionRelativeBias</span><span class="p">(</span>
            <span class="s1">&#39;multi_dconv_head_att&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecSelfAttentionRelativeBias</span><span class="p">(</span><span class="s1">&#39;dec_self_attention&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">gated_ffn_activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ffw_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DenseReluDense</span><span class="p">(</span>
            <span class="s1">&#39;dense_relu_dense&#39;</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ffw_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DenseReluDenseGated</span><span class="p">(</span>
            <span class="s1">&#39;dense_relu_dense&#39;</span><span class="p">,</span> <span class="n">gated_ffn_activation</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_gated_gelu</span><span class="p">:</span>
          <span class="n">moe_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MoEGated</span><span class="p">(</span><span class="s1">&#39;moe&#39;</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">moe_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MoE</span><span class="p">(</span><span class="s1">&#39;moe&#39;</span><span class="p">,</span> <span class="n">decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">decoder_sub_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">atten_layer</span><span class="p">,</span> <span class="n">moe_layer</span><span class="p">,</span> <span class="n">atten_layer</span><span class="p">,</span> <span class="n">ffw_layer</span><span class="p">]</span>
        <span class="n">num_decoder_layers</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span> <span class="o">//</span> <span class="mi">2</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">decoder_sub_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">atten_layer</span><span class="p">,</span> <span class="n">ffw_layer</span><span class="p">]</span>
        <span class="n">num_decoder_layers</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span>
      <span class="n">dec</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DecoderLayerStack</span><span class="p">(</span>
          <span class="s1">&#39;decoder&#39;</span><span class="p">,</span>
          <span class="n">decoder_sub_layers</span><span class="p">,</span>
          <span class="n">num_decoder_layers</span><span class="p">,</span>
          <span class="n">conv_kernel_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">conv_kernel_size</span><span class="p">,</span>
          <span class="n">norm_type</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
          <span class="n">norm_policy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">norm_policy</span><span class="p">,</span>
          <span class="n">use_repeat_layer</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_repeat_layer</span><span class="p">,</span>
          <span class="n">spmd_pipeline_stages</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span><span class="p">,</span>
          <span class="n">spmd_pipeline_microbatches</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_microbatches</span><span class="p">,</span>
          <span class="n">start_layer_id</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">start_layer_id</span><span class="p">,</span>
          <span class="n">has_final_layer</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">has_final_layer</span><span class="p">)</span>
    <span class="n">dec</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">emb_w_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;w_split&#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">)</span>
    <span class="n">dec_out_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;dec_out_split&#39;</span><span class="p">,</span>
                                <span class="n">b</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">blm_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:],</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">logits_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;logits_split&#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">logits_split</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dec&#39;</span><span class="p">,</span> <span class="n">dec</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;emb_w_split&#39;</span><span class="p">,</span> <span class="n">emb_w_split</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dec_out_split&#39;</span><span class="p">,</span> <span class="n">dec_out_split</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;logits_split&#39;</span><span class="p">,</span> <span class="n">logits_split</span><span class="p">)</span>

<div class="viewcode-block" id="UniTransformer._ComputeDecoderInput"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._ComputeDecoderInput">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeDecoderInput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">gating_func</span> <span class="o">==</span> <span class="s1">&#39;hashing&#39;</span><span class="p">:</span>
      <span class="n">expert_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">e_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_embedding_layer</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_emb</span><span class="p">,</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_pos_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_pos_emb</span><span class="p">,</span>
                                    <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>

    <span class="n">decoded_input</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">vec</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">segment_id</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">segment_pos</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">,</span>
        <span class="n">encoder_output</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
        <span class="n">encoder_segment_id</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">),</span>
        <span class="n">encoder_segment_pos</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">),</span>
        <span class="n">aux_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">gating_func</span> <span class="o">==</span> <span class="s1">&#39;hashing&#39;</span><span class="p">:</span>
      <span class="n">decoded_input</span><span class="o">.</span><span class="n">expert_id</span> <span class="o">=</span> <span class="n">expert_id</span>
    <span class="k">return</span> <span class="n">decoded_input</span></div>

<div class="viewcode-block" id="UniTransformer.ComputePredictions"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.ComputePredictions">[docs]</a>  <span class="k">def</span> <span class="nf">ComputePredictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward propagation through one tower of the model.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing variable values of this task</span>
<span class="sd">        copied to this tower&#39;s devices.</span>
<span class="sd">      input_batch: A `.NestedMap` object containing input tensors to this tower.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A dict containing metrics pairs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">decoder_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeDecoderInput</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">)</span>
      <span class="n">all_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">)</span>
      <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">vec</span><span class="p">,</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">aux_loss</span>

      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">has_final_layer</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">aux_loss</span>

      <span class="n">dec_outputs</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_out_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_out_split</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="p">)</span>
      <span class="c1"># TODO(lepikhin): we only support</span>
      <span class="c1"># shared_embedding_and_softmax_weights=True at the moment.</span>
      <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">dec_emb</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span>
      <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_w_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">,</span>
                                               <span class="n">softmax_weights</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">softmax_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="c1"># to enable fprop_dtype = tf.bfloat16</span>
        <span class="n">softmax_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">softmax_weights</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">dec_outputs</span><span class="p">,</span> <span class="p">[</span><span class="n">dec_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,VM-&gt;BLV&#39;</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">softmax_weights</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span><span class="p">,</span>
                                        <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">aux_loss</span></div>

<div class="viewcode-block" id="UniTransformer._ComputeNonPadding"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._ComputeNonPadding">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeNonPadding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;paddings&#39;</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">paddings</span><span class="p">,</span>
                     <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>

    <span class="n">non_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>

    <span class="c1"># Negative target labels now indicate tokens that are to be used as</span>
    <span class="c1"># autoregressive inputs, but not counted in the loss.</span>
    <span class="n">non_padding</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">non_padding</span></div>

<div class="viewcode-block" id="UniTransformer._ComputeSoftLabels"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._ComputeSoftLabels">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeSoftLabels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>
    <span class="k">if</span> <span class="s1">&#39;soft_labels&#39;</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;using input_batch.tgt.soft_labels: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span>
                      <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">soft_labels</span><span class="p">)</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">soft_labels</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">label_smoothing</span>
      <span class="n">off_value</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">vocab_size</span>
      <span class="n">on_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">label_smoothing</span> <span class="o">+</span> <span class="n">off_value</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">({</span><span class="s1">&#39;on_value&#39;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">:</span> <span class="n">off_value</span><span class="p">})</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="n">vocab_size</span><span class="p">,</span>
          <span class="n">on_value</span><span class="o">=</span><span class="n">on_value</span><span class="p">,</span>
          <span class="n">off_value</span><span class="o">=</span><span class="n">off_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_labels</span></div>

<div class="viewcode-block" id="UniTransformer.ComputePerTokenLoss"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.ComputePerTokenLoss">[docs]</a>  <span class="k">def</span> <span class="nf">ComputePerTokenLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeSoftLabels</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">soft_labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">log_z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_loss_increment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">log_z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">z_loss_increment</span>

      <span class="n">non_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeNonPadding</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">non_padding</span></div>

<div class="viewcode-block" id="UniTransformer.ComputeLoss"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.ComputeLoss">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">logits</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">predictions</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeSoftLabels</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

      <span class="n">entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">soft_labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

      <span class="n">top1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span>
          <span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">acc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">top1</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">acc1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">entropy</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">acc1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">entropy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="n">soft_labels_entropy</span> <span class="o">=</span> <span class="n">loss</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">log_z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_loss_increment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">log_z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">z_loss_increment</span>

      <span class="n">non_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeNonPadding</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

      <span class="n">per_token_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">non_padding</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span><span class="p">:</span>
        <span class="n">per_token_z_loss_increment</span> <span class="o">=</span> <span class="n">z_loss_increment</span> <span class="o">*</span> <span class="n">non_padding</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">loss_denominator</span><span class="p">:</span>
        <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">loss_denominator</span>
      <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">use_tgt_labels_size_as_loss_denominator</span><span class="p">:</span>
        <span class="c1"># E.g. loss is going to be tiny if inputs are not packed and only a</span>
        <span class="c1"># fraction of tgt_labels are non-padding.</span>
        <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">non_padding</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">)</span>
      <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_token_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">loss_denom</span>
      <span class="n">avg_z_loss_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_token_z_loss_increment</span><span class="p">)</span> <span class="o">/</span>
                              <span class="n">loss_denom</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">z_loss</span> <span class="k">else</span> <span class="mf">0.0</span>

      <span class="n">soft_labels_entropy</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">soft_labels_entropy</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">)</span> <span class="o">/</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">))</span>
      <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">aux_loss_coef</span> <span class="o">*</span> <span class="n">aux_loss</span>

      <span class="n">num_items_in_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="n">num_nonpadding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">_ToInt32</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
      <span class="n">batch_capacity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

      <span class="n">whole_tgt_correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">acc1</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">non_padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

      <span class="c1"># TODO(lepikhin): consider returning</span>
      <span class="c1">#   {&#39;loss&#39;: (unnormalized per_token_loss, tf.reduce_sum(non_padding))}</span>
      <span class="n">per_step_loss</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
      <span class="p">}</span>

      <span class="n">eval_metrics</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">&#39;num_packed_examples&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_items_in_batch</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;batch_utilized_ratio&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_nonpadding</span> <span class="o">/</span> <span class="n">batch_capacity</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;acc1&#39;</span><span class="p">:</span>
              <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">acc1</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">),</span>
               <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">)),</span>
          <span class="s1">&#39;whole_tgt_accuracy&#39;</span><span class="p">:</span>
              <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">whole_tgt_correct</span><span class="p">)</span> <span class="o">/</span>
               <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">whole_tgt_correct</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">whole_tgt_correct</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="mf">1.0</span>
              <span class="p">),</span>
          <span class="s1">&#39;mean_xent&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">entropy</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">)</span> <span class="o">/</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">)),</span>
          <span class="s1">&#39;soft_labels_xent&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">soft_labels_entropy</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">)),</span>
          <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;aux_loss&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">aux_loss_coef</span> <span class="o">*</span> <span class="n">aux_loss</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;avg_z_loss_increment&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">avg_z_loss_increment</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
      <span class="p">}</span>
      <span class="k">if</span> <span class="s1">&#39;word_count&#39;</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="p">:</span>
        <span class="c1"># +input_batch.num_sentences to account for the end of sequence symbol.</span>
        <span class="n">num_words</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">word_count</span> <span class="o">+</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">num_sentences</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)),</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
        <span class="n">eval_metrics</span><span class="p">[</span><span class="s1">&#39;num_words&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">eval_metrics</span><span class="p">[</span><span class="s1">&#39;log_pplx_per_word&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">entropy</span> <span class="o">*</span> <span class="n">non_padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">num_words</span><span class="p">)</span>
      <span class="c1"># During training, the tpu summary tensors are added in _BPropGenTrainOps.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">enable_tpu_summary</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetTpuSummaryTensors</span><span class="p">()):</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;TpuSummaryTensor=&gt;EvalMetric </span><span class="si">%r</span><span class="s1"> </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">))</span>
          <span class="n">eval_metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">eval_metrics</span><span class="p">,</span> <span class="n">per_step_loss</span></div>

<div class="viewcode-block" id="UniTransformer.FilterPerExampleTensors"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.FilterPerExampleTensors">[docs]</a>  <span class="k">def</span> <span class="nf">FilterPerExampleTensors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">per_step</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">per_step</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">debug</span> <span class="k">else</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="UniTransformer._BPropGenTrainOps"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._BPropGenTrainOps">[docs]</a>  <span class="k">def</span> <span class="nf">_BPropGenTrainOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vmap</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs the backward graph.&quot;&quot;&quot;</span>
    <span class="n">train_ops</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_BPropGenTrainOps</span><span class="p">(</span><span class="n">vmap</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">enable_tpu_summary</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span>
                <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetTpuSummaryTensors</span><span class="p">()):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;TpuSummaryTensor=&gt;EvalMetric </span><span class="si">%r</span><span class="s1"> </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">AddEvalMetric</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ops</span></div>

<div class="viewcode-block" id="UniTransformer.ProcessFPropResults"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.ProcessFPropResults">[docs]</a>  <span class="k">def</span> <span class="nf">ProcessFPropResults</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">per_step</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">sess</span><span class="p">,</span> <span class="n">metrics</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">per_step</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">iterations_per_loop</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_values</span> <span class="ow">in</span> <span class="n">per_step</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">iterations_per_loop</span> <span class="o">=</span> <span class="n">iterations_per_loop</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">per_step_values</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">iterations_per_loop</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">per_step_values</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations_per_loop</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_values</span> <span class="ow">in</span> <span class="n">per_step</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Each per_step_values is an aggregation of outfeed tensor over</span>
        <span class="c1"># iterations_per_loop steps.</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Step = </span><span class="si">{}</span><span class="s1">, </span><span class="si">{}</span><span class="s1"> = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">global_step</span> <span class="o">+</span> <span class="n">t</span> <span class="o">-</span> <span class="n">iterations_per_loop</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">per_step_values</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span></div>

<div class="viewcode-block" id="UniTransformer._top_k_fn"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._top_k_fn">[docs]</a>  <span class="k">def</span> <span class="nf">_top_k_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span></div>

<div class="viewcode-block" id="UniTransformer.DecodeIds"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer.DecodeIds">[docs]</a>  <span class="k">def</span> <span class="nf">DecodeIds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="c1"># beam search</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;DecodeIds theta=</span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;DecodeIds input_batch=</span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;decode_ids&#39;</span><span class="p">):</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="c1"># Use full tgt.ids as prefix</span>
      <span class="n">prefix</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span>
      <span class="n">prefix_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="n">prefix_max</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

      <span class="n">beam_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">decoder_beam_size</span>
      <span class="k">assert</span> <span class="n">prefix_max</span> <span class="o">%</span> <span class="n">beam_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">prefix_max</span><span class="p">,</span> <span class="n">beam_size</span><span class="p">)</span>
      <span class="n">max_steps</span> <span class="o">=</span> <span class="p">(</span><span class="n">prefix_max</span> <span class="o">//</span> <span class="n">beam_size</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">decoder_max_steps</span><span class="p">)</span>

      <span class="n">dec_state</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">StateLayer</span><span class="o">.</span><span class="n">InitState</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">])</span>

      <span class="n">flat_bs</span> <span class="o">=</span> <span class="n">flat_beam_search_helper</span><span class="o">.</span><span class="n">flat_beam_search</span><span class="p">(</span>
          <span class="n">batch_size</span><span class="p">,</span>
          <span class="n">beam_size</span><span class="p">,</span>
          <span class="n">max_steps</span><span class="p">,</span>
          <span class="n">dec_callback</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_FlatBeamSearchCallback</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span>
                                         <span class="n">input_batch</span><span class="p">),</span>
          <span class="n">dec_state</span><span class="o">=</span><span class="n">dec_state</span><span class="p">,</span>
          <span class="n">eos_id</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">decoder_eos_id</span><span class="p">,</span>
          <span class="n">bos_id</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">decoder_bos_id</span><span class="p">,</span>
          <span class="n">length_norm_alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
          <span class="n">beam_gap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">top_k_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_top_k_fn</span><span class="p">(),</span>
          <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
          <span class="n">prefix_len</span><span class="o">=</span><span class="n">prefix_len</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;flat_bs: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">flat_bs</span><span class="p">)</span>

      <span class="n">loop_vars</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">nbest</span> <span class="o">=</span> <span class="n">flat_bs</span>
      <span class="p">(</span><span class="n">topk_ids</span><span class="p">,</span> <span class="n">topk_lens</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">)</span> <span class="o">=</span> <span class="n">nbest</span>
      <span class="k">del</span> <span class="n">loop_vars</span><span class="p">,</span> <span class="n">dec_state</span>

      <span class="n">bs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="n">bs</span><span class="o">.</span><span class="n">topk_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">topk_ids</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">bs</span><span class="o">.</span><span class="n">topk_lens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">topk_lens</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">bs</span><span class="o">.</span><span class="n">topk_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">topk_scores</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">bs</span><span class="o">.</span><span class="n">topk_hyps_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">topk_scores</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">bs</span></div>

<div class="viewcode-block" id="UniTransformer._FlatBeamSearchCallback"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.UniTransformer._FlatBeamSearchCallback">[docs]</a>  <span class="k">def</span> <span class="nf">_FlatBeamSearchCallback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">tgt_id</span><span class="p">,</span> <span class="n">tgt_segment_id</span><span class="p">,</span>
                              <span class="n">tgt_pos</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="n">tgt_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tgt_id</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">tgt_segment_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tgt_segment_id</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="n">tgt_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tgt_pos</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_emb</span><span class="p">,</span> <span class="n">tgt_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_pos_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_pos_emb</span><span class="p">,</span> <span class="n">tgt_pos</span><span class="p">)</span>

    <span class="n">theta_with_state</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">StateLayer</span><span class="o">.</span><span class="n">UpdateTheta</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">dec_state</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">gshard_layers</span><span class="o">.</span><span class="n">OverrideLayer</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="s1">&#39;dec_self_attention_bias&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">((</span><span class="n">tgt_mask</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">))</span>

    <span class="c1"># These 3 args are required by the API but not used in any way.</span>
    <span class="n">enc_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">src_segment_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">)</span>
    <span class="n">src_segment_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">)</span>
    <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">vec</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">segment_id</span><span class="o">=</span><span class="n">tgt_segment_id</span><span class="p">,</span>
        <span class="n">segment_pos</span><span class="o">=</span><span class="n">tgt_pos</span><span class="p">,</span>
        <span class="n">encoder_output</span><span class="o">=</span><span class="n">enc_output</span><span class="p">,</span>
        <span class="n">encoder_segment_id</span><span class="o">=</span><span class="n">src_segment_ids</span><span class="p">,</span>
        <span class="n">encoder_segment_pos</span><span class="o">=</span><span class="n">src_segment_pos</span><span class="p">,</span>
        <span class="n">aux_loss</span><span class="o">=</span><span class="n">aux_loss</span><span class="p">)</span>
    <span class="n">all_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta_with_state</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_segment_ids</span><span class="p">,</span> <span class="n">src_segment_pos</span><span class="p">,</span> <span class="n">aux_loss</span>
    <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">vec</span>
    <span class="n">dec_outputs</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">dec_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_out_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dec_out_split</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;model_dim_reshape_segments&#39;</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span> <span class="ow">and</span>
        <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
      <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">dec_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># TODO(lepikhin): we only support</span>
    <span class="c1"># shared_embedding_and_softmax_weights=True at the moment.</span>
    <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">dec_emb</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span>
    <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_w_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">,</span> <span class="n">softmax_weights</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">softmax_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
      <span class="c1"># to enable fprop_dtype = tf.bfloat16</span>
      <span class="n">softmax_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">softmax_weights</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,VM-&gt;BLV&#39;</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">softmax_weights</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="c1"># TODO(krikun): make sure this is not needed for beam search</span>
    <span class="c1"># if p.logits_abs_max is not None:</span>
    <span class="c1">#   logits = py_utils.clip_by_value(logits, -p.logits_abs_max,</span>
    <span class="c1">#                                   p.logits_abs_max)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="n">dec_state</span> <span class="o">=</span> <span class="n">gshard_layers</span><span class="o">.</span><span class="n">StateLayer</span><span class="o">.</span><span class="n">UpdateState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">dec</span><span class="p">,</span>
                                                     <span class="n">dec_state</span><span class="p">)</span>
    <span class="n">gshard_layers</span><span class="o">.</span><span class="n">OverrideLayer</span><span class="o">.</span><span class="n">Clear</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">dec_state</span></div></div>


<span class="c1"># TODO(huangyp): Build a common BaseTransformer for [Bert|Uni|Bi]Transformer.</span>
<div class="viewcode-block" id="BertTransformer"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer">[docs]</a><span class="k">class</span> <span class="nc">BertTransformer</span><span class="p">(</span><span class="n">base_model</span><span class="o">.</span><span class="n">BaseTask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Encoder-only TransformerModel with gspmd annotations.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="BertTransformer.Params"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;builder&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;GShard Builder.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Vocabulary size&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Sequence length.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;max_length&#39;</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span>
        <span class="s1">&#39;Max sequence length. Second pos_emb Tensor dim is set to &#39;</span> <span class="o">+</span>
        <span class="s1">&#39;max_length when positional_embedding is true.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Batch size. Unused.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_transformer_layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Number of blocks in builder.</span><span class="si">{Encoder}</span><span class="s1">LayerStack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;loss_denominator&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;If positive, set the denominator directly.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;aux_loss_coef&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;Multiplier for GShard aux_loss.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;Label smoothing.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;logits_abs_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Logits clipping.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;z_loss&#39;</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="s1">&#39;if z_loss is nonzero, we add a loss equal to &#39;</span>
        <span class="s1">&#39;z_loss * tf.math.square(tf.math.reduce_logsumexp(logits, -1))&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;positional_embedding&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Positional embs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;gated_ffn_activation&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Transformer gated FFN activation.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_repeat_layer&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use RepeatLayer to wrap the layer stack.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_spmd_pipeline_stages&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;If &gt; 1, SPMD-shardable pipelining is used with this many stages.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_spmd_pipeline_microbatches&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Number of microbatches when num_spmd_pipeline_stages &gt; 1.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;moe&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;True for Mixture-of-Experts, False for canonical Transformer model, &#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;moe_gated_gelu&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Use gated GELU for the MoE layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="s1">&#39;Transformer non-gated FFN activation.&#39;</span><span class="p">)</span>

    <span class="c1"># MLM specific</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;mlm_loss_weight&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s1">&#39;The weight of the masked LM loss in the overall loss, in [0, 1].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;masked_lm&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaskedLmDataAugmenter</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
             <span class="s1">&#39;Data augmenter for masked lm.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;mask_token_id&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Masked token in the model SPM.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The constructor of the model.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">mlm_loss_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">masked_lm</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>
      <span class="n">p</span><span class="o">.</span><span class="n">masked_lm</span><span class="o">.</span><span class="n">mask_token_id</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">mask_token_id</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;masked_lm&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">masked_lm</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_repeat_layer</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">deterministic_dropout</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span><span class="p">):</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span> <span class="o">==</span> <span class="s1">&#39;silu&#39;</span><span class="p">:</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span> <span class="o">==</span> <span class="s1">&#39;gelu&#39;</span><span class="p">:</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">gated_ffn_activation</span>
      <span class="n">gated_ffn_activation</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">enc_emb</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="s1">&#39;enc_emb&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;enc_emb&#39;</span><span class="p">,</span> <span class="n">enc_emb</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
      <span class="n">enc_pos_emb</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="s1">&#39;enc_pos_emb&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;enc_pos_emb&#39;</span><span class="p">,</span> <span class="n">enc_pos_emb</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
      <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">SelfAttention</span><span class="p">(</span><span class="s1">&#39;self_attention&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">atten_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">SelfAttentionRelativeBias</span><span class="p">(</span><span class="s1">&#39;dec_self_attention&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gated_ffn_activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">ffw_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DenseReluDense</span><span class="p">(</span><span class="s1">&#39;dense_relu_dense&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ffw_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">DenseReluDenseGated</span><span class="p">(</span><span class="s1">&#39;dense_relu_dense&#39;</span><span class="p">,</span>
                                        <span class="n">gated_ffn_activation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">moe_gated_gelu</span><span class="p">:</span>
        <span class="n">moe_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MoEGated</span><span class="p">(</span><span class="s1">&#39;moe&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">moe_layer</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MoE</span><span class="p">(</span><span class="s1">&#39;moe&#39;</span><span class="p">)</span>
      <span class="n">encoder_sub_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">atten_layer</span><span class="p">,</span> <span class="n">moe_layer</span><span class="p">,</span> <span class="n">atten_layer</span><span class="p">,</span> <span class="n">ffw_layer</span><span class="p">]</span>
      <span class="n">num_encoder_layers</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">encoder_sub_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">atten_layer</span><span class="p">,</span> <span class="n">ffw_layer</span><span class="p">]</span>
      <span class="n">num_encoder_layers</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_transformer_layers</span>

    <span class="n">enc</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">EncoderLayerStack</span><span class="p">(</span>
        <span class="s1">&#39;encoder&#39;</span><span class="p">,</span>
        <span class="n">encoder_sub_layers</span><span class="p">,</span>
        <span class="n">num_encoder_layers</span><span class="p">,</span>
        <span class="n">use_repeat_layer</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_repeat_layer</span><span class="p">,</span>
        <span class="n">spmd_pipeline_stages</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_stages</span><span class="p">,</span>
        <span class="n">spmd_pipeline_microbatches</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_spmd_pipeline_microbatches</span><span class="p">)</span>
    <span class="n">enc</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">emb_w_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;w_split&#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">)</span>
    <span class="n">enc_out_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;enc_out_split&#39;</span><span class="p">,</span>
                                <span class="n">b</span><span class="o">.</span><span class="n">_AdjustMSplit</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">blm_split</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:],</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">logits_split</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="s1">&#39;logits_split&#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">logits_split</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;enc&#39;</span><span class="p">,</span> <span class="n">enc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;emb_w_split&#39;</span><span class="p">,</span> <span class="n">emb_w_split</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;enc_out_split&#39;</span><span class="p">,</span> <span class="n">enc_out_split</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;logits_split&#39;</span><span class="p">,</span> <span class="n">logits_split</span><span class="p">)</span>

<div class="viewcode-block" id="BertTransformer._ComputeEncInput"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer._ComputeEncInput">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeEncInput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># TODO(huangyp): maskables should exclude cls token once cls is implemented.</span>
    <span class="n">maskables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeNonPadding</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;masked_pos&#39;</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="p">:</span>
      <span class="n">emb_ids</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">masked_ids</span>
      <span class="n">masked_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">masked_pos</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">mlm_loss_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">emb_ids</span><span class="p">,</span> <span class="n">masked_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masked_lm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">masked_lm</span><span class="p">,</span>
                                                 <span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span>
                                                 <span class="mf">1.0</span> <span class="o">-</span> <span class="n">maskables</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">emb_ids</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span>
      <span class="n">masked_pos</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">maskables</span>

    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">enc_emb</span><span class="p">,</span> <span class="n">emb_ids</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">positional_embedding</span><span class="p">:</span>
      <span class="n">y</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_pos_emb</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">enc_pos_emb</span><span class="p">,</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">)</span>
    <span class="n">nmap</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">vec</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">segment_id</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">segment_pos</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">segment_pos</span><span class="p">,</span>
        <span class="n">masked_pos</span><span class="o">=</span><span class="n">masked_pos</span><span class="p">,</span>
        <span class="n">aux_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">nmap</span></div>

<div class="viewcode-block" id="BertTransformer.ComputePredictions"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer.ComputePredictions">[docs]</a>  <span class="k">def</span> <span class="nf">ComputePredictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward propagation through one tower of the model.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing variable values of this task</span>
<span class="sd">        copied to this tower&#39;s devices.</span>
<span class="sd">      input_batch: A `.NestedMap` object containing input tensors to this tower.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A NestedMap with fields:</span>
<span class="sd">        mlm_logits: [B, L, V] tensor for the softmax output.</span>
<span class="sd">        masked_pos: [B, L] tensor indicating which token was masked.</span>
<span class="sd">        aux_loss: aux_loss from moe layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeEncInput</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">)</span>
      <span class="n">all_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">enc</span><span class="p">,</span> <span class="n">enc_input</span><span class="p">)</span>
      <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">vec</span><span class="p">,</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">aux_loss</span>
      <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_out_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">enc_out_split</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>
      <span class="c1"># When use embedding w for softmax.</span>
      <span class="n">enc_outputs</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">enc_emb</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span>
      <span class="n">softmax_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_w_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">emb_w_split</span><span class="p">,</span>
                                               <span class="n">softmax_weights</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">enc_outputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">softmax_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="n">softmax_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">softmax_weights</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">model_dim_reshape_segments</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">enc_outputs</span><span class="p">,</span> <span class="p">[</span><span class="n">enc_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">enc_outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

      <span class="c1"># TODO(huangyp): implement faster mlm softmax using bf_py_utils.GatherK.</span>
      <span class="n">mlm_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;BLM,VM-&gt;BLV&#39;</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">softmax_weights</span><span class="p">)</span>
      <span class="n">mlm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">mlm_logits</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlm_logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">mlm_logits</span><span class="p">,</span> <span class="o">-</span><span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span><span class="p">,</span>
                                            <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span><span class="p">)</span>
      <span class="n">mlm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits_split</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">logits_split</span><span class="p">,</span> <span class="n">mlm_logits</span><span class="p">)</span>
      <span class="c1"># TODO(huangyp): Adds classification logits.</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">mlm_logits</span><span class="o">=</span><span class="n">mlm_logits</span><span class="p">,</span>
          <span class="n">mlm_masked_pos</span><span class="o">=</span><span class="n">enc_input</span><span class="o">.</span><span class="n">masked_pos</span><span class="p">,</span>
          <span class="n">aux_loss</span><span class="o">=</span><span class="n">aux_loss</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertTransformer._ComputeNonPadding"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer._ComputeNonPadding">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeNonPadding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="c1"># TODO(huangyp): Consider special tokens like eos.</span>
    <span class="k">if</span> <span class="s1">&#39;paddings&#39;</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">paddings</span><span class="p">,</span>
                     <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>

    <span class="n">non_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">segment_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>

    <span class="c1"># Negative target labels now indicate tokens that are to be used as</span>
    <span class="c1"># autoregressive inputs, but not counted in the loss.</span>
    <span class="n">non_padding</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">non_padding</span></div>

<div class="viewcode-block" id="BertTransformer._ComputeSoftLabels"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer._ComputeSoftLabels">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeSoftLabels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">label_smoothing</span>
    <span class="n">off_value</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">vocab_size</span>
    <span class="n">on_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">label_smoothing</span> <span class="o">+</span> <span class="n">off_value</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">({</span><span class="s1">&#39;on_value&#39;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">:</span> <span class="n">off_value</span><span class="p">})</span>
    <span class="n">soft_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">on_value</span><span class="o">=</span><span class="n">on_value</span><span class="p">,</span>
        <span class="n">off_value</span><span class="o">=</span><span class="n">off_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_labels</span></div>

<div class="viewcode-block" id="BertTransformer.ComputePerTokenLoss"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer.ComputePerTokenLoss">[docs]</a>  <span class="k">def</span> <span class="nf">ComputePerTokenLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeSoftLabels</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">soft_labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">log_z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_loss_increment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">log_z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">z_loss_increment</span>

      <span class="n">non_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeNonPadding</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">non_padding</span></div>

<div class="viewcode-block" id="BertTransformer.ComputeLoss"><a class="viewcode-back" href="../../../lingvo.core.gshard_builder.html#lingvo.core.gshard_builder.BertTransformer.ComputeLoss">[docs]</a>  <span class="k">def</span> <span class="nf">ComputeLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="c1"># TODO(huangyp): Adds classification loss.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">mlm_logits</span>
      <span class="n">masked_pos</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">mlm_masked_pos</span>
      <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">aux_loss</span>
      <span class="n">soft_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeSoftLabels</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>

      <span class="n">entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">soft_labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

      <span class="n">top1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">acc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">input_batch</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">top1</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">acc1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">entropy</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">acc1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">entropy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="n">soft_labels_entropy</span> <span class="o">=</span> <span class="n">loss</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">log_z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_loss_increment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">log_z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">z_loss_increment</span>

      <span class="n">non_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeNonPadding</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
      <span class="n">masked_non_padding</span> <span class="o">=</span> <span class="n">non_padding</span> <span class="o">*</span> <span class="n">masked_pos</span>
      <span class="n">num_masked_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">masked_non_padding</span><span class="p">)</span>

      <span class="n">per_token_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">masked_non_padding</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">z_loss</span><span class="p">:</span>
        <span class="n">per_token_z_loss_increment</span> <span class="o">=</span> <span class="n">z_loss_increment</span> <span class="o">*</span> <span class="n">masked_non_padding</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">loss_denominator</span><span class="p">:</span>
        <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">loss_denominator</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_denom</span> <span class="o">=</span> <span class="n">num_masked_pos</span>
      <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_token_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">loss_denom</span>
      <span class="n">avg_z_loss_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_token_z_loss_increment</span><span class="p">)</span> <span class="o">/</span>
                              <span class="n">loss_denom</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">z_loss</span> <span class="k">else</span> <span class="mf">0.0</span>

      <span class="n">soft_labels_entropy</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">soft_labels_entropy</span> <span class="o">*</span> <span class="n">masked_non_padding</span><span class="p">)</span> <span class="o">/</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">masked_non_padding</span><span class="p">))</span>
      <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">aux_loss_coef</span> <span class="o">*</span> <span class="n">aux_loss</span>

      <span class="n">per_step_loss</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
      <span class="p">}</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">non_padding</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

      <span class="n">eval_metrics</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">&#39;acc1&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">acc1</span> <span class="o">*</span> <span class="n">masked_non_padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_masked_pos</span><span class="p">,</span>
                   <span class="n">num_masked_pos</span><span class="p">),</span>
          <span class="s1">&#39;mean_xent&#39;</span><span class="p">:</span>
              <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">entropy</span> <span class="o">*</span> <span class="n">masked_non_padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_masked_pos</span><span class="p">,</span>
               <span class="n">num_masked_pos</span><span class="p">),</span>
          <span class="s1">&#39;soft_labels_xent&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">soft_labels_entropy</span><span class="p">,</span> <span class="n">num_masked_pos</span><span class="p">),</span>
          <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_masked_pos</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;num_total_tokens&#39;</span><span class="p">:</span>
              <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">non_padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span>
          <span class="s1">&#39;num_masked_tokens&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">num_masked_pos</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span>
          <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;aux_loss&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">aux_loss_coef</span> <span class="o">*</span> <span class="n">aux_loss</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
          <span class="s1">&#39;avg_z_loss_increment&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">avg_z_loss_increment</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
      <span class="p">}</span>
      <span class="c1"># During training, the tpu summary tensors are added in _BPropGenTrainOps.</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetTpuSummaryTensors</span><span class="p">()):</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;TpuSummaryTensor=&gt;EvalMetric </span><span class="si">%r</span><span class="s1"> </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">))</span>
          <span class="n">eval_metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">wgt</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">eval_metrics</span><span class="p">,</span> <span class="n">per_step_loss</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>