

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.core.layers &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.layers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Common layers.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">activations</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">bn_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">builder_layers</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">computation_cost</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">conv_layers_with_time_padding</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">pruning_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">quant_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">recurrent</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">schedule</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">summary_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">tshape</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sympy</span>

<span class="c1"># pylint:disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">inplace_ops</span>
<span class="c1"># pylint:enable=g-direct-tensorflow-import</span>


<div class="viewcode-block" id="DeconvLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer">[docs]</a><span class="k">class</span> <span class="nc">DeconvLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Deconv (transposed conv2d) layer.</span>

<span class="sd">  DeconvLayer is different from ConvTransposeLayer in that</span>
<span class="sd">  DeconvLayer does not support padding and biasing. Hence,</span>
<span class="sd">  it&#39;s simpler and more basic than ConvTransposeLayer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="DeconvLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter shape. Must be a sequence of length 4. Elements are in&#39;</span>
        <span class="s1">&#39; the order of height, width, out_channel, in_channel.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter stride to use. Must be a pair of ints. The first int&#39;</span>
        <span class="s1">&#39; specifies the stride on the height dimension. The second int&#39;</span>
        <span class="s1">&#39; specifies the stride on the width dimension.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span>

<div class="viewcode-block" id="DeconvLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="DeconvLayer.OutShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer.OutShape">[docs]</a>  <span class="k">def</span> <span class="nf">OutShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the output shape given the input shape.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">t_stride</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">f_stride</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
        <span class="n">in_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">t_stride</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">f_stride</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">])</span></div>

<div class="viewcode-block" id="DeconvLayer._ApplyConv"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer._ApplyConv">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span>
    <span class="c1"># TODO(miachen): remove casting once tf.nn.conv2d supports tf.float64.</span>
    <span class="k">assert</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">w</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># TODO(zhifengc): Try some better way to do Deconv. Search for</span>
    <span class="c1"># &quot;resize-convolution&quot;.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">w</span><span class="p">,</span>
        <span class="n">output_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">OutShape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)),</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span></div>

<div class="viewcode-block" id="DeconvLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeconvLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply deconvolution to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap object containing weights&#39; values of this layer and its</span>
<span class="sd">        children layers.</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, height,</span>
<span class="sd">        width, channel].</span>

<span class="sd">    Returns:</span>
<span class="sd">      outputs. outputs is expected to have shape [batch, height * height_stride,</span>
<span class="sd">      width * width_stride, out_channel].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span></div></div>


<span class="c1"># A subset of activation functions are supported by TFLite as fused activation</span>
<span class="c1"># functions with a preceding matmul or conv. If this is the case, then they</span>
<span class="c1"># require special treatment for quantization.</span>
<span class="n">_TFLITE_FUSED_ACTIVATION_NAMES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RELU6&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">LOG_SCALE_CLAMP_BOUND</span> <span class="o">=</span> <span class="mf">20.0</span>


<div class="viewcode-block" id="IdentityLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.IdentityLayer">[docs]</a><span class="k">class</span> <span class="nc">IdentityLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Identity layer, adds name and propagates its input.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="IdentityLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.IdentityLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Identity mapping.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The input tensor or the input NestedMap.</span>
<span class="sd">      *args: Arguments to be ignored.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor with the same shape and type of inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span></div>

<div class="viewcode-block" id="IdentityLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.IdentityLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<span class="c1"># TODO(yonghui/jonathanasdf): Remove the forwarded links.</span>
<span class="n">_ComputeConvOutputShape</span> <span class="o">=</span> <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">ComputeConvOutputShape</span>
<span class="n">_ComputeConvOutputPadding</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">conv_layers_with_time_padding</span><span class="o">.</span><span class="n">ComputeConvOutputPadding</span><span class="p">)</span>
<span class="n">BatchNormLayer</span> <span class="o">=</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">BatchNormLayer</span>
<span class="n">BatchNormLayerNoPadding</span> <span class="o">=</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">BatchNormLayerNoPadding</span>
<span class="n">AddingAccumulator</span> <span class="o">=</span> <span class="n">bn_layers</span><span class="o">.</span><span class="n">AddingAccumulator</span>


<div class="viewcode-block" id="BaseConv2DLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer">[docs]</a><span class="k">class</span> <span class="nc">BaseConv2DLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for 2D convolution layers.</span>

<span class="sd">  Has support for optional batch-normalization, activation and sequence</span>
<span class="sd">  padding.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BaseConv2DLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter shape. Must be a sequence of length 4. Elements are in&#39;</span>
        <span class="s1">&#39; the order of height (time), width (frequency), in_channel,&#39;</span>
        <span class="s1">&#39; out_channel. When causal_convolution is True, filter_shape[1]&#39;</span>
        <span class="s1">&#39; is the actual number of trained weights in the time dimension&#39;</span>
        <span class="s1">&#39; of the kernel.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter stride to use. Must be a pair of ints. The first int&#39;</span>
        <span class="s1">&#39; specifies the stride on the time dimension. The second int&#39;</span>
        <span class="s1">&#39; specifies the stride on the frequency dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dilation_rate&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;If &gt; 1, dilation rate for atrous convolution. &#39;</span>
        <span class="s1">&#39;Must be a pair of ints. &#39;</span>
        <span class="s1">&#39;The first int specifies the dilation rate on the time dimension. &#39;</span>
        <span class="s1">&#39;The second int specifies the dilation rate on the frequency &#39;</span>
        <span class="s1">&#39;dimension. &#39;</span>
        <span class="s1">&#39;If any value of dilation_rate is &gt; 1, then all values of strides &#39;</span>
        <span class="s1">&#39;must be 1.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Activation function to use. Options are RELU, RELU6, SIGMOID, &#39;</span>
        <span class="s1">&#39;TANH, NONE.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether or not to apply a bias before activation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether or not to apply batch norm.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;bn_decay&#39;</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="s1">&#39;Decay in updating the mean and variance moving average used in&#39;</span>
        <span class="s1">&#39; batch normalization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;bn_fold_weights&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Fold the batch norm parameters into the convolution weights at &#39;</span>
        <span class="s1">&#39;eval/inference time as per https://arxiv.org/pdf/1712.05877.pdf. &#39;</span>
        <span class="s1">&#39;Requires that batch_norm be True and is incompatible with some other &#39;</span>
        <span class="s1">&#39;parameters (conv_last=True).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;causal_convolution&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, conv layer output only depends on time steps in&#39;</span>
        <span class="s1">&#39; the past.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;conv_last&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, apply the convolution transformation as the last step, &#39;</span>
        <span class="s1">&#39;i.e., first apply batch normalization on the input, followed &#39;</span>
        <span class="s1">&#39;by activation, and finally the convolution. &#39;</span>
        <span class="s1">&#39;Otherwise, apply convolution first, followed by batch &#39;</span>
        <span class="s1">&#39;normalization and activation. Not compatible with bn_fold_weights &#39;</span>
        <span class="s1">&#39;or quantization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weight_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, apply weight normalization to weights as proposed by&#39;</span>
        <span class="s1">&#39; Salimans and Kingma, 2016: https://arxiv.org/abs/1602.07868&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;disable_activation_quantization&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Disables the quantization tracking/clamping for the output &#39;</span>
        <span class="s1">&#39;activation. This is most often used in conjunction with a concat &#39;</span>
        <span class="s1">&#39;layer which needs to have a merged set of statistics.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span>
    <span class="c1"># Bias is not needed with batch_norm=True.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;NONE&#39;</span> <span class="ow">or</span> <span class="n">activations</span><span class="o">.</span><span class="n">IsSupported</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="c1"># batch normalization dimension is number of input channels</span>
      <span class="c1"># (filter_shape[2]) if we apply batch_norm on input and convolution</span>
      <span class="c1"># in the end, number of output channels otherwise.</span>
      <span class="n">bn_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_last</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span>
      <span class="n">bn_params</span> <span class="o">=</span> <span class="n">BatchNormLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">dim</span><span class="o">=</span><span class="n">bn_dim</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bn_decay</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">params_init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="n">bn_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_bn_folded</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span> <span class="s1">&#39;bn_fold_weights requires batch_norm = True&#39;</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_last</span><span class="p">,</span> <span class="s1">&#39;bn_fold_weights requires conv_last = False&#39;</span>

    <span class="c1"># TODO(yonghui): implement the variational noise logic.</span>

<div class="viewcode-block" id="BaseConv2DLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="s1">&#39;b&#39;</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
              <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">],</span>
              <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
              <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="s1">&#39;g&#39;</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
              <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_output_shape</span><span class="p">,</span>
              <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
              <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">disable_activation_quantization</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TFLITE_FUSED_ACTIVATION_NAMES</span> <span class="ow">and</span>
          <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;pre_activation&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseConv2DLayer._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Backwards compatibility: manually call child.InstantiateVariables()</span>
    <span class="c1"># outside of tf.variable_scope(p.name).</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The number of output channels for this conv layer.&quot;&quot;&quot;</span>
    <span class="c1"># Normal convolution filter shape is [..., out_channels].</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">filter_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Final dims of the filter corresponding to the output channels.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A one (standard conv) or two (depthwise conv) element shape representing</span>
<span class="sd">      the final dimensions of the filter weights that are output channel</span>
<span class="sd">      specific for this layer. This shape is needed for any arithmetic that</span>
<span class="sd">      needs to convert between a linear list of filter weights and the</span>
<span class="sd">      arrangement in the actual filter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Standard convolution has all output channels in the last dim.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_is_bn_folded</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Whether batchnorm folded weights are effectively enabled.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">))</span>

<div class="viewcode-block" id="BaseConv2DLayer._EvaluateConvKernel"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._EvaluateConvKernel">[docs]</a>  <span class="k">def</span> <span class="nf">_EvaluateConvKernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="p">,</span>
                          <span class="n">padding_algorithm</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates the lower level convolution kernel.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: As to tf.nn.convolution.</span>
<span class="sd">      filter_w: As to tf.nn.depthwise_conv2d.</span>
<span class="sd">      strides: As to tf.nn.convolution.</span>
<span class="sd">      dilation_rate: As to tf.nn.convolution.</span>
<span class="sd">      padding_algorithm: As to tf.nn.convolution (padding argument).</span>
<span class="sd">      data_format: As to tf.nn.convolution.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Convolution kernel output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="BaseConv2DLayer.OutputShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer.OutputShape">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">OutputShape</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_ComputeConvOutputShape</span><span class="p">(</span><span class="n">in_shape</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                   <span class="n">params</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                   <span class="n">params</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>

<div class="viewcode-block" id="BaseConv2DLayer.OutShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer.OutShape">[docs]</a>  <span class="k">def</span> <span class="nf">OutShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the output shape given the input shape.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">_ComputeConvOutputShape</span><span class="p">(</span><span class="n">in_shape</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                   <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseConv2DLayer._GetWeights"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._GetWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_GetWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">theta</span><span class="p">,</span>
                  <span class="n">convolution_lambda</span><span class="p">,</span>
                  <span class="n">folded_bn_padding</span><span class="p">,</span>
                  <span class="n">cast_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets a dictionary of weights and biases for the convolution.</span>

<span class="sd">    This is necessary for some operating modes where the weights are fused</span>
<span class="sd">    with batch normalization differently for training vs eval.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing underlying weights values of this</span>
<span class="sd">        layer and its children layers.</span>
<span class="sd">      convolution_lambda: Lambda which takes the convolution weights and runs</span>
<span class="sd">        the convolution.</span>
<span class="sd">      folded_bn_padding: Padding to apply to folded batch normalization moment</span>
<span class="sd">        computation (or None for no padding).</span>
<span class="sd">      cast_dtype: If not None, cast weights to the given dtype.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tuple of (filter, biases).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Original weights.</span>
    <span class="n">filter_w</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span>
    <span class="n">filter_output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_output_shape</span>
    <span class="c1"># TODO(miachen): remove casting once tf.nn.conv2d supports tf.float64.</span>
    <span class="k">if</span> <span class="n">cast_dtype</span><span class="p">:</span>
      <span class="n">filter_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">filter_w</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">filter_output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Normalize along the last dim (standard conv).</span>
        <span class="n">filter_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">filter_w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
      <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">filter_output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Normalize along the last two dimensions (depthwise conv).</span>
        <span class="n">filter_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">filter_w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">filter_output_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Unsupported weight norm filter shape&#39;</span>

    <span class="c1"># Original bias.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)],</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">filter_w</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Pass-through if weights are not folded with batch normalization.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_bn_folded</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">b</span>

    <span class="c1"># If batch norm is fused with weights, then compute the weights as from</span>
    <span class="c1"># figure C.8 of https://arxiv.org/pdf/1712.05877.pdf for training and</span>
    <span class="c1"># figure C.6 for eval.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="c1"># Gets current moments without updating.</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">GetCurrentMoments</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Updates moments based on a trial run of the convolution.</span>
      <span class="n">raw_conv_output</span> <span class="o">=</span> <span class="n">convolution_lambda</span><span class="p">(</span><span class="n">filter_w</span><span class="p">)</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">ComputeAndUpdateMoments</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">raw_conv_output</span><span class="p">,</span> <span class="n">folded_bn_padding</span><span class="p">)</span>

    <span class="c1"># Fold weights and bias. Note that this layer&#39;s bias is not used (not</span>
    <span class="c1"># applicable for batch norm case).</span>
    <span class="n">sigma_recip</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">scale_correction</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">sigma_recip</span>
    <span class="c1"># Normal conv will have all weights in the last dim</span>
    <span class="c1"># ([_, _, _, output_channels]), which matches the 1D layout from</span>
    <span class="c1"># batch norm. Depthwise uses the last two dims so reshape</span>
    <span class="c1"># ([_, _, in_c, c_multiplier]).</span>
    <span class="n">scale_correction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale_correction</span><span class="p">,</span> <span class="n">filter_output_shape</span><span class="p">)</span>
    <span class="n">filter_w</span> <span class="o">=</span> <span class="n">filter_w</span> <span class="o">*</span> <span class="n">scale_correction</span>
    <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">beta</span> <span class="o">-</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">mean</span> <span class="o">*</span> <span class="n">sigma_recip</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">b</span></div>

<div class="viewcode-block" id="BaseConv2DLayer._ApplyConv"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._ApplyConv">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyConv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">folded_bn_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">cast_dtype</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">cast_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cast_dtype</span><span class="p">)</span>

    <span class="n">padding_algorithm</span> <span class="o">=</span> <span class="s1">&#39;SAME&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">causal_convolution</span><span class="p">:</span>
      <span class="c1"># Causal convolution is only applied in time (height) dimension.</span>
      <span class="c1"># Use VALID padding and shift the inputs to the right to ensure that the</span>
      <span class="c1"># first output only depends on the first input and so on. The output is</span>
      <span class="c1"># the same size as the input, as if the convolution used SAME padding.</span>
      <span class="n">padding_algorithm</span> <span class="o">=</span> <span class="s1">&#39;VALID&#39;</span>
      <span class="c1"># The effective spatial filter size for dilated convolutions is</span>
      <span class="c1"># (kernel - 1) * dilation_rate + 1 as according to</span>
      <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/nn/convolution.</span>
      <span class="n">causal_pad_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

      <span class="c1"># Apply padding in width dimension to mimic SAME padding.</span>
      <span class="c1"># Using the similar logic as above to produce the same number of output</span>
      <span class="c1"># as if SAME padding is used.</span>
      <span class="n">width_pad_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

      <span class="c1"># The amount of padding on the left is tricky. If stride &gt; 1, total</span>
      <span class="c1"># padding required for SAME padding would be:</span>
      <span class="c1">#   pad = ceil(input_size / stride - 1) * stride + eff_kernel - input_size</span>
      <span class="c1"># where eff_kernel = (kernel - 1) * dilation_rate + 1</span>
      <span class="c1"># TensorFlow also pads more on the right / bottom side if total padding</span>
      <span class="c1"># required is an odd number, so pad_left = pad // 2</span>
      <span class="c1"># Therefore pad_left could depend on input size, which might be dynamic.</span>
      <span class="c1"># Here we only handle two special cases where 1) stride = 1, then</span>
      <span class="c1">#   pad_left = (eff_kernel - 1) // 2</span>
      <span class="c1"># and 2) kernel = 1, then</span>
      <span class="c1">#   pad_left = 0</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Causal convolution only supports width stride = 1 &#39;</span>
                         <span class="s1">&#39;or filter width = 1.&#39;</span><span class="p">)</span>
      <span class="n">width_pad_left</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">width_pad_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
      <span class="n">width_pad_right</span> <span class="o">=</span> <span class="n">width_pad_size</span> <span class="o">-</span> <span class="n">width_pad_left</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">causal_pad_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                               <span class="p">[</span><span class="n">width_pad_left</span><span class="p">,</span> <span class="n">width_pad_right</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="c1"># Lambda for computing the actual convolution.</span>
    <span class="k">def</span> <span class="nf">ComputeRawConvolution</span><span class="p">(</span><span class="n">filter_w</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EvaluateConvKernel</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span>
          <span class="n">filter_w</span><span class="o">=</span><span class="n">filter_w</span><span class="p">,</span>
          <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
          <span class="n">dilation_rate</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">,</span>
          <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span>
          <span class="n">padding_algorithm</span><span class="o">=</span><span class="n">padding_algorithm</span><span class="p">)</span>

    <span class="n">filter_w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetWeights</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">ComputeRawConvolution</span><span class="p">,</span> <span class="n">folded_bn_padding</span><span class="p">,</span> <span class="n">cast_dtype</span><span class="o">=</span><span class="n">cast_dtype</span><span class="p">)</span>

    <span class="c1"># TODO(miachen): remove casting once tf.nn.conv2d supports tf.float64.</span>
    <span class="k">assert</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">filter_w</span><span class="o">.</span><span class="n">dtype</span>

    <span class="n">filter_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">filter_w</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ComputeRawConvolution</span><span class="p">(</span><span class="n">filter_w</span><span class="p">)</span>

    <span class="c1"># Note that we always apply the bias (which may be zero) because some</span>
    <span class="c1"># normalization mechanisms do implicitly produce a bias.</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="BaseConv2DLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply convolution to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        frequency, channel]. The time dimension corresponds to the height</span>
<span class="sd">        dimension as in images and the frequency dimension corresponds to the</span>
<span class="sd">        width dimension as in images.</span>
<span class="sd">      paddings: The paddings tensor. If None, the inputs have no paddings in the</span>
<span class="sd">        sense of sequence training (e.g., in CNN models). Otherwise, it is</span>
<span class="sd">        expected to be of shape [batch, time].</span>

<span class="sd">    Returns:</span>
<span class="sd">      outputs, out_paddings pair.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
      <span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]],</span> <span class="mi">0</span><span class="p">))</span>
      <span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="c1"># Zeroing out padded inputs.</span>
      <span class="n">qpadding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRPadding</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
      <span class="c1"># Select based padding is required for quantized inference but is</span>
      <span class="c1"># causing regressions on other platforms. TODO: Remove use_select</span>
      <span class="c1"># attribute when root-caused/resolved.</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span>
          <span class="n">qpadding</span><span class="p">,</span>
          <span class="n">inputs</span><span class="p">,</span>
          <span class="n">use_select</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">conv_padding</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># NOTE: this may be slightly inaccurate when p.dilation_rate[0] &gt; 1.</span>
        <span class="c1"># But there&#39;s likely no real problems. Trying to set it gives an error:</span>
        <span class="c1"># pooling with SAME padding is not implemented for dilation_rate &gt; 1.</span>
        <span class="c1"># NOTE: window=p.filter_stride[0] means output i will be padded if any</span>
        <span class="c1"># input in the stride between the two conv centers are padded.</span>
        <span class="n">conv_padding</span> <span class="o">=</span> <span class="n">_ComputeConvOutputPadding</span><span class="p">(</span>
            <span class="n">paddings</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">conv_last</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ComputeConvLast</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">conv_padding</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Compute</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">conv_padding</span><span class="p">)</span>

      <span class="c1"># Lastly zeroing out padded states.</span>
      <span class="k">if</span> <span class="n">conv_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qpadding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QRPadding</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">conv_padding</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Select based padding is required for quantized inference but is</span>
        <span class="c1"># causing regressions on other platforms. TODO: Remove use_select</span>
        <span class="c1"># attribute when root-caused/resolved.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span>
            <span class="n">qpadding</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">use_select</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

      <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span>
          <span class="n">out</span><span class="p">,</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">BaseConv2DLayer</span><span class="o">.</span><span class="n">OutShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">conv_padding</span></div>

<div class="viewcode-block" id="BaseConv2DLayer._Compute"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._Compute">[docs]</a>  <span class="k">def</span> <span class="nf">_Compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">conv_padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the forward prop (conv, bn, act).&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">bn_padding</span> <span class="o">=</span> <span class="n">conv_padding</span>
    <span class="k">if</span> <span class="n">bn_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">bn_padding_expanded</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">batch_time</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">bn_padding</span><span class="p">)</span>
      <span class="n">batch_time_any_any</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">bn_padding_expanded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bn_padding</span><span class="p">,</span>
                                       <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">bn_padding_expanded</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bn_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">batch_time_any_any</span><span class="p">)</span>
      <span class="p">],</span> <span class="n">out</span><span class="p">)</span>

    <span class="c1"># Only apply batch norm if it was not folded into the weights.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">bn_padding_expanded</span><span class="p">)</span>

    <span class="c1"># Apply activation.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TFLITE_FUSED_ACTIVATION_NAMES</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;pre_activation&#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">disable_activation_quantization</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="BaseConv2DLayer._ComputeConvLast"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BaseConv2DLayer._ComputeConvLast">[docs]</a>  <span class="k">def</span> <span class="nf">_ComputeConvLast</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">conv_padding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the forward prop in conv_last mode (bn, act, conv).&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">out_padding</span> <span class="o">=</span> <span class="n">paddings</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">out_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_padding_expanded</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">batch_time</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">out_padding</span><span class="p">)</span>
        <span class="n">batch_time_any_any</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">batch_time_any_any</span><span class="p">)</span>
        <span class="p">],</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">out_padding_expanded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out_padding</span><span class="p">,</span>
                                          <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_time</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">))</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">out_padding_expanded</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyConv</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="Conv2DLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayer">[docs]</a><span class="k">class</span> <span class="nc">Conv2DLayer</span><span class="p">(</span><span class="n">BaseConv2DLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convolution layer, with optional batch-normalization and activation.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Conv2DLayer._EvaluateConvKernel"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayer._EvaluateConvKernel">[docs]</a>  <span class="k">def</span> <span class="nf">_EvaluateConvKernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="p">,</span>
                          <span class="n">padding_algorithm</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">filter_w</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">dilations</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding_algorithm</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ConvNN2DLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvNN2DLayer">[docs]</a><span class="k">class</span> <span class="nc">ConvNN2DLayer</span><span class="p">(</span><span class="n">BaseConv2DLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convolution layer, based on tf.nn.conv2d instead of tf.nn.convolution.</span>

<span class="sd">  tf.nn.convolution is using a different implementation on atrous convolutions,</span>
<span class="sd">  by wrapping the actual convolution with space_to_batch and batch_to_space.</span>
<span class="sd">  This implementation is not supported in tflite conversion, hence we need</span>
<span class="sd">  a different layer for using atrous convolutions.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ConvNN2DLayer._EvaluateConvKernel"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvNN2DLayer._EvaluateConvKernel">[docs]</a>  <span class="k">def</span> <span class="nf">_EvaluateConvKernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="p">,</span>
                          <span class="n">padding_algorithm</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">filter_w</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">dilations</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span></div></div>


<span class="c1"># Alias of Conv2DLayer (for compatibility with historical uses).</span>
<span class="n">ConvLayer</span> <span class="o">=</span> <span class="n">Conv2DLayer</span>


<div class="viewcode-block" id="DepthwiseConv2DLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DepthwiseConv2DLayer">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseConv2DLayer</span><span class="p">(</span><span class="n">BaseConv2DLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Depthwise conv 2D layer.</span>

<span class="sd">  paper: https://arxiv.org/abs/1610.02357</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="DepthwiseConv2DLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DepthwiseConv2DLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="c1"># Redefine &#39;filter_shape&#39; since the semantic of shape elements is different</span>
    <span class="c1"># from regular Conv2D.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Delete</span><span class="p">(</span><span class="s1">&#39;filter_shape&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter shape. Must be a sequence of length 4. Elements are in&#39;</span>
        <span class="s1">&#39; the order of height (time), width (frequency), in_channel,&#39;</span>
        <span class="s1">&#39; channel_multipliers. &#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The number of output channels for this conv layer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Depthwise convolution filter shape is:</span>
    <span class="c1">#   [..., in_channels, channel_multiplier].</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">filter_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Final dims of the filter corresponding to the output channels.&quot;&quot;&quot;</span>
    <span class="c1"># Depthwise convolution uses the final two dims for output channels.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">in_c</span><span class="p">,</span> <span class="n">c_mul</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">in_c</span><span class="p">,</span> <span class="n">c_mul</span><span class="p">]</span>

<div class="viewcode-block" id="DepthwiseConv2DLayer._EvaluateConvKernel"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DepthwiseConv2DLayer._EvaluateConvKernel">[docs]</a>  <span class="k">def</span> <span class="nf">_EvaluateConvKernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="p">,</span>
                          <span class="n">padding_algorithm</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depthwise_conv2d</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="nb">filter</span><span class="o">=</span><span class="n">filter_w</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">dilations</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding_algorithm</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SeparableConv2DLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SeparableConv2DLayer">[docs]</a><span class="k">class</span> <span class="nc">SeparableConv2DLayer</span><span class="p">(</span><span class="n">Conv2DLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Separable 2D convolution.</span>

<span class="sd">  This class aggregates a DepthwiseConv2DLayer that feeds in to the point</span>
<span class="sd">  wise convolution defined by this layer. Since the point wise convolution</span>
<span class="sd">  controls the output, this class is defined in terms of that and delegates</span>
<span class="sd">  to a depthwise sub-layer.</span>

<span class="sd">  The `filter_shape` parameter is rewritten on initialization from the form:</span>
<span class="sd">    (h, w, cin, cout)</span>
<span class="sd">  To:</span>
<span class="sd">    Depthwise filter: (h, w, cin, p.depth_multiplier)</span>
<span class="sd">    Pointwise filter (on this instance): (1, 1, cin * p.depth_multiplier, cout)</span>

<span class="sd">  This way, the layer is configured as if it were a normal 2D convolution</span>
<span class="sd">  but is internally reconfigured to be separable.</span>

<span class="sd">  paper: https://arxiv.org/abs/1610.02357</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SeparableConv2DLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SeparableConv2DLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;depth_multiplier&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;Number of depthwise convolution output channels per input channel. &#39;</span>
        <span class="s1">&#39;The total number of depthwise convolution output channels will be.&#39;</span>
        <span class="s1">&#39;equal to in_channel * depth_multiplier.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;depthwise_tpl&#39;</span><span class="p">,</span>
             <span class="n">DepthwiseConv2DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;NONE&#39;</span><span class="p">),</span>
             <span class="s1">&#39;Template for the depthwise conv sub-layer.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="c1"># Rewrite the filter.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">cout</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">filter_shape</span>
    <span class="n">params</span><span class="o">.</span><span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cin</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">depth_multiplier</span><span class="p">,</span> <span class="n">cout</span><span class="p">)</span>
    <span class="n">depthwise_filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">depth_multiplier</span><span class="p">)</span>

    <span class="c1"># Dilation rate and stride go to the depthwise layer and reset ours.</span>
    <span class="n">depthwise_filter_stride</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">filter_stride</span>
    <span class="n">depthwise_dilation_rate</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">dilation_rate</span>
    <span class="n">params</span><span class="o">.</span><span class="n">filter_stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">dilation_rate</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">del</span> <span class="n">params</span>

    <span class="c1"># Create the depthwise sub-layer.</span>
    <span class="n">depthwise_params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">depthwise_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">filter_shape</span><span class="o">=</span><span class="n">depthwise_filter_shape</span><span class="p">,</span>
        <span class="n">filter_stride</span><span class="o">=</span><span class="n">depthwise_filter_stride</span><span class="p">,</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="n">depthwise_dilation_rate</span><span class="p">,</span>
        <span class="n">causal_convolution</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">causal_convolution</span><span class="p">,</span>
        <span class="n">weight_norm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">,</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span>
        <span class="n">bn_decay</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bn_decay</span><span class="p">,</span>
        <span class="n">bn_fold_weights</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span><span class="p">)</span>
    <span class="n">depthwise_params</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;depthwise_conv&#39;</span><span class="p">,</span> <span class="n">depthwise_params</span><span class="p">)</span>

<div class="viewcode-block" id="SeparableConv2DLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SeparableConv2DLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
                                                 <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span></div>

<div class="viewcode-block" id="SeparableConv2DLayer.OutShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SeparableConv2DLayer.OutShape">[docs]</a>  <span class="k">def</span> <span class="nf">OutShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the output shape given the input shape.&quot;&quot;&quot;</span>
    <span class="n">in_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">depthwise_conv</span><span class="o">.</span><span class="n">OutShape</span><span class="p">(</span><span class="n">in_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">OutShape</span><span class="p">(</span><span class="n">in_shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ProjectionLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer">[docs]</a><span class="k">class</span> <span class="nc">ProjectionLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Projection layer, with batch normalization and relu activation.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="ProjectionLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Activation function to use. Options are RELU, RELU6, SIGMOID, &#39;</span>
        <span class="s1">&#39;TANH, NONE.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Whether or not to apply batch norm.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_bias&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether or not to introduce the bias params to the layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bias_init&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Initial value for the bias&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;affine_last&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, apply the affine transformation as the last step, i.e., &#39;</span>
        <span class="s1">&#39;first apply batch normalization on the input, followed &#39;</span>
        <span class="s1">&#39;by activation, and finally the affine transformation. &#39;</span>
        <span class="s1">&#39;Otherwise, apply affine transformation first, followed by batch &#39;</span>
        <span class="s1">&#39;normalization and activation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weight_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;If true, apply weight normalization to weights as proposed by&#39;</span>
        <span class="s1">&#39; Salimans and Kingma, 2016: https://arxiv.org/abs/1602.07868&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;bn_fold_weights&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Fold the batch norm parameters into the convolution weights at &#39;</span>
        <span class="s1">&#39;eval/inference time as per https://arxiv.org/pdf/1712.05877.pdf. &#39;</span>
        <span class="s1">&#39;Defaults to None which means that it will be disabled by default &#39;</span>
        <span class="s1">&#39;and enabled when quantized training is enabled. Not compatible with &#39;</span>
        <span class="s1">&#39;affine_last=True&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bn_params&#39;</span><span class="p">,</span>
             <span class="n">BatchNormLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.999</span><span class="p">),</span>
             <span class="s1">&#39;Default params for batch norm layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;apply_pruning&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to prune the weights while training&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;pruning_hparams_dict&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Pruning related hyperparameters. A dict &#39;</span>
        <span class="s1">&#39;with hyperparameter: value pairs. See google-research.model_pruning.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_einsum&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use tf.einsum for optimizing &#39;</span>
        <span class="s1">&#39;computations. When this is set to False, this causes an increase in &#39;</span>
        <span class="s1">&#39;TPU memory usage (b/158336491).  When this is set to True, it might &#39;</span>
        <span class="s1">&#39; cause problems with model quantization for on device inference &#39;</span>
        <span class="s1">&#39;(b/146421936)&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_blocked_matmul&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use blocked matrix &#39;</span>
        <span class="s1">&#39;multiplications. This allows for weight updates to be paralellized &#39;</span>
        <span class="s1">&#39;across the cores for Shampoo optimizer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;block_dim&#39;</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="s1">&#39;Dimension of the block&#39;</span><span class="p">)</span>
    <span class="c1"># Non-default quantization behaviour for weights.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Quantization domain for the weights.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;xla_num_partitions&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Obsolete. Kept for backwards compatibility.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">STATIC_VALUES</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">STATIC_VALUES</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;NONE&#39;</span> <span class="ow">or</span> <span class="n">activations</span><span class="o">.</span><span class="n">IsSupported</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">xla_num_partitions</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s1">&#39;ProjectionLayer.batch_norm not set explicitly for </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s1">&#39;Projection layer enables both batch_norm and has_bias. &#39;</span>
          <span class="s1">&#39;This is generally redundant/wasteful and may introduce &#39;</span>
          <span class="s1">&#39;accuracy problems in some inference scenarios.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_bn_folded</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;bn_fold_weights requires use_blocked_matmul = False&#39;</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">affine_last</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;Folded batchnorm is not compatible with affine_last&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_einsum</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;use_einsum requires use_blocked_matmul = False&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">,</span> <span class="p">(</span>
          <span class="s1">&#39;Enabling xla_sharding requires use_blocked_matmul = False&#39;</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="n">bn_params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bn_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">bn_params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
      <span class="n">bn_params</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">affine_last</span> <span class="k">else</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="n">bn_params</span><span class="p">)</span>
    <span class="c1"># TODO(yonghui): implement the variational noise logic.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
        <span class="s1">&#39;projection_aqt&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">],</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">compression_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># only apply compression on tall matrices (input_dim &gt; output_dim)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">apply_compression</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">&gt;</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>

<div class="viewcode-block" id="ProjectionLayer._GetBlockedMatMulInputOutputMultipliers"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._GetBlockedMatMulInputOutputMultipliers">[docs]</a>  <span class="k">def</span> <span class="nf">_GetBlockedMatMulInputOutputMultipliers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get number of input and output blocks.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Number of input and output blocks.</span>
    <span class="n">w_im</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span>
    <span class="n">w_om</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span>
    <span class="c1"># Add padding if input_dim / output_dim is not divisible by block_dim.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">w_im</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">w_om</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">w_im</span><span class="p">,</span> <span class="n">w_om</span></div>

<div class="viewcode-block" id="ProjectionLayer._GetBlockedWeightMatrix"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._GetBlockedWeightMatrix">[docs]</a>  <span class="k">def</span> <span class="nf">_GetBlockedWeightMatrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># w is 3D Tensor of shape [i * o, block_dim, block_dim] such that</span>
    <span class="c1"># i * block_dim = num_inputs (modulo padding).</span>
    <span class="c1"># j * block_dim = num_outputs</span>
    <span class="c1">#</span>
    <span class="c1"># To efficiently apply forward prop, we transpose and reshape w into</span>
    <span class="c1"># shape [i * block_dim, o, block_dim]</span>
    <span class="n">w_im</span><span class="p">,</span> <span class="n">w_om</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetBlockedMatMulInputOutputMultipliers</span><span class="p">()</span>
    <span class="n">block_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span>
    <span class="n">w_4d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">w_im</span><span class="p">,</span> <span class="n">w_om</span><span class="p">,</span> <span class="n">block_dim</span><span class="p">,</span> <span class="n">block_dim</span><span class="p">])</span>
    <span class="c1"># Transpose to [i, block_dim, o, block_dim].</span>
    <span class="n">w_4d_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w_4d</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w_4d_t</span><span class="p">,</span> <span class="p">[</span><span class="n">w_im</span> <span class="o">*</span> <span class="n">block_dim</span><span class="p">,</span> <span class="n">w_om</span><span class="p">,</span> <span class="n">block_dim</span><span class="p">])</span>
    <span class="c1"># Slice out padding from the weight matrix.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">w_om</span><span class="p">,</span> <span class="n">block_dim</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">w</span></div>

<div class="viewcode-block" id="ProjectionLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">:</span>
      <span class="n">w_im</span><span class="p">,</span> <span class="n">w_om</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetBlockedMatMulInputOutputMultipliers</span><span class="p">()</span>
      <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">w_im</span> <span class="o">*</span> <span class="n">w_om</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_pruning</span><span class="p">:</span>
      <span class="n">mask_w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span><span class="n">w_pc</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                        <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
                                        <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">threshold_w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">([],</span>
                                             <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
                                             <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">b_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bias_init</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">bias_split_dims_mapping</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
      <span class="n">g_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="n">weights_var_name</span> <span class="o">=</span> <span class="s1">&#39;w&#39;</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_pruning</span><span class="p">:</span>
      <span class="n">mask_var_name</span> <span class="o">=</span> <span class="s1">&#39;mask&#39;</span>
      <span class="n">threshold_var_name</span> <span class="o">=</span> <span class="s1">&#39;threshold&#39;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="n">mask_var_name</span><span class="p">,</span> <span class="n">mask_w_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="n">threshold_var_name</span><span class="p">,</span> <span class="n">threshold_w_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">MaskWeightFn</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">mask_var_name</span><span class="p">),</span> <span class="s1">&#39;masked_w&#39;</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">weights_var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="n">MaskWeightFn</span><span class="p">)</span>
      <span class="n">pruning_utils</span><span class="o">.</span><span class="n">AddToPruningCollections</span><span class="p">(</span>
          <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">weights_var_name</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span>
                                                        <span class="n">mask_var_name</span><span class="p">),</span>
          <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">threshold_var_name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">weights_var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">ApplyPruning</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span>
                                           <span class="n">weights_var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                           <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">compression_op</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetLastCompressionOp</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">b_pc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">g_pc</span><span class="p">)</span>

    <span class="c1"># Determine quantization needs based on whether fusing activation</span>
    <span class="c1"># or not.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_qt_name</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;activation&#39;</span>
                            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span> <span class="k">else</span> <span class="s1">&#39;affine_matmul&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span> <span class="ow">and</span>
        <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TFLITE_FUSED_ACTIVATION_NAMES</span><span class="p">):</span>
      <span class="c1"># Not a fused activation function.</span>
      <span class="c1"># Need a qtensor to track the pre-activation tensor. The name is</span>
      <span class="c1"># compatible with older checkpoints.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span> <span class="o">=</span> <span class="s1">&#39;affine_matmul&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_qt_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="ProjectionLayer._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Backwards compatibility: manually call child.InstantiateVariables()</span>
    <span class="c1"># outside of tf.variable_scope(p.name).</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

<div class="viewcode-block" id="ProjectionLayer.NumOutputNodes"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer.NumOutputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">NumOutputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_qt_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Name of QTensor used for the output value.</span>

<span class="sd">    Useful for grabbing the quantization of the output.</span>

<span class="sd">    Returns:</span>
<span class="sd">      String name of output qtensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_qt_name</span>

<div class="viewcode-block" id="ProjectionLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply projection to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor.  Shaped [..., input_dim].</span>
<span class="sd">      paddings: The paddings tensor.  Shaped [..., 1], where all but the last</span>
<span class="sd">        dimension match.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output after applying projection, and optionally batch normalization and</span>
<span class="sd">      relu non-linearity.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">[</span>
            <span class="s1">&#39;compression_option&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">9</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_compression</span><span class="p">:</span>
          <span class="c1"># compression_option 9 corresponds to input compression</span>
          <span class="c1"># redirect w to point to c</span>
          <span class="n">w</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">c_matrix_tfvar</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">affine_last</span><span class="p">:</span>
        <span class="c1"># Reversed computation. Does not handle folding.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
          <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckNumerics</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
          <span class="n">out</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyProjectionKernel</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">with_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Normal ordered projection.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_bn_folded</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
          <span class="c1"># Everything folded together. This is the only variant that supports</span>
          <span class="c1"># quantization.</span>
          <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyProjectionKernel</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">quant</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Projection kernel(no activation fn) -&gt; BN -&gt; Activation fn.</span>
          <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyProjectionKernel</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">with_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
              <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckNumerics</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">QRPadding</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="n">out</span><span class="p">)</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_is_bn_folded</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Whether batchnorm folded weights are effectively enabled.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">))</span>

<div class="viewcode-block" id="ProjectionLayer._GetWeights"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._GetWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_GetWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the weights for the computation.</span>

<span class="sd">    Weights will always have weight_norm applied and may have batch_norm</span>
<span class="sd">    folded if enabled.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: Inputs (needed for batchnorm folding).</span>
<span class="sd">      paddings: Paddings (needed for batchnorm folding).</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tuple of (w, b) to use for the forward pass. b may be None if bias is</span>
<span class="sd">      disabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">:</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetBlockedWeightMatrix</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">theta</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                       <span class="n">py_utils</span><span class="o">.</span><span class="n">ToStaticShape</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">]))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_bn_folded</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

    <span class="c1"># If batch norm is fused with weights, then compute the weights as from</span>
    <span class="c1"># figure C.8 of https://arxiv.org/pdf/1712.05877.pdf for training and</span>
    <span class="c1"># figure C.6 for eval.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="c1"># Gets current moments without updating.</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">GetCurrentMoments</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Updates moments based on a trial run of the kernel (without activation</span>
      <span class="c1"># function).</span>
      <span class="n">raw_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyProjectionKernel</span><span class="p">(</span>
          <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">with_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">ComputeAndUpdateMoments</span><span class="p">(</span>
          <span class="n">theta</span><span class="o">.</span><span class="n">bn</span><span class="p">,</span> <span class="n">raw_output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>

    <span class="c1"># Fold weights and bias.</span>
    <span class="n">sigma_recip</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="n">scale_correction</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">sigma_recip</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">scale_correction</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">mean</span> <span class="o">*</span> <span class="n">sigma_recip</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span></div>

<div class="viewcode-block" id="ProjectionLayer._ApplyProjectionKernel"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._ApplyProjectionKernel">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyProjectionKernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">w</span><span class="p">,</span>
                             <span class="n">b</span><span class="p">,</span>
                             <span class="n">inputs</span><span class="p">,</span>
                             <span class="n">with_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">quant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">bn</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies matmul/bias/activation in one step.</span>

<span class="sd">    Note that it is important that these three ops be computed in this way as</span>
<span class="sd">    downstream inference engines (esp. for quantized inference) can recognize</span>
<span class="sd">    and fuse them. For floating point, this is an optimization, but for</span>
<span class="sd">    quantization, it is required.</span>

<span class="sd">    Args:</span>
<span class="sd">      w: Weight matrix.</span>
<span class="sd">      b: Bias vector (or None).</span>
<span class="sd">      inputs: FProp inputs.</span>
<span class="sd">      with_activation: Whether to also compute the activation function.</span>
<span class="sd">      quant: Whether to apply quantization.</span>
<span class="sd">      bn: Apply batchnorm.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor reshaped.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_blocked_matmul</span><span class="p">:</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span><span class="s1">&#39;projection_aqt&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_einsum</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_compression</span><span class="p">:</span>
          <span class="n">out</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetProjectLastDim</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ProjectLastDim</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ToStaticShape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])),</span> <span class="n">w</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;projection_aqt&#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ToStaticShape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]))</span>
      <span class="c1"># TODO(shivaniagrawal): There are the following dimmensions: bn, nmk, the</span>
      <span class="c1"># the correct thing to do here might be scaling on every m and every k,</span>
      <span class="c1"># while we are doing every k only.</span>
      <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span><span class="s1">&#39;projection_aqt&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bn,nmk-&gt;bmk&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;projection_aqt&#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
      <span class="c1"># Create an output layer [b, num_outputs].</span>
      <span class="n">bsz</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">out</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">[</span><span class="n">bsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">block_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">bsz</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">out_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">+=</span> <span class="n">b</span>  <span class="c1"># NOTE: Bias on matmul is never quantized.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                 <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyActivationFunction</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">with_activation</span><span class="p">,</span> <span class="n">quant</span><span class="p">)</span></div>

<div class="viewcode-block" id="ProjectionLayer._ApplyActivationFunction"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer._ApplyActivationFunction">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyActivationFunction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">out</span><span class="p">,</span>
                               <span class="n">inputs</span><span class="p">,</span>
                               <span class="n">with_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">quant</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies the activation function in one step.</span>

<span class="sd">    Args:</span>
<span class="sd">      out: The result of applying the weight matrix (and bias) to the inputs.</span>
<span class="sd">      inputs: FProp inputs.</span>
<span class="sd">      with_activation: Whether to also compute the activation function.</span>
<span class="sd">      quant: Whether to apply quantization.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor reshaped.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">with_activation</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span> <span class="o">!=</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span><span class="p">:</span>
        <span class="c1"># Track quantization for unfused activation function.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pre_activation_qt_name</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckNumerics</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">quant</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_qt_name</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_einsum</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">out</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
              <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="n">py_utils</span><span class="o">.</span><span class="n">ToStaticShape</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">])</span>
          <span class="p">],</span>
                    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="ProjectionLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ProjectionLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">assert</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">in_dim</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">other_dims</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">/</span> <span class="n">in_dim</span>
    <span class="c1"># matmuls.</span>
    <span class="n">flops</span> <span class="o">+=</span> <span class="n">other_dims</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="c1"># activations.</span>
    <span class="n">flops</span> <span class="o">+=</span> <span class="n">other_dims</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">*</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFlops</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
      <span class="n">flops</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
      <span class="n">bn_meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">bn_params</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">bn_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span> <span class="n">out_shape</span><span class="p">)</span>
      <span class="n">flops</span> <span class="o">+=</span> <span class="n">bn_meta</span><span class="o">.</span><span class="n">flops</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span><span class="p">:</span>
      <span class="c1"># l2 normalize + element-wise multiply.</span>
      <span class="n">flops</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">+</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">out_shape</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="FCLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FCLayer">[docs]</a><span class="k">class</span> <span class="nc">FCLayer</span><span class="p">(</span><span class="n">ProjectionLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fully-connected layer (matmul + bias + optional activation).&quot;&quot;&quot;</span>

<div class="viewcode-block" id="FCLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FCLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">p</span></div></div>


<div class="viewcode-block" id="FeedForwardNet"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FeedForwardNet">[docs]</a><span class="k">class</span> <span class="nc">FeedForwardNet</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simple multiple layer feedforward network.</span>

<span class="sd">  This class represents a stack of fully connected feedforward network. Each</span>
<span class="sd">  layer in the network can be configured for whether or not to have batch-norm</span>
<span class="sd">  applied to its output, its activation function, whether or not to apply</span>
<span class="sd">  dropout to post-activation output.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="FeedForwardNet.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FeedForwardNet.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input to the network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_layer_dims&#39;</span><span class="p">,</span> <span class="p">[],</span> <span class="s1">&#39;Depth of the hidden layer outputs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;projection&#39;</span><span class="p">,</span> <span class="n">ProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Projection layer params. A single parameter that will be shared by&#39;</span>
        <span class="s1">&#39;all layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Dropout layer params. Can be a single params or a tuple/list of params&#39;</span>
        <span class="s1">&#39; having the same length as the number of layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether or not to apply BN to hidden layer output. &#39;</span>
        <span class="s1">&#39;This can be a single bool or a tuple/list of bools having the&#39;</span>
        <span class="s1">&#39; same length as the number of layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
        <span class="s1">&#39;The activation function to use. Can be a single string, or a&#39;</span>
        <span class="s1">&#39; tuple/list of strings having the same length as the number&#39;</span>
        <span class="s1">&#39; of layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;has_bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Whether or not to use bias for projection layers.&#39;</span>
        <span class="s1">&#39;This can be a None, single bool or a tuple/list of bools having the &#39;</span>
        <span class="s1">&#39;same length as the number of layers. If None, the has_bias is set to &#39;</span>
        <span class="s1">&#39;True whenever batch_norm is False for each projection layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weight_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether or not to apply weight normalization to weights. This can be &#39;</span>
        <span class="s1">&#39;a single bool or a tuple/list of bools having the same length as the &#39;</span>
        <span class="s1">&#39;number of layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;skip_connections&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Must be None.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;bn_fold_weights&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Force folding the batch normalization &#39;</span>
        <span class="s1">&#39;weights in the projection layer.&#39;</span><span class="p">)</span>
    <span class="c1"># TODO(rpang): retire weight_split_dims_mapping_list and</span>
    <span class="c1"># activation_split_dims_mapping_list. Use</span>
    <span class="c1"># {weight,activation}_split_dims_mapping (defined in BaseLayer) instead.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weight_split_dims_mapping_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;A list of weight_split_dims_mapping for each sub-layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;activation_split_dims_mapping_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;A list of activation_split_dims_mapping for each sub-layer.&#39;</span><span class="p">)</span>
    <span class="c1"># Non-default quantization behaviour for the weights.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Quantization domain for the weights.&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">skip_connections</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="n">batch_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_norm</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">batch_norm</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_norm</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
    <span class="n">weight_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_norm</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_norm</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">weight_norm</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight_norm</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>

    <span class="n">activation</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
      <span class="n">activation</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="n">has_bias</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">has_bias</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">has_bias</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">has_bias</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">has_bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">has_bias</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
    <span class="c1"># Set has_bias to (not batch_norm) if None.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">has_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">has_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="ow">not</span> <span class="n">batch_norm</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">params_dropout_layers</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params_dropout_layers</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dropout_layers</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">params_dropout_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">params_dropout_layers</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weight_split_dims_mapping_list</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping_list</span>
      <span class="n">activation_split_dims_mapping_list</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping_list</span>
      <span class="k">if</span> <span class="n">activation_split_dims_mapping_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation_split_dims_mapping_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">weight_split_dims_mapping_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
      <span class="n">activation_split_dims_mapping_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_split_dims_mapping_list</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_split_dims_mapping_list</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_layers</span>

    <span class="c1"># Residual connections work better in the form of:</span>
    <span class="c1">#   y = x + Affine(Activation(BatchNorm(x)))</span>
    <span class="n">params_fc_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">out_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">proj_out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
      <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
      <span class="n">params_i</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">weight_norm</span><span class="o">=</span><span class="n">weight_norm</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">has_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span>
          <span class="n">output_dim</span><span class="o">=</span><span class="n">proj_out_dim</span><span class="p">,</span>
          <span class="n">bn_fold_weights</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bn_fold_weights</span><span class="p">,</span>
          <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
          <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">weight_split_dims_mapping_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">activation_split_dims_mapping</span><span class="o">=</span><span class="n">activation_split_dims_mapping_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="n">params_fc_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params_i</span><span class="p">)</span>
      <span class="n">in_dim</span> <span class="o">=</span> <span class="n">out_dim</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params_i</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params_i</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;fc&#39;</span><span class="p">,</span> <span class="n">params_fc_layers</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">params_dropout_layers</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns output dimension of the FeedForwardNet.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<div class="viewcode-block" id="FeedForwardNet.FPropAllLayers"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FeedForwardNet.FPropAllLayers">[docs]</a>  <span class="k">def</span> <span class="nf">FPropAllLayers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;FProp, returns all layers including the input and output layers.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">)</span>
    <span class="n">in_dim</span><span class="p">,</span> <span class="n">layer_in</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">inputs</span>
    <span class="n">all_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer_in</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">layer_in</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">layer_in</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
                                      <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">in_dim</span><span class="p">)])</span>
      <span class="p">],</span> <span class="n">layer_in</span><span class="p">)</span>
      <span class="n">out_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">layer_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layer_in</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
      <span class="n">layer_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layer_out</span><span class="p">)</span>
      <span class="n">all_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_out</span><span class="p">)</span>
      <span class="n">layer_in</span> <span class="o">=</span> <span class="n">layer_out</span>
      <span class="n">in_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
    <span class="k">return</span> <span class="n">all_layers</span></div>

<div class="viewcode-block" id="FeedForwardNet.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FeedForwardNet.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FPropAllLayers</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></div>

<div class="viewcode-block" id="FeedForwardNet.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FeedForwardNet.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">assert</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>  <span class="c1"># throw-away graph.</span>
      <span class="n">instance</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Instantiate</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">fc</span> <span class="ow">in</span> <span class="n">instance</span><span class="o">.</span><span class="n">fc</span><span class="p">:</span>
        <span class="n">proj_params</span> <span class="o">=</span> <span class="n">fc</span><span class="o">.</span><span class="n">params</span>
        <span class="n">proj_shape</span> <span class="o">=</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">proj_params</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
        <span class="n">proj_meta</span> <span class="o">=</span> <span class="n">proj_params</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">proj_params</span><span class="p">,</span> <span class="n">proj_shape</span><span class="p">)</span>
        <span class="n">flops</span> <span class="o">+=</span> <span class="n">proj_meta</span><span class="o">.</span><span class="n">flops</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">out_shape</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="StackingOverTime"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.StackingOverTime">[docs]</a><span class="k">class</span> <span class="nc">StackingOverTime</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Stacking applied along the time axis.</span>

<span class="sd">     At each time step of an input sequence, elements are stacked over the</span>
<span class="sd">     window of (&#39;left_context&#39; + 1 + &#39;right_context&#39;) steps around the current</span>
<span class="sd">     time step. Zeros will be padded to the left or right of the sequence for</span>
<span class="sd">     elements around the boundaries. Finally the stacked outputs are emitted</span>
<span class="sd">     once every &#39;stride&#39; steps.</span>

<span class="sd">     E.g. if an input sequence is: [4], [1], [9], [3], [5], [2], [8]</span>
<span class="sd">     left_context = 1, right_context = 1, stride = 3,</span>
<span class="sd">     then the output sequence would be: [0, 4, 1], [9, 3, 5], [2, 8, 0]</span>

<span class="sd">     Note that this layer only performs tensor transformation, so there are no</span>
<span class="sd">     learnable parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="StackingOverTime.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.StackingOverTime.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;left_context&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Number of time steps to stack on the left to the central step.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;right_context&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Number of time steps to stack on the right to the central step.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;The stride for emitting the stacked output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pad_with_left_frame&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to use the left frame for padding instead of 0s.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">&gt;=</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">&gt;=</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">&gt;=</span> <span class="mi">1</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">window_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stacking window size.</span>

<span class="sd">    The output dimension will be window_size * the input dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Window size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">+</span> <span class="mi">1</span>

<div class="viewcode-block" id="StackingOverTime._ApplyStack"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.StackingOverTime._ApplyStack">[docs]</a>  <span class="k">def</span> <span class="nf">_ApplyStack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The core function to apply the stacking to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: [batch, time, depth].</span>
<span class="sd">      pad_value: the padding value for left/right context.</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch, ceil(time / stride), depth * stacking_window_length] tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">inputs_max_len</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pad_with_left_frame</span><span class="p">:</span>
        <span class="n">left_pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">repeats</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">left_pad</span><span class="p">,</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
            <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_value</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Add zero paddings to the left and right of the input sequence.</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">left_context</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
            <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_value</span><span class="p">)</span>

      <span class="c1"># Make window_size() copies of the padded sequence with the original</span>
      <span class="c1"># sequence length, where each copy is offset by 1 time step.</span>
      <span class="n">pieces</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">):</span>
        <span class="n">pieces</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">inputs_max_len</span><span class="p">])</span>
      <span class="c1"># Apply stacking.</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">pieces</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Apply striding.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">::</span><span class="n">p</span><span class="o">.</span><span class="n">stride</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="StackingOverTime.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.StackingOverTime.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply the stacking to inputs along the time axis.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        feature].</span>
<span class="sd">      paddings: The paddings tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        1], where all but the last dimension match inputs. Each value is 0 or 1</span>
<span class="sd">        indicating whether a time step of a sequence is padded in the inputs to</span>
<span class="sd">        reach the max length in the batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">      (outputs, out_paddings) pair.</span>
<span class="sd">        outputs is of shape [batch, ceil(time / stride), feature * stacking].</span>
<span class="sd">        out_paddings is of shape [batch, ceil(time / stride), 1]. out_paddings</span>
<span class="sd">        will be 0 if any of the corresponding input padding is 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="c1"># Checks the inputs shape has 3 dimensions.</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="c1"># Checks the paddings shape has 3 dimensions, and the last one is 1.</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="c1"># Checks the first two dimensions of inputs and paddings match.</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">],</span>
        <span class="n">inputs</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyStack</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

      <span class="c1"># Stack the padding values with the same context and stride parameters.</span>
      <span class="c1"># Then take the minimum padding values within each stacking window, since</span>
      <span class="c1"># an output time step becomes a padded one only if all of the underlying</span>
      <span class="c1"># stacked steps are padded ones.</span>
      <span class="n">out_paddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ApplyStack</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">pad_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">out_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">out_paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">out_paddings</span></div>

<div class="viewcode-block" id="StackingOverTime.Unstack"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.StackingOverTime.Unstack">[docs]</a>  <span class="k">def</span> <span class="nf">Unstack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stacked</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inverts stacking over time.</span>

<span class="sd">    Given &#39;stacked&#39; outputs from this StackingOverTime layer,</span>

<span class="sd">      stacked, _ = this_layer.FProp(inputs),</span>

<span class="sd">    this method attempts to reconstruct the original &#39;inputs&#39;.</span>

<span class="sd">    If stride &gt; window_size, the original input cannot be recovered, and a</span>
<span class="sd">    ValueError is raised.</span>

<span class="sd">    Otherwise, if right_context + 1 &gt;= stride, this method returns a Tensor that</span>
<span class="sd">    is identical to &#39;inputs&#39; but potentially longer due to paddings.</span>

<span class="sd">    If right_context + 1 &lt; stride, this method returns a Tensor that may be up</span>
<span class="sd">    to ```stride - right_context - 1``` frames shorter than the original input,</span>
<span class="sd">    but identical in the frames that are returned. e.g.::</span>

<span class="sd">      left_context = 2, right_context = 1, stride = 4</span>
<span class="sd">      input sequence:     1 2 3 4 5 6 7 8</span>
<span class="sd">      after padding:  0 0 1 2 3 4 5 6 7 8 0</span>
<span class="sd">      windows:</span>
<span class="sd">        [0 0 (1) 2] 3 4 5 6 7 8 0</span>
<span class="sd">         0 0 1 2 [3 4 (5) 6] 7 8 0</span>
<span class="sd">      stacked:</span>
<span class="sd">        [[0 0 1 2], [3 4 5 6]]</span>
<span class="sd">      unstacked:</span>
<span class="sd">        [1 2 3 4 5 6], which is 4 - 1 - 1 = 2 (stride - right_context - 1)</span>
<span class="sd">        frames shorter than the original input.</span>

<span class="sd">    `Unstack()` can be used to project the outputs of downstream layers back to</span>
<span class="sd">    the shape of the original unstacked inputs. For example::</span>

<span class="sd">        inputs = ...  # [batch, length, input_dim]</span>
<span class="sd">        # [batch, ceil(length / stride), rnn_dim]</span>
<span class="sd">        rnn_out = rnn.FProp(stacking.FProp(inputs)[0])</span>
<span class="sd">        # [batch, length, rnn_dim]</span>
<span class="sd">        back_projected_rnn_out = py_utils.PadOrTrimTo(</span>
<span class="sd">            stacking.Unstack(tf.tile(rnn_out, [1, 1, stacking.window_size])),</span>
<span class="sd">            py_utils.GetShape(inputs))</span>

<span class="sd">    Note this method does not take or return a separate padding tensor. The</span>
<span class="sd">    caller is responsible for knowing which of outputs are padding (e.g. based</span>
<span class="sd">    on the padding of the original FProp inputs).</span>

<span class="sd">    Args:</span>
<span class="sd">      stacked: Tensor of shape [batch, time, window_size * feature_dim], assumed</span>
<span class="sd">        to be the output of `FProp`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The reconstructed input Tensor, with shape</span>
<span class="sd">      [batch, (frames - 1) * stride + right_context + 1, feature_dim].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if stride &gt; window_size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Can&#39;t invert StackingOverTime with stride (</span><span class="si">%d</span><span class="s2">) &gt; window_size (</span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">))</span>

    <span class="c1"># Reshape to allow indexing individual frames within each stacked window.</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">stacked_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">stacked</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span>
                         <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">stacked_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Compute the index of the window and frame in &#39;stacked&#39; where each frame of</span>
    <span class="c1"># the original input is located, and extract them with tf.gather_nd.</span>
    <span class="c1"># First compute for all except the last window, since these elements have</span>
    <span class="c1"># the potential of being looked up from the next window.</span>
    <span class="n">input_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">stacked_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">input_indices</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span>
    <span class="n">in_next_window</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">window_index</span> <span class="o">=</span> <span class="n">input_indices</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="n">in_next_window</span>
    <span class="n">frame_index</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">+</span> <span class="n">mod</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">stride</span> <span class="o">*</span> <span class="n">in_next_window</span>
    <span class="c1"># Now handle the last window explicitly and concatenate onto the existing</span>
    <span class="c1"># window_index/frame_index tensors.</span>
    <span class="n">last_window_length</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">right_context</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">window_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">window_index</span><span class="p">,</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">last_window_length</span><span class="p">],</span> <span class="n">stacked_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">frame_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">frame_index</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">left_context</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">last_window_length</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Stack the indices for tf.gather_nd.</span>
    <span class="n">window_and_frame_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">window_index</span><span class="p">,</span> <span class="n">frame_index</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">window_and_frame_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">window_and_frame_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="n">window_and_frame_indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="PoolingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer">[docs]</a><span class="k">class</span> <span class="nc">PoolingLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pooling layer, by default performs max-pooling.</span>

<span class="sd">  Quantization notes: Unlike the common pattern, the pooling layer inputs</span>
<span class="sd">  and output must be quantized to the same range, so it tracks both (vs</span>
<span class="sd">  just the output). The preceding layer must have its output quantization</span>
<span class="sd">  disabled.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="PoolingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;window_shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Window shape. Must be a pair of ints. Elements are in&#39;</span>
        <span class="s1">&#39; the order of height (time), width (frequency).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;window_stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Window stride to use. Must be a pair of ints. The first int&#39;</span>
        <span class="s1">&#39; specifies the stride on the time dimension. The second int&#39;</span>
        <span class="s1">&#39; specifies the stride on the frequency dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pooling_type&#39;</span><span class="p">,</span> <span class="s1">&#39;MAX&#39;</span><span class="p">,</span> <span class="s1">&#39;Pooling type: MAX|AVG&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;padding_algorithm&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Padding algorithm. See the &quot;returns&quot; section of &#39;</span>
        <span class="s1">&#39;`tf.nn.convolution` for details. &#39;</span>
        <span class="s1">&#39;Roughly, VALID = NO_PADDING and SAME (default) = PAD INPUT&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">window_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">window_stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">window_shape</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">window_stride</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">pooling_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;MAX&#39;</span><span class="p">,</span> <span class="s1">&#39;AVG&#39;</span><span class="p">]</span>

<div class="viewcode-block" id="PoolingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="PoolingLayer.OutputShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer.OutputShape">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">OutputShape</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">_ComputeConvOutputShape</span><span class="p">(</span>
        <span class="n">in_shape</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">window_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">p</span><span class="o">.</span><span class="n">window_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">padding_algorithm</span><span class="p">)</span></div>

<div class="viewcode-block" id="PoolingLayer.OutShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer.OutShape">[docs]</a>  <span class="k">def</span> <span class="nf">OutShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the output shape given the input shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">OutputShape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="PoolingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PoolingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">theta</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">,</span>
      <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
      <span class="n">paddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Apply pooling to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        frequency, channel]. The time dimension corresponds to the height</span>
<span class="sd">        dimension as in images and the frequency dimension corresponds to the</span>
<span class="sd">        width dimension as in images.</span>
<span class="sd">      paddings: The paddings tensor. It is expected to be of shape [batch,</span>
<span class="sd">        time]. Defaults to None, which means there no paddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An (output, paddings) tensor tuple if paddings is not None, else just</span>
<span class="sd">      output tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">window_stride</span>
    <span class="n">window</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">window_shape</span>
    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">))</span>
      <span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_padding</span> <span class="o">=</span> <span class="n">_ComputeConvOutputPadding</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">window</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                <span class="n">p</span><span class="o">.</span><span class="n">padding_algorithm</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pooling_type</span> <span class="o">==</span> <span class="s1">&#39;MAX&#39;</span><span class="p">:</span>
          <span class="c1"># Fill dtype.min in padded positions.</span>
          <span class="n">min_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">min</span>
          <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ApplyPadding</span><span class="p">(</span><span class="n">paddings</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                                         <span class="n">inputs</span><span class="p">,</span> <span class="n">min_value</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out_padding</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

      <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span>
          <span class="n">window</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">pooling_type</span><span class="p">,</span>
          <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
          <span class="n">padding</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">padding_algorithm</span><span class="p">,</span>
          <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">pooling_type</span> <span class="o">==</span> <span class="s1">&#39;AVG&#39;</span><span class="p">:</span>
        <span class="c1"># Count the fraction of non-padding elements inside each pooling window.</span>
        <span class="n">in_mask</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">paddings</span>
        <span class="n">non_padding_ratio</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span>
            <span class="n">in_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
            <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">window_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span>
            <span class="n">pooling_type</span><span class="o">=</span><span class="s1">&#39;AVG&#39;</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">window_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">padding_algorithm</span><span class="p">)</span>
        <span class="c1"># Divide by non-padding ratios to eliminate the effect of padded values.</span>
        <span class="n">out</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reciprocal_no_nan</span><span class="p">(</span><span class="n">non_padding_ratio</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

      <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">out_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">out_padding</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">out_padding</span>
      <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="BlurPoolLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BlurPoolLayer">[docs]</a><span class="k">class</span> <span class="nc">BlurPoolLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;BlurPool from https://arxiv.org/pdf/1904.11486.pdf.</span>

<span class="sd">  This layer blurs the input with a fixed filter and performs subsampling</span>
<span class="sd">  afterwards. Only supports 2x1 or 2x2 spatial reduction.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BlurPoolLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BlurPoolLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;blur_filter&#39;</span><span class="p">,</span> <span class="s1">&#39;B5&#39;</span><span class="p">,</span> <span class="s1">&#39;One of [R2, T3, B5]; the fixed blur filter.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;subsample_type&#39;</span><span class="p">,</span> <span class="s1">&#39;1D&#39;</span><span class="p">,</span> <span class="s1">&#39;Choose between [1D, 2D] subsampling.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_channels&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Number of input channels.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">blur_filter</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;R2&#39;</span><span class="p">,</span> <span class="s1">&#39;T3&#39;</span><span class="p">,</span> <span class="s1">&#39;B5&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">subsample_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;1D&#39;</span><span class="p">,</span> <span class="s1">&#39;2D&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_channels</span>

    <span class="n">filter_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;B5&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="s1">&#39;T3&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="n">base_filter</span> <span class="o">=</span> <span class="n">filter_dict</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">blur_filter</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">subsample_type</span> <span class="o">==</span> <span class="s1">&#39;2D&#39;</span><span class="p">:</span>
      <span class="n">base_filter</span> <span class="o">=</span> <span class="n">base_filter</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">base_filter</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">base_filter</span> <span class="o">=</span> <span class="n">base_filter</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">base_filter</span> <span class="o">/=</span> <span class="n">base_filter</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_blur_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">base_filter</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">conv_params</span> <span class="o">=</span> <span class="n">DepthwiseConv2DLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;NONE&#39;</span><span class="p">,</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">filter_stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">filter_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_blur_filter</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;blur_conv&#39;</span><span class="p">,</span> <span class="n">conv_params</span><span class="p">)</span>

<div class="viewcode-block" id="BlurPoolLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.BlurPoolLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">theta</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">,</span>
      <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
      <span class="n">paddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Apply blur pooling.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        frequency, channel]. The time dimension corresponds to the height</span>
<span class="sd">        dimension as in images and the frequency dimension corresponds to the</span>
<span class="sd">        width dimension as in images.</span>
<span class="sd">      paddings: The paddings tensor. It is expected to be of shape [batch,</span>
<span class="sd">        time]. Defaults to None, which means there no paddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An (output, paddings) tensor tuple if paddings is not None, else just</span>
<span class="sd">      output tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">))</span>
      <span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># blur</span>
    <span class="n">theta_cp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">blur_conv</span><span class="p">)</span>
    <span class="n">theta_cp</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_blur_filter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">out_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blur_conv</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta_cp</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>

    <span class="c1"># b/142399320</span>
    <span class="c1"># Use stride in blur conv for subsampling once non-square stride gets</span>
    <span class="c1"># supported.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">subsample_type</span> <span class="o">==</span> <span class="s1">&#39;2D&#39;</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">out_padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">out_padding</span> <span class="o">=</span> <span class="n">_ComputeConvOutputPadding</span><span class="p">(</span>
          <span class="n">out_padding</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_algorithm</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">out_padding</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">out_padding</span>
    <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="SingleShardEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">SingleShardEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Embedding layer that is not sharded.</span>

<span class="sd">  This embedding layer is expected to be replicated over all compute devices</span>
<span class="sd">  (e.g. tpu cores). It is intended to support small to medium embedding tables</span>
<span class="sd">  (&lt; 50k) only.</span>

<span class="sd">  This is intended to be a unification of EmbeddingLayer and</span>
<span class="sd">  SimpleEmbeddingLayer (and cleanup of both). It is targeting the most common</span>
<span class="sd">  use-case we have in speech/nmt/tts/deeprank. Currently we often first</span>
<span class="sd">  configure a model using EmbeddingLayer, and then call ChangeToSimpleEmbedding</span>
<span class="sd">  to switch to SimpleEmbedding where  we lose some configuration (e.g.</span>
<span class="sd">  scale_by_sqrt_dim).</span>

<span class="sd">  TODO(lingvo): Implement the matmul option which should be more efficient for</span>
<span class="sd">  small vocabs (e.g. &lt; 1k vocab).</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SingleShardEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num tokens in vocab.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(embedding_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

<div class="viewcode-block" id="SingleShardEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;emb_var&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="SingleShardEmbeddingLayer.EmbLookupDefaultTheta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer.EmbLookupDefaultTheta">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultTheta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="SingleShardEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple with the weight matrix for the embedding.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_id_validation&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">emb_var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">embs</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="SingleShardEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="EmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">EmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Embedding layer.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="EmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;max_num_shards&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num param shards.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;on_ps&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;True if to perform the embedding lookup on ps.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(embedding_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="c1"># Min number of params per shard.</span>
  <span class="n">MIN_PARAMS_PER_SHARD</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">256</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">max_num_shards</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

    <span class="n">total_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_actual_shards</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">max_num_shards</span><span class="p">,</span>
        <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">total_size</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">MIN_PARAMS_PER_SHARD</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
        <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actual_shards</span><span class="p">))</span>

<div class="viewcode-block" id="EmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="c1"># EmbeddingLayer handles vars/theta differently from other layers</span>
    <span class="c1"># because when embedding shards are placed on ps, it&#39;s more</span>
    <span class="c1"># efficiently to do embedding lookups on ps and sends the result</span>
    <span class="c1"># back to the worker.</span>
    <span class="n">emb_vars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">emb_shards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actual_shards</span><span class="p">):</span>
      <span class="n">var_name</span> <span class="o">=</span> <span class="s1">&#39;var_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
      <span class="n">emb_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">])</span>
      <span class="c1"># NOTE: self.theta[var_name] has transformations such as variational noise</span>
      <span class="c1"># applied via theta_fn in self.CreateVariable. For embedding layer we</span>
      <span class="c1"># apply variational noise explicitly in EmbLookup, so we do not use</span>
      <span class="c1"># self.theta[var_name] here.</span>
      <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">on_ps</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">)</span>
      <span class="n">emb_shards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="c1"># Remove from _private_vars / _private_thetas to be added later as wm.</span>
      <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
      <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">emb_vars</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">emb_shards</span></div>

<div class="viewcode-block" id="EmbeddingLayer.EmbLookupDefaultTheta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EmbeddingLayer.EmbLookupDefaultTheta">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultTheta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="EmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple with the weight matrix for the embedding.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_id_validation&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;vn&#39;</span><span class="p">):</span>
      <span class="n">embs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">embs</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SimpleEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">SimpleEmbeddingLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An embedding layer that is simple to compile (by XLA and Toco).</span>

<span class="sd">  The params use_matmul and use_gather control how the lookup is performed.</span>
<span class="sd">  If neither is True, then a loop is used to compute the embedding.</span>

<span class="sd">  This layer is &quot;simple&quot; in comparison to &#39;EmbeddingLayer&#39; in that it does</span>
<span class="sd">  not shard the embeddings.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SimpleEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Depth of the input. I.e., the number of classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_matmul&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, use a matmul to implement &#39;</span>
        <span class="s1">&#39;the embedding lookup. Depending on vocab_size and #ids, &#39;</span>
        <span class="s1">&#39;e.g., when vocab_size is small, use_matmul can be more &#39;</span>
        <span class="s1">&#39;efficient. On the other hand, use_matmul creates a 0/1 &#39;</span>
        <span class="s1">&#39;sparse matrix and hence may use more memory than the &#39;</span>
        <span class="s1">&#39;final output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;fprop_mode&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Sets the mode used for computing the fprop &#39;</span>
        <span class="s1">&#39;(different inference engines have different capabilities and this &#39;</span>
        <span class="s1">&#39;accomodates them). Can be &quot;loop&quot;, &quot;matmul&quot; or &quot;gather&quot;. If None, &#39;</span>
        <span class="s1">&#39;defaults to &quot;matmul&quot; if use_matmul or &quot;loop&quot; if false.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_3d_weight_tensor&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, and use_matmul is False,&#39;</span>
        <span class="s1">&#39;in TPU compatibility mode, we reshape the normal 2D weight&#39;</span>
        <span class="s1">&#39;tensor to [num_rows, embed_dim] to be &#39;</span>
        <span class="s1">&#39;[num_rows, embed_dim // 128, 128].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;apply_pruning&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to prune the weights while training&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(embedding_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;pruning_hparams_dict&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Pruning related hyperparameters. A dict &#39;</span>
        <span class="s1">&#39;with hyperparameter: value pairs. See google-research.model_pruning.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="n">valid_fprop_modes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;loop&#39;</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">,</span> <span class="s1">&#39;gather&#39;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_mode</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">=</span> <span class="s1">&#39;matmul&#39;</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_matmul</span> <span class="k">else</span> <span class="s1">&#39;gather&#39;</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="ow">in</span> <span class="n">valid_fprop_modes</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;fprop_mode must be one of </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">valid_fprop_modes</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">weight_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetWeightShape</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span><span class="s1">&#39;emb_aqt&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">compression_op</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">apply_compression</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<div class="viewcode-block" id="SimpleEmbeddingLayer._FpropImpl"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer._FpropImpl">[docs]</a>  <span class="k">def</span> <span class="nf">_FpropImpl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embs</span><span class="p">,</span> <span class="n">ids_vec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The embedding lookup implementation.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">emb_shape_suf</span><span class="p">,</span> <span class="n">weight_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetWeightShape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">EmbBprop</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Embedding backprop.</span>

<span class="sd">      Effectively, it computes:</span>
<span class="sd">        num = size of xs.ids_vec</span>
<span class="sd">        dembs = zeros_like(xs.embs)</span>
<span class="sd">        for i in range(num):</span>
<span class="sd">          dembs[xs.ids_vec[i], :] += dys[i, :]</span>
<span class="sd">        return dembs, zeros_like(xs.ids_vec)</span>

<span class="sd">      Args:</span>
<span class="sd">        xs: A NestedMap containing:</span>
<span class="sd">          - embs: The embedding matrix. Unused in the backprop.</span>
<span class="sd">          - ids_vec: A vector of int32 embedding ids.</span>
<span class="sd">        ys: Required by py_utils._DefineDefun, not used here.</span>
<span class="sd">        dys: A matrix of size (size of xs.ids_vec, embedding dims).</span>

<span class="sd">      Returns:</span>
<span class="sd">        A NestedMap containing:</span>

<span class="sd">          - embs: A matrix of the same shape of xs.embs. Gradients for xs.embs.</span>
<span class="sd">          - ids_vec: Zeros. Same shape as xs.ids_vec.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">del</span> <span class="n">ys</span>
      <span class="n">num</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">dembs</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">dys_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">dys</span><span class="p">)</span>
        <span class="n">dys</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dys</span><span class="p">,</span> <span class="p">[</span><span class="n">dys_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="n">emb_shape_suf</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">EmbBpropLoop</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># row_id = state.ids_vec[i]</span>
        <span class="n">row_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="c1"># row = state.drets[i]</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">drets</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">emb_shape_suf</span><span class="p">)</span>
        <span class="c1"># state.dembs[row_id] = row</span>
        <span class="n">state</span><span class="o">.</span><span class="n">dembs</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_add</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">dembs</span><span class="p">,</span> <span class="p">[</span><span class="n">row_id</span><span class="p">],</span> <span class="n">row</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

      <span class="n">dembs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ForLoop</span><span class="p">(</span>
          <span class="n">body</span><span class="o">=</span><span class="n">EmbBpropLoop</span><span class="p">,</span>
          <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">limit</span><span class="o">=</span><span class="n">num</span><span class="p">,</span>
          <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">loop_state</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">ids_vec</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">,</span> <span class="n">drets</span><span class="o">=</span><span class="n">dys</span><span class="p">,</span> <span class="n">dembs</span><span class="o">=</span><span class="n">dembs</span><span class="p">))</span><span class="o">.</span><span class="n">dembs</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
        <span class="n">dembs</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>

      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">embs</span><span class="o">=</span><span class="n">dembs</span><span class="p">,</span> <span class="n">ids_vec</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">ids_vec</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">EmbFprop</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Embedding forward prop.</span>

<span class="sd">      Effectively, it computes:</span>
<span class="sd">        num = size of xs.ids_vec</span>
<span class="sd">        rets = zeros([num, embedding dim])</span>
<span class="sd">        for i in range(num):</span>
<span class="sd">          rets[i, :] = xs.embs[xs.ids_vec[i], :]</span>
<span class="sd">        return rets</span>

<span class="sd">      Args:</span>
<span class="sd">        xs: A NestedMap containing:</span>
<span class="sd">          - embs: The embedding matrix.</span>
<span class="sd">          - ids_vec: A vector of int32 embedding ids.</span>

<span class="sd">      Returns:</span>
<span class="sd">        The result of embedding lookups. A matrix of shape</span>
<span class="sd">        [num ids in xs.ids_vec, embedding dims].</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">num</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">rets</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">num</span><span class="p">]</span> <span class="o">+</span> <span class="n">emb_shape_suf</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">EmbFpropLoop</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># row_id = state.ids_vec[i]</span>
        <span class="n">row_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="c1"># row = state.embs[row_id]</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span> <span class="n">row_id</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">emb_shape_suf</span><span class="p">)</span>
        <span class="c1"># state.rets[i] = row</span>
        <span class="n">state</span><span class="o">.</span><span class="n">rets</span> <span class="o">=</span> <span class="n">inplace_ops</span><span class="o">.</span><span class="n">alias_inplace_update</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">rets</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">row</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

      <span class="n">rets</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ForLoop</span><span class="p">(</span>
          <span class="n">body</span><span class="o">=</span><span class="n">EmbFpropLoop</span><span class="p">,</span>
          <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">limit</span><span class="o">=</span><span class="n">num</span><span class="p">,</span>
          <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">loop_state</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
              <span class="n">embs</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span> <span class="n">ids_vec</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">,</span> <span class="n">rets</span><span class="o">=</span><span class="n">rets</span><span class="p">))</span><span class="o">.</span><span class="n">rets</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">rets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rets</span><span class="p">,</span> <span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)])</span>
      <span class="k">return</span> <span class="n">rets</span>

    <span class="k">def</span> <span class="nf">EmbMatmul</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Lookups embedding vectors by doing Matmul with one-hot vector.&quot;&quot;&quot;</span>
      <span class="c1"># lhs[i, j] is True iff xs.ids_vec[i] == j.</span>
      <span class="n">lhs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">xs</span><span class="o">.</span><span class="n">embs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">EmbGather</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Lookups embedding vectors.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
        <span class="c1"># If tf.gather is used, the gradient for the wm will be represented as</span>
        <span class="c1"># IndexedSlices which is sparse. tf.tpu.cross_replica_sum turns</span>
        <span class="c1"># IndexedSlices into a dense tensor with undefined first dimension.</span>
        <span class="c1"># This may cause issues on TPU so instead we just wrap this with</span>
        <span class="c1"># tf.identity which allows tf.tpu.cross_replica_sum to properly compute</span>
        <span class="c1"># the first dim.</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">embs</span><span class="p">),</span> <span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># The above fix tf.tpu_cross_replica_sum causes issues</span>
        <span class="c1"># on inference graphs in which the EmbeddingLayer is on the host</span>
        <span class="c1"># as the tf.identity prevents ResourceGather from being used.</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span> <span class="n">xs</span><span class="o">.</span><span class="n">ids_vec</span><span class="p">)</span>

    <span class="n">xs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">embs</span><span class="o">=</span><span class="n">embs</span><span class="p">,</span> <span class="n">ids_vec</span><span class="o">=</span><span class="n">ids_vec</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">==</span> <span class="s1">&#39;matmul&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span><span class="n">EmbMatmul</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">==</span> <span class="s1">&#39;loop&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">EmbFprop</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">bak</span><span class="o">=</span><span class="n">EmbBprop</span><span class="p">,</span> <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">==</span> <span class="s1">&#39;gather&#39;</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">EmbGather</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer._GetWeightShape"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer._GetWeightShape">[docs]</a>  <span class="k">def</span> <span class="nf">_GetWeightShape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">tpu_compat</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span> <span class="o">!=</span> <span class="s1">&#39;matmul&#39;</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_3d_weight_tensor</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span> <span class="o">%</span> <span class="mi">128</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">emb_shape_suf</span> <span class="o">=</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span> <span class="o">//</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_shape_suf</span> <span class="o">=</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">emb_shape_suf</span> <span class="o">=</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)]</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">emb_shape_suf</span>
    <span class="k">return</span> <span class="n">emb_shape_suf</span><span class="p">,</span> <span class="n">weight_shape</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">weight_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetWeightShape</span><span class="p">()</span>

    <span class="c1"># Define weights</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_pruning</span><span class="p">:</span>
      <span class="n">mask_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                      <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
                                      <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">threshold_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">([],</span>
                                           <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
                                           <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">mask_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
          <span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="n">threshold_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">MaskWeightFn</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">mask</span><span class="p">,</span> <span class="s1">&#39;masked_weights&#39;</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;wm&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="n">MaskWeightFn</span><span class="p">)</span>
      <span class="n">pruning_utils</span><span class="o">.</span><span class="n">AddToPruningCollections</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">mask</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;wm&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">ApplyPruning</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;wm&#39;</span><span class="p">,</span>
                                           <span class="n">pc</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">compression_op</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetLastCompressionOp</span><span class="p">()</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer.EmbLookupDefaultTheta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer.EmbLookupDefaultTheta">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultTheta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer.EmbLookupDefaultThetaOnCpu"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer.EmbLookupDefaultThetaOnCpu">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultThetaOnCpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A faster path for CPU inference than the default gather.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)]],</span>
                          <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer._FlatFProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer._FlatFProp">[docs]</a>  <span class="k">def</span> <span class="nf">_FlatFProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple collection of weights for the layer.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of the flattened inputs to the embedding lookup, and a tensor that</span>
<span class="sd">      is ready to be reshaped into the final shape in FProp.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;ids should be a tf.Tensor!&#39;</span><span class="p">)</span>
      <span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ids</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;ids should be tf.int32, but is </span><span class="si">%s</span><span class="s1">!&#39;</span><span class="p">,</span> <span class="n">ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_id_validation&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">flat_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">)</span>
    <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span><span class="s1">&#39;emb_aqt&#39;</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_compression</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetEmbeddingLookupResult</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">flat_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fprop_mode</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FpropImpl</span><span class="p">(</span><span class="n">wm</span><span class="p">,</span> <span class="n">flat_ids</span><span class="p">)</span>

    <span class="n">embs_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;emb_aqt&#39;</span><span class="p">,</span> <span class="n">embs_result</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;vn&#39;</span><span class="p">):</span>
      <span class="n">embs_result</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">embs_result</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="k">return</span> <span class="n">flat_ids</span><span class="p">,</span> <span class="n">embs_result</span></div>

<div class="viewcode-block" id="SimpleEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple collection of weights for the layer.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">embs_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FlatFProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs_result</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">embs_result</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                         <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embs_result</span></div></div>


<div class="viewcode-block" id="EinsumEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">EinsumEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An embedding layer that uses einsum to avoid reshaping.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="EinsumEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Depth of the input. I.e., the number of classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(embedding_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>

<div class="viewcode-block" id="EinsumEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Define weights</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="c1"># Apply VN on theta.wm so that this layer can be used within a recurrent</span>
    <span class="c1"># loop.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;wm&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span></div>

<div class="viewcode-block" id="EinsumEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="EinsumEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple collection of weights for the layer.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Emulate tf.nn.embedding_lookup(theta.wm, ids) with tf.einsum.</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ProjectLastDim</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span><span class="n">embs_result</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
                                         <span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embs_result</span></div></div>


<div class="viewcode-block" id="OneHotEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.OneHotEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">OneHotEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates one-hot embeddings with uncertainties.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="OneHotEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.OneHotEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
             <span class="s1">&#39;Depth of the input. I.e., the number of classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;uncertainty&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Uncertainty of the correct ID.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span>

<div class="viewcode-block" id="OneHotEmbeddingLayer.EmbLookupDefaultTheta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.OneHotEmbeddingLayer.EmbLookupDefaultTheta">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultTheta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="OneHotEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.OneHotEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="OneHotEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.OneHotEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lookups embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple collection of weights for the layer.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">theta</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_id_validation&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>
    <span class="n">low_confidence</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">high_confidence</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">ids</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">on_value</span><span class="o">=</span><span class="n">high_confidence</span><span class="p">,</span>
        <span class="n">off_value</span><span class="o">=</span><span class="n">low_confidence</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">embs_result</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embs_result</span></div></div>


<div class="viewcode-block" id="PositionalEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">PositionalEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates sinusoidals with respect to the position in time and dimension.</span>

<span class="sd">  Implements the positional embedding layer from &#39;Attention is All You Need&#39;,</span>
<span class="sd">  the Transformer Network.</span>

<span class="sd">  Code and comments are adapted from tensor2tensor/layers/common_attention.py</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="PositionalEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;min_timescale&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Start of the geometric index.&#39;</span>
        <span class="s1">&#39;Determines the periodicity of the added signal.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;max_timescale&#39;</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="s1">&#39;End of the geometric index. &#39;</span>
        <span class="s1">&#39;Determines the frequency of the added signal.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the embedding to be generated.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;trainable_scaling&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Introduces a trainable scaling parameter (a scalar) that&#39;</span>
        <span class="s1">&#39; multiplies the positional embedding in FProp.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;trainable_scaling_init&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
             <span class="s1">&#39;Initial value of the scaling parameter.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;frequency_scaling&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Introduces a trainable frequency scaling parameter (a scalar) that&#39;</span>
        <span class="s1">&#39; multiplies the frequency of the sinusoids.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">min_timescale</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">max_timescale</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>

<div class="viewcode-block" id="PositionalEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">trainable_scaling</span><span class="p">:</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">frequency_scaling</span><span class="p">:</span>
      <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;freq_scale&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="PositionalEmbeddingLayer._PosEmbeddingsFromPositions"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer._PosEmbeddingsFromPositions">[docs]</a>  <span class="k">def</span> <span class="nf">_PosEmbeddingsFromPositions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">position</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates the positional embeddings given the position tensor.</span>

<span class="sd">    Factors out the common code from FProp and FPropWithPosition. Returns</span>
<span class="sd">    positional embeddings corresponding to the input position tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      position: Position tensor of dtype float and shape [bs, seq_length] to</span>
<span class="sd">        generate positional embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a Tensor of shape [bs, seq_length, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">seq_length</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">position</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_timescales</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">log_timescale_increment</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">max_timescale</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">min_timescale</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">num_timescales</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">inv_timescales</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">min_timescale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">num_timescales</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">*</span>
        <span class="o">-</span><span class="n">log_timescale_increment</span><span class="p">)</span>

    <span class="n">scaled_time</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">inv_timescales</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">frequency_scaling</span><span class="p">:</span>
      <span class="n">scaled_time</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">freq_scale</span><span class="p">)</span>

    <span class="n">signal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">scaled_time</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">scaled_time</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">signal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
        <span class="n">signal</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]])</span>
    <span class="n">signal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">trainable_scaling</span><span class="p">:</span>
      <span class="n">signal</span> <span class="o">*=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">trainable_scaling_init</span> <span class="o">+</span> <span class="n">theta</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">signal</span></div>

<div class="viewcode-block" id="PositionalEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a Tensor of sinusoids with different frequencies.</span>

<span class="sd">    Each channel (dimension) of the generated positionanl embedding Tensor</span>
<span class="sd">    corresponds to a sinusoid of different frequency and phase.</span>

<span class="sd">    This allows attention to learn to use absolute and relative positions.</span>
<span class="sd">    Timing signals should be added to some precursors of both the query and the</span>
<span class="sd">    memory inputs to attention.</span>

<span class="sd">    The use of relative position is possible because sin(x+y) and cos(x+y) can</span>
<span class="sd">    be experessed in terms of y, sin(x) and cos(x).</span>

<span class="sd">    In particular, we use a geometric sequence of timescales starting with</span>
<span class="sd">    min_timescale and ending with max_timescale.  The number of different</span>
<span class="sd">    timescales is equal to channels (dimension) / 2. For each timescale, we</span>
<span class="sd">    generate the two sinusoidal signals sin(timestep/timescale) and</span>
<span class="sd">    cos(timestep/timescale).  All of these sinusoids are concatenated in</span>
<span class="sd">    the channels dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      seq_length: Sequence length of the embeddings to be generated</span>

<span class="sd">    Returns:</span>
<span class="sd">      a Tensor of shape [seq_length, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seq_length</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">])</span>
    <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PosEmbeddingsFromPositions</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pos_emb</span><span class="p">,</span> <span class="p">[</span><span class="n">seq_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>

<div class="viewcode-block" id="PositionalEmbeddingLayer.FPropWithPosition"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.PositionalEmbeddingLayer.FPropWithPosition">[docs]</a>  <span class="k">def</span> <span class="nf">FPropWithPosition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">position_tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a Tensor of sinusoids with different frequencies.</span>

<span class="sd">    Uses the provided position tensor to generate positional embeddings. Refer</span>
<span class="sd">    to FProp description for details of sinusoidal positional embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      position_tensor: Position tensor of shape [bs, seq_length] to generate</span>
<span class="sd">        positional embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">      a Tensor of shape [bs, seq_length, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">position_tensor</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_PosEmbeddingsFromPositions</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="RelativePositionalEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.RelativePositionalEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">RelativePositionalEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Relative positional embedding.</span>

<span class="sd">  Section 3.2 of https://arxiv.org/pdf/1803.02155.pdf</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="RelativePositionalEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.RelativePositionalEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;radius&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Radius of the relative window size. Distance are clipped to &#39;</span>
        <span class="s1">&#39;[-radius, radius].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Dimension of embedding.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">radius</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">radius</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;params.radius must be a positive int, but is </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                       <span class="n">params</span><span class="o">.</span><span class="n">radius</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">dim</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;params.dim must be a positive int, but is </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                       <span class="n">params</span><span class="o">.</span><span class="n">radius</span><span class="p">)</span>

<div class="viewcode-block" id="RelativePositionalEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.RelativePositionalEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">radius</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="RelativePositionalEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.RelativePositionalEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">relative_distance</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes relative positional embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap of Tensors of layer weights.</span>
<span class="sd">      relative_distance: A Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape relative_distance.shape + [params.dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">clipped_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">relative_distance</span><span class="p">,</span> <span class="o">-</span><span class="n">params</span><span class="o">.</span><span class="n">radius</span><span class="p">,</span>
                                       <span class="n">params</span><span class="o">.</span><span class="n">radius</span><span class="p">)</span>
    <span class="c1"># Right-shift indices to make them all non-negative.</span>
    <span class="n">calibrated_indices</span> <span class="o">=</span> <span class="n">clipped_indices</span> <span class="o">+</span> <span class="n">params</span><span class="o">.</span><span class="n">radius</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">calibrated_indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="SinusoidalPositionalEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SinusoidalPositionalEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">SinusoidalPositionalEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates sinusoidals with respect to the position in time and dimension.</span>

<span class="sd">  Implements the a variant of the positional embedding layer from &#39;Attention is</span>
<span class="sd">  All You Need&#39;, the Transformer Network that doesn&#39;t require tuning of the</span>
<span class="sd">  max_timescale/min_timescale. See this blog post and Ron&#39;s colab.</span>
<span class="sd">  https://kazemnejad.com/blog/transformer_architecture_positional_encoding</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SinusoidalPositionalEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SinusoidalPositionalEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the embedding to be generated.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;embedding_dim needs to be even.&#39;</span><span class="p">)</span>

<div class="viewcode-block" id="SinusoidalPositionalEmbeddingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SinusoidalPositionalEmbeddingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a Tensor of sinusoids with different frequencies.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      seq_length: Sequence length of the embeddings to be generated</span>

<span class="sd">    Returns:</span>
<span class="sd">      a Tensor of shape [seq_length, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seq_length</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">num_timescales</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="n">num_timescales</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">seq_length</span><span class="p">)</span>
    <span class="n">scaled_pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">positions</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">freq</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">sincos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">scaled_pos</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">scaled_pos</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sincos</span><span class="p">,</span> <span class="p">[</span><span class="n">seq_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span></div></div>


<div class="viewcode-block" id="SoftmaxLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for softmax layers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="SoftmaxLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for SoftmaxLayer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of target classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;logits_abs_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;If not None, logits are clipped to be within&#39;</span>
        <span class="s1">&#39; [-logits_abs_max, logits_abs_max]. This can be a scalar&#39;</span>
        <span class="s1">&#39; or a scalar tensor. Applies back pressure at training time; ignored&#39;</span>
        <span class="s1">&#39; for inference.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;chunk_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;If non-zero, computes the per example &#39;</span>
        <span class="s1">&#39;xent by small chunks along the batch dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Quantization domain for logits.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Quantization domain for the weights.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="SoftmaxLayer.Logits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer.Logits">[docs]</a>  <span class="k">def</span> <span class="nf">Logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">unused</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the logits computed before the softmax.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;GetLogits is not implemented.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SoftmaxLayer.XentLossFromLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer.XentLossFromLogits">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossFromLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">unused</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Xent loss from pre-computed logits.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;XentLossFromLogits is not implemented.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SoftmaxLayer.XentLoss"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer.XentLoss">[docs]</a>  <span class="k">def</span> <span class="nf">XentLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes cross entropy.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="SoftmaxLayer._FProp2D"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer._FProp2D">[docs]</a>  <span class="k">def</span> <span class="nf">_FProp2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">theta</span><span class="p">,</span>
               <span class="n">inputs</span><span class="p">,</span>
               <span class="n">class_weights</span><span class="p">,</span>
               <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Specialized FProp for matrix inputs.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s1">&#39;Subclasses of SoftmaxLayer must implement _FProp2D&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SoftmaxLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SoftmaxLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">class_weights</span><span class="p">,</span>
            <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes logit, cross entropy etc.</span>

<span class="sd">    This function can both work with class_ids, or probability distributions</span>
<span class="sd">    over classes. Exactly one of class_ids or class_probabilities must be</span>
<span class="sd">    provided.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: a list of a single tensor, or a single tensor with the shape [...,</span>
<span class="sd">        input_dim].</span>
<span class="sd">      class_weights: a tensor with shape [...] containing the weights for each</span>
<span class="sd">        target word.</span>
<span class="sd">      class_ids: a tensor with shape [..., 1] of int32 dtype containing the</span>
<span class="sd">        target class labels.</span>
<span class="sd">      class_probabilities: a tensor with shape [..., num_classes] of float</span>
<span class="sd">        values indicating class-membership probabilities.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` containing the following fields</span>

<span class="sd">      - logits: with shape [..., num_classes]. Unnormalized softmax&#39;s logits.</span>
<span class="sd">      - per_example_argmax: with shape [...]. argmax of i-th example.</span>
<span class="sd">      - per_example_xent: with shape [...]. Cross entropy between i-th example&#39;s</span>
<span class="sd">        prediction and its label.</span>
<span class="sd">      - per_example_weight: with shape [...]. class_weights casted to</span>
<span class="sd">        this layer&#39;s dtype.</span>
<span class="sd">      - total_xent: A scalar. The sum of per_example_weight * per_example_xent.</span>
<span class="sd">      - total_weight: A scalar. The sum of per_example_weight.</span>
<span class="sd">      - avg_xent: A scalar. total_loss / total_weight.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Consolidate list/single value into a list.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>

    <span class="c1"># If inputs are matrices already, delegate to _FProp2D.</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FProp2D</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span>
                           <span class="n">class_probabilities</span><span class="p">)</span>

    <span class="c1"># Remembers the original shape[1:-1].</span>
    <span class="n">shape_mid</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Reshape inputs to matrices, labels to vectors, etc.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ToStaticShape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span>
    <span class="p">]</span>
    <span class="n">class_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span>

    <span class="c1"># Delegates to _FProp2D.</span>
    <span class="n">xent_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_FProp2D</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span>
                              <span class="n">class_probabilities</span><span class="p">)</span>

    <span class="c1"># Reshapes xent_loss fields according to the inputs&#39; shape.</span>
    <span class="n">xent_loss</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">xent_loss</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape_mid</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">per_example_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape_mid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_argmax</span><span class="p">,</span>
                                              <span class="n">per_example_shape</span><span class="p">)</span>
    <span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_xent</span><span class="p">,</span>
                                            <span class="n">per_example_shape</span><span class="p">)</span>
    <span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_weight</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xent_loss</span><span class="o">.</span><span class="n">per_example_weight</span><span class="p">,</span>
                                              <span class="n">per_example_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">xent_loss</span></div></div>


<div class="viewcode-block" id="SimpleFullSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax">[docs]</a><span class="k">class</span> <span class="nc">SimpleFullSoftmax</span><span class="p">(</span><span class="n">SoftmaxLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A somewhat simple softmax layer.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="SimpleFullSoftmax.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for SimpleFullSoftmax.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_sampled&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of samples to use for the sampled soft-max. &#39;</span>
        <span class="s1">&#39;Default value of 0 means no sampling is done; if set to &gt; 0 then &#39;</span>
        <span class="s1">&#39;training will use sampled soft-max when both chunk_size == 0 and &#39;</span>
        <span class="s1">&#39;FProp is called with class_probabilities=None.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;num_shards&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;Number of shards to split params into. num_shards should&#39;</span>
        <span class="s1">&#39; divide num_classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;apply_pruning&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether to prune the weights while training&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;pruning_hparams_dict&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Pruning related hyperparameters. A dict with hyperparameter: value&#39;</span>
        <span class="s1">&#39;pairs. See google-research.model_pruning.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_num_classes_major_weight&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether to use num_classes as major dimension for weight params. &#39;</span>
        <span class="s1">&#39;This shows performance benefit especially when sharing embedding &#39;</span>
        <span class="s1">&#39;and softmax. By removing the transpose before gather, it allows &#39;</span>
        <span class="s1">&#39;better XLA fusions and optimizations.&#39;</span><span class="p">)</span>

    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_bias&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether or not to use a bias variable.&#39;</span>
        <span class="s1">&#39;Not using bias is not compatible with sampled softmax &#39;</span>
        <span class="s1">&#39;(num_sampled &gt; 0).&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bias_init&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Weight initialization constant for bias.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a SimpleFullSoftmax layer.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

    <span class="c1"># We shard params across the class dimension.</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">num_shards</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Sampled softmax requires bias.&#39;</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_shards</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateAqtWeight</span><span class="p">(</span>
          <span class="s1">&#39;softmax_aqt&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">compression_ops</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="SimpleFullSoftmax._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">num_classes_per_shard</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">num_shards</span>
    <span class="c1"># When using sampled soft-max we&#39;d rather work with weights of</span>
    <span class="c1"># shape=[num_classes_per_shard, p.input_dim] to avoid an expensive transpose</span>
    <span class="c1"># op before computing the sampled_softmax_loss.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">weights_shard_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">num_classes_per_shard</span><span class="p">]</span>
    <span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span>
    <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">weight_split_dims_mapping</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                               <span class="n">weight_split_dims_mapping</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">use_num_classes_major_weight</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">weights_shard_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_classes_per_shard</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">weight_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="n">weight_split_dims_mapping</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">weights_shard_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">weight_split_dims_mapping</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_pruning</span><span class="p">:</span>
      <span class="n">mask_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                      <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
                                      <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">threshold_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">([],</span>
                                           <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
                                           <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">):</span>
      <span class="n">weights_var_name</span> <span class="o">=</span> <span class="s1">&#39;weight_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_pruning</span><span class="p">:</span>
        <span class="n">mask_var_name</span> <span class="o">=</span> <span class="s1">&#39;mask_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
        <span class="n">threshold_var_name</span> <span class="o">=</span> <span class="s1">&#39;threshold_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="n">mask_var_name</span><span class="p">,</span> <span class="n">mask_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="n">threshold_var_name</span><span class="p">,</span> <span class="n">threshold_pc</span><span class="p">,</span> <span class="n">theta_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">MaskWeightFn</span><span class="p">(</span><span class="n">mask_var_name</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">mask_var_name</span><span class="p">),</span>
              <span class="s1">&#39;masked_weights&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="n">weights_var_name</span><span class="p">,</span>
            <span class="n">pc</span><span class="p">,</span>
            <span class="n">theta_fn</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">MaskWeightFn</span><span class="p">,</span> <span class="n">mask_var_name</span><span class="p">))</span>
        <span class="n">pruning_utils</span><span class="o">.</span><span class="n">AddToPruningCollections</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">weights_var_name</span><span class="p">),</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">mask_var_name</span><span class="p">),</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span> <span class="n">threshold_var_name</span><span class="p">))</span>

      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">weights_var_name</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
          <span class="c1"># matrix compression path. call ApplyPruning to setup compression op</span>
          <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">ApplyPruning</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">pruning_hparams_dict</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span>
                                               <span class="n">weights_var_name</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                               <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">compression_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
              <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetLastCompressionOp</span><span class="p">())</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_classes_per_shard</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bias_init</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">bias_split_dims_mapping</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;bias_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">AddVN</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;inputs&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;logits&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._GetInputs"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._GetInputs">[docs]</a>  <span class="k">def</span> <span class="nf">_GetInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">inputs</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._ConcatWeights"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._ConcatWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_ConcatWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="c1"># Add per-step noise if configured so.</span>
    <span class="n">concat_axis</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">:</span>
      <span class="n">concat_axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="s1">&#39;weight_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="s1">&#39;bias_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">)]</span>
      <span class="n">new_theta</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span>
          <span class="n">p</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">biases</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_shards</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">new_theta</span><span class="o">.</span><span class="n">wm</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">new_theta</span><span class="o">.</span><span class="n">wm</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span>
          <span class="n">p</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">concat_axis</span><span class="p">),</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_theta</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._LogitsUsingConcatenatedWeightsHelper"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._LogitsUsingConcatenatedWeightsHelper">[docs]</a>  <span class="k">def</span> <span class="nf">_LogitsUsingConcatenatedWeightsHelper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_shards</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">:</span>
        <span class="c1"># TODO(shivaniagrawal): having two transpose is expensive, we should</span>
        <span class="c1"># optimize this by allowing feature axis to other that last axis.</span>
        <span class="c1"># For this particular case num_classes is the first dimension, transpose</span>
        <span class="c1"># of weight would make it last dimension; we scale on the axis</span>
        <span class="c1"># corresponding to num_classes.</span>
        <span class="n">wm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span><span class="s1">&#39;softmax_aqt&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">wm</span><span class="p">),</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ToAqtWeight</span><span class="p">(</span><span class="s1">&#39;softmax_aqt&#39;</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">feature_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">ApplyCompression</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="c1"># compression path. call GetMatmulResult.</span>
        <span class="c1"># inputs and wm are both rank 2. using GetMatmulResult</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">pruning_utils</span><span class="o">.</span><span class="n">PruningOp</span><span class="o">.</span><span class="n">GetMatmulResult</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">)</span>

      <span class="c1"># We used weight&#39;s output_dimension, i.e. p.num_classes as feature axis</span>
      <span class="c1"># while quantizing weight.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FromAqtWeight</span><span class="p">(</span><span class="s1">&#39;softmax_aqt&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
      <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

      <span class="c1"># x * w + b</span>
      <span class="c1"># Note that theta.wm and theta.bias are transformed to concated/clipped</span>
      <span class="c1"># by caller.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="c1"># Clip logits by range.</span>
    <span class="c1"># Note that this is generally not used in conjunction with quantization and</span>
    <span class="c1"># shouldn&#39;t be needed at inference time as the quantized matmul above will</span>
    <span class="c1"># take care of clipping naturally based on the data type and qparams.</span>
    <span class="n">abs_max</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span>
    <span class="k">if</span> <span class="n">abs_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
      <span class="n">abs_min</span> <span class="o">=</span> <span class="o">-</span><span class="n">abs_max</span>  <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">abs_min</span><span class="p">,</span> <span class="n">abs_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._LogitsUsingConcatenatedWeights"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._LogitsUsingConcatenatedWeights">[docs]</a>  <span class="k">def</span> <span class="nf">_LogitsUsingConcatenatedWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LogitsUsingConcatenatedWeightsHelper</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax.SimpleLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax.SimpleLogits">[docs]</a>  <span class="k">def</span> <span class="nf">SimpleLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the simple logits computed before the softmax.</span>

<span class="sd">    Compared to the Logits function, this one has only weights, no bias for the</span>
<span class="sd">    linear projection.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A tensor with the shape [N, input_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits: [N, num_classes]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;inputs&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ConcatWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QWeight</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax.Logits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax.Logits">[docs]</a>  <span class="k">def</span> <span class="nf">Logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the logits computed before the softmax.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: a list of a single tensor, or a single tensor with the shape [N,</span>
<span class="sd">        input_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits [batch, num_classes]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LogitsUsingConcatenatedWeights</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ConcatWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._XentLossByChunk"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._XentLossByChunk">[docs]</a>  <span class="k">def</span> <span class="nf">_XentLossByChunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes per-example xent loss between activation and class_ids.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># We reshape activation from a matrix to a 3-D tensor (a sequence</span>
    <span class="c1"># of matrices), where the 2nd dimenion is p.chunk_size.  Because</span>
    <span class="c1"># the batch dimenion may not be multiple of p.chunk_size, we pad</span>
    <span class="c1"># zeros.</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">activation</span><span class="p">))</span>
    <span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">dim0</span> <span class="o">*</span> <span class="n">dim1</span> <span class="o">-</span> <span class="n">batch</span>
    <span class="n">padded_activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">activation</span><span class="p">,</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">pad</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">activation</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">class_ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">padded_class_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">class_ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">pad</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">class_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">id_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">id_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
    <span class="n">padded_class_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">padded_class_ids</span><span class="p">,</span> <span class="n">id_dtype</span><span class="p">)</span>

    <span class="c1"># For each chunk, we compute logits of padded_activation[i, :, :],</span>
    <span class="c1"># and its xent loss with padded_class_ids[i, :].</span>
    <span class="k">def</span> <span class="nf">ChunkFn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">del</span> <span class="n">state0</span>
      <span class="n">activation</span><span class="p">,</span> <span class="n">class_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">class_ids</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_LogitsUsingConcatenatedWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>
      <span class="n">xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">class_ids</span><span class="p">)</span>
      <span class="n">amax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">xent</span><span class="o">=</span><span class="n">xent</span><span class="p">,</span> <span class="n">amax</span><span class="o">=</span><span class="n">amax</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="n">acc</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ConcatWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">xent</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">amax</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">id_dtype</span><span class="p">)),</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">padded_activation</span><span class="p">,</span> <span class="p">[</span><span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">]),</span>
            <span class="n">class_ids</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">padded_class_ids</span><span class="p">,</span> <span class="p">[</span><span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">])),</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">ChunkFn</span><span class="p">)</span>

    <span class="c1"># acc.xent has the shape [dim0, dim1]. acc.xent[i, :] are</span>
    <span class="c1"># per-example xent loss for examples in the i-th chunk.  We</span>
    <span class="c1"># reshape acc.xent to a vector and slice the first &#39;batch&#39; values.</span>
    <span class="k">def</span> <span class="nf">GetBatch</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[:</span><span class="n">batch</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">GetBatch</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">xent</span><span class="p">),</span> <span class="n">GetBatch</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">amax</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax._FProp2D"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax._FProp2D">[docs]</a>  <span class="k">def</span> <span class="nf">_FProp2D</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">theta</span><span class="p">,</span>
               <span class="n">inputs</span><span class="p">,</span>
               <span class="n">class_weights</span><span class="p">,</span>
               <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes xent loss and log-prob logit.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">XentLossFromLogits</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span><span class="p">:</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_XentLossByChunk</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">XentLossFromLogits</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Use sampled soft-max in training mode with p.num_sampled set.</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">&gt;</span> <span class="mi">0</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">use_bias</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span>
          <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Using sampled_softmax_loss(..., num_sampled=</span><span class="si">%d</span><span class="s1">, &#39;</span>
          <span class="s1">&#39;num_classes=</span><span class="si">%d</span><span class="s1">) in SimpleFullSoftmax::_FProp2D&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
          <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
      <span class="c1"># tf.nn.sampled_softmax_loss will call tf.embedding_lookup. And when</span>
      <span class="c1"># tf.embedding_lookup is used, the gradient for the weights will be</span>
      <span class="c1"># represented as IndexedSlices which is sparse. tf.tpu.cross_replica_sum</span>
      <span class="c1"># turns IndexedSlices into a dense tensor with undefined first dimension.</span>
      <span class="c1"># This may cause issues on TPU so instead we just wrap this with</span>
      <span class="c1"># tf.identity which allows tf.tpu.cross_replica_sum to properly compute</span>
      <span class="c1"># the first dim.</span>
      <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span>
          <span class="n">weights</span><span class="o">=</span><span class="p">[</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="s1">&#39;weight_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">)</span>
          <span class="p">],</span>
          <span class="n">biases</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">theta</span><span class="p">[</span><span class="s1">&#39;bias_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_shards</span><span class="p">)],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
          <span class="n">inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_GetInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
          <span class="n">num_sampled</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
          <span class="n">num_classes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
          <span class="n">seed</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
      <span class="c1"># Avoid computing logits; per_example_argmax is going to be always right.</span>
      <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">class_ids</span><span class="p">)</span>

    <span class="n">label_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">total_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_example_xent</span> <span class="o">*</span> <span class="n">label_weights</span><span class="p">)</span>
    <span class="n">total_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">label_weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">log_probs</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span>
        <span class="n">per_example_argmax</span><span class="o">=</span><span class="n">per_example_argmax</span><span class="p">,</span>
        <span class="n">per_example_xent</span><span class="o">=</span><span class="n">per_example_xent</span><span class="p">,</span>
        <span class="n">per_example_weight</span><span class="o">=</span><span class="n">label_weights</span><span class="p">,</span>
        <span class="n">total_xent</span><span class="o">=</span><span class="n">total_xent</span><span class="p">,</span>
        <span class="n">total_weight</span><span class="o">=</span><span class="n">total_weights</span><span class="p">,</span>
        <span class="n">avg_xent</span><span class="o">=</span><span class="n">total_xent</span> <span class="o">/</span> <span class="n">total_weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="SimpleFullSoftmax.XentLossFromLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SimpleFullSoftmax.XentLossFromLogits">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossFromLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">theta</span><span class="p">,</span>
                         <span class="n">logits</span><span class="p">,</span>
                         <span class="n">class_weights</span><span class="p">,</span>
                         <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes cross-entropy, argmax etc. from logits.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">class_probabilities</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span>
          <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Using sparse_softmax_cross_entropy_with_logits() in &#39;</span>
          <span class="s1">&#39;SimpleFullSoftmax::_FProp2D logits_shape=</span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
      <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;This set of arguments is not supported for XentLossFromLogits.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span></div></div>


<div class="viewcode-block" id="FocalFullSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FocalFullSoftmax">[docs]</a><span class="k">class</span> <span class="nc">FocalFullSoftmax</span><span class="p">(</span><span class="n">SimpleFullSoftmax</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An extended softmax layer with focal loss.</span>

<span class="sd">  Focal loss: https://arxiv.org/abs/1708.02002, Eq (3) and (4).</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="FocalFullSoftmax.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FocalFullSoftmax.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;focal_loss_alpha&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;The weighting factor alpha with shape [#classes] for focal loss.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;focal_loss_gamma&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The modulating factor scalar gamma for focal loss.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="FocalFullSoftmax.XentLossFromLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FocalFullSoftmax.XentLossFromLogits">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossFromLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">theta</span><span class="p">,</span>
                         <span class="n">logits</span><span class="p">,</span>
                         <span class="n">class_weights</span><span class="p">,</span>
                         <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes cross-entropy, argmax etc. from logits.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyFocalLoss</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">label_ids</span><span class="o">=</span><span class="n">class_ids</span><span class="p">,</span>
        <span class="n">label_probs</span><span class="o">=</span><span class="n">class_probabilities</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">focal_loss_alpha</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">focal_loss_gamma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span></div></div>


<div class="viewcode-block" id="EinsumSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumSoftmax">[docs]</a><span class="k">class</span> <span class="nc">EinsumSoftmax</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simple softmax layer implemented with Einsum to avoid reshape ops.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="EinsumSoftmax.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumSoftmax.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of target classes.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;focal_loss_alpha&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;The weighting factor alpha with shape [#classes] for focal loss.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;focal_loss_gamma&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The modulating factor scalar gamma for focal loss.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="EinsumSoftmax._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumSoftmax._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">weight_split_dims_mapping</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span>
    <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">weight_split_dims_mapping</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                               <span class="n">weight_split_dims_mapping</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">weight_split_dims_mapping</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
        <span class="s1">&#39;b&#39;</span><span class="p">,</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">bias_split_dims_mapping</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]))</span></div>

<div class="viewcode-block" id="EinsumSoftmax.Logits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumSoftmax.Logits">[docs]</a>  <span class="k">def</span> <span class="nf">Logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the logits computed before the softmax.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: a single tensor with the shape [..., input_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits [..., num_classes].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">:</span>
      <span class="c1"># A common path.</span>
      <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">chr</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">97</span><span class="p">,</span> <span class="mi">123</span><span class="p">)])</span>  <span class="c1"># abc...xyz</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{0}</span><span class="s1">y,yz-&gt;</span><span class="si">{0}</span><span class="s1">z&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...d,dv-&gt;...v&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
        <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">activation_split_dims_mapping</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="EinsumSoftmax.XentLossFromLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.EinsumSoftmax.XentLossFromLogits">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossFromLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">theta</span><span class="p">,</span>
                         <span class="n">logits</span><span class="p">,</span>
                         <span class="n">class_weights</span><span class="p">,</span>
                         <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes cross-entropy, argmax etc. from logits.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyFocalLoss</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">label_ids</span><span class="o">=</span><span class="n">class_ids</span><span class="p">,</span>
        <span class="n">label_probs</span><span class="o">=</span><span class="n">class_probabilities</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">focal_loss_alpha</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">focal_loss_gamma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span></div></div>


<div class="viewcode-block" id="SharedSoftmaxLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SharedSoftmaxLayer">[docs]</a><span class="k">class</span> <span class="nc">SharedSoftmaxLayer</span><span class="p">(</span><span class="n">SimpleFullSoftmax</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Shared softmax layer for decoder embedding/softmax matrix.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="SharedSoftmaxLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SharedSoftmaxLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for SharedSoftmaxLayer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(input_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Set to be compatible with embedding layer, &#39;</span>
        <span class="s1">&#39; and it is equivalent to input_dim&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Set to be compatible with embedding layer, and &#39;</span>
        <span class="s1">&#39;it is equivalent to num_classes&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="SharedSoftmaxLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SharedSoftmaxLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
            <span class="n">summarize</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
            <span class="n">message</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">:class_id_validation&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="p">))</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>

    <span class="n">wm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ConcatWeights</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">wm</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_weight_params</span><span class="p">:</span>
      <span class="n">wm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">wm</span><span class="p">)</span>
    <span class="n">embs_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">wm</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs_result</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="o">**</span><span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">embs_result</span></div></div>


<div class="viewcode-block" id="SingleShardFullSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardFullSoftmax">[docs]</a><span class="k">class</span> <span class="nc">SingleShardFullSoftmax</span><span class="p">(</span><span class="n">SoftmaxLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Full softmax layer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a SingleShardFullSoftmax layer.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="n">linear_p</span> <span class="o">=</span> <span class="n">builder_layers</span><span class="o">.</span><span class="n">LinearLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span>
        <span class="n">input_dims</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">output_dims</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
        <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">linear_p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">weight_split_dims_mapping</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">bias_split_dims_mapping</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">bias_p</span> <span class="o">=</span> <span class="n">builder_layers</span><span class="o">.</span><span class="n">BiasLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span>
        <span class="n">dims</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span>
        <span class="n">weight_split_dims_mapping</span><span class="o">=</span><span class="n">bias_split_dims_mapping</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">bias_p</span><span class="p">)</span>

<div class="viewcode-block" id="SingleShardFullSoftmax.Logits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardFullSoftmax.Logits">[docs]</a>  <span class="k">def</span> <span class="nf">Logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the logits computed before the softmax.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A single tensor with shape [..., input_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      logits [..., num_classes]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">after_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">after_proj</span><span class="p">)</span>
    <span class="c1"># Clip logits by range.</span>
    <span class="c1"># Note that this is generally not used in conjunction with quantization and</span>
    <span class="c1"># shouldn&#39;t be needed at inference time as the quantized matmul above will</span>
    <span class="c1"># take care of clipping naturally based on the data type and qparams.</span>
    <span class="n">abs_max</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">logits_abs_max</span>
    <span class="k">if</span> <span class="n">abs_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
      <span class="n">abs_min</span> <span class="o">=</span> <span class="o">-</span><span class="n">abs_max</span>  <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">abs_min</span><span class="p">,</span> <span class="n">abs_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>

<div class="viewcode-block" id="SingleShardFullSoftmax.XentLossFromLogits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardFullSoftmax.XentLossFromLogits">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossFromLogits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">theta</span><span class="p">,</span>
                         <span class="n">logits</span><span class="p">,</span>
                         <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes cross-entropy, argmax etc. from logits.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">class_probabilities</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">fpdtype</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span>
      <span class="k">if</span> <span class="n">fpdtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
        <span class="c1"># This is needed in order to workaround the limitation that</span>
        <span class="c1"># tf.nn.sparse_softmax_cross_entropy_with_logits is not implemented for</span>
        <span class="c1"># bf16 on cpu.</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">class_ids</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">fpdtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
        <span class="n">per_example_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">per_example_xent</span><span class="p">,</span> <span class="n">fpdtype</span><span class="p">)</span>

      <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">ArgMax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span></div>

<div class="viewcode-block" id="SingleShardFullSoftmax.XentLossByChunk"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardFullSoftmax.XentLossByChunk">[docs]</a>  <span class="k">def</span> <span class="nf">XentLossByChunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes per-example xent loss.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">act_orig_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">act_orig_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span>
    <span class="n">num_chunks</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="n">chunk_size</span>

    <span class="n">num_chunks</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floormod</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">),</span>
            <span class="n">summarize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">message</span><span class="o">=</span><span class="s1">&#39;assert_equal&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">num_chunks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ReshapeX</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
      <span class="n">x_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">new_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">num_chunks</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>

    <span class="n">activation</span> <span class="o">=</span> <span class="n">ReshapeX</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">class_ids</span> <span class="o">=</span> <span class="n">ReshapeX</span><span class="p">(</span><span class="n">class_ids</span><span class="p">)</span>
    <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">ReshapeX</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">)</span>

    <span class="c1"># For each chunk, we compute logits of activation[i, :, :],</span>
    <span class="c1"># and its xent loss with class_ids[i, :].</span>
    <span class="k">def</span> <span class="nf">ChunkFn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="k">del</span> <span class="n">state0</span>
      <span class="n">activation</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">activation</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;class_ids&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;class_probabilities&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">XentLossFromLogits</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
          <span class="n">xent</span><span class="o">=</span><span class="n">per_example_xent</span><span class="p">,</span> <span class="n">amax</span><span class="o">=</span><span class="n">per_example_argmax</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="n">inputs_nmap</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs_nmap</span><span class="o">.</span><span class="n">class_ids</span> <span class="o">=</span> <span class="n">class_ids</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">inputs_nmap</span><span class="o">.</span><span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">class_probabilities</span>

    <span class="n">xent_state0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">activation</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">argmax_out_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
    <span class="n">amax_state0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">activation</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">argmax_out_dtype</span><span class="p">)</span>

    <span class="n">acc</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">recurrent</span><span class="o">.</span><span class="n">Recurrent</span><span class="p">(</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
        <span class="n">state0</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">xent</span><span class="o">=</span><span class="n">xent_state0</span><span class="p">,</span> <span class="n">amax</span><span class="o">=</span><span class="n">amax_state0</span><span class="p">),</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs_nmap</span><span class="p">,</span>
        <span class="n">cell_fn</span><span class="o">=</span><span class="n">ChunkFn</span><span class="p">)</span>

    <span class="c1"># acc.xent has the shape [dim0, dim1]. acc.xent[i, :] are</span>
    <span class="c1"># per-example xent loss for examples in the i-th chunk.  We</span>
    <span class="c1"># reshape acc.xent to a vector and slice the first &#39;batch&#39; values.</span>
    <span class="k">def</span> <span class="nf">GetBatch</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">act_orig_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">GetBatch</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">xent</span><span class="p">),</span> <span class="n">GetBatch</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">amax</span><span class="p">)</span></div>

<div class="viewcode-block" id="SingleShardFullSoftmax.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardFullSoftmax.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">class_weights</span><span class="p">,</span>
            <span class="n">class_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">class_probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes logits, cross entropy etc.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: a single tensor with shape [..., input_dim].</span>
<span class="sd">      class_weights: a tensor with shape [..., 1] containing the weights for</span>
<span class="sd">        each target word.</span>
<span class="sd">      class_ids: a tensor with shape [..., 1] of int32 dtype containing the</span>
<span class="sd">        target class labels.</span>
<span class="sd">      class_probabilities: a tensor with shape [..., num_classes] of float</span>
<span class="sd">        values indicating class-membership probabilities.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` containing the following fields</span>

<span class="sd">      - logits: with shape [..., num_classes]. Unnormalized softmax&#39;s logits.</span>
<span class="sd">      - per_example_argmax: with shape [...]. argmax of i-th example.</span>
<span class="sd">      - per_example_xent: with shape [...]. Cross entropy between i-th example&#39;s</span>
<span class="sd">        prediction and its label.</span>
<span class="sd">      - per_example_weight: with shape [...]. class_weights casted to</span>
<span class="sd">        this layer&#39;s dtype.</span>
<span class="sd">      - total_xent: A scalar. The sum of per_example_weight * per_example_xent.</span>
<span class="sd">      - total_weight: A scalar. The sum of per_example_weight.</span>
<span class="sd">      - avg_xent: A scalar. total_loss / total_weight.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">ids_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">probs_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">class_weights</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">ids_shape</span><span class="p">)</span>
    <span class="n">class_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="n">ids_shape</span><span class="p">)</span>
      <span class="n">class_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">class_ids</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_probabilities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">,</span> <span class="n">probs_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
      <span class="c1"># Chunking.</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">XentLossByChunk</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Logits</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">per_example_xent</span><span class="p">,</span> <span class="n">per_example_argmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">XentLossFromLogits</span><span class="p">(</span>
          <span class="n">theta</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">class_ids</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>

    <span class="n">label_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">total_xent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">per_example_xent</span> <span class="o">*</span> <span class="n">label_weights</span><span class="p">)</span>
    <span class="n">total_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">label_weights</span><span class="p">)</span>
    <span class="n">output_nmap</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">per_example_argmax</span><span class="o">=</span><span class="n">per_example_argmax</span><span class="p">,</span>
        <span class="n">per_example_xent</span><span class="o">=</span><span class="n">per_example_xent</span><span class="p">,</span>
        <span class="n">per_example_weight</span><span class="o">=</span><span class="n">label_weights</span><span class="p">,</span>
        <span class="n">total_xent</span><span class="o">=</span><span class="n">total_xent</span><span class="p">,</span>
        <span class="n">total_weight</span><span class="o">=</span><span class="n">total_weights</span><span class="p">,</span>
        <span class="n">avg_xent</span><span class="o">=</span><span class="n">total_xent</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_weights</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">output_nmap</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span>
      <span class="n">output_nmap</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span>
    <span class="k">return</span> <span class="n">output_nmap</span></div></div>


<div class="viewcode-block" id="SingleShardSharedEmbeddingSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardSharedEmbeddingSoftmax">[docs]</a><span class="k">class</span> <span class="nc">SingleShardSharedEmbeddingSoftmax</span><span class="p">(</span><span class="n">SingleShardFullSoftmax</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A shared softmax/embedding layer.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="SingleShardSharedEmbeddingSoftmax.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardSharedEmbeddingSoftmax.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Num tokens in vocab.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;scale_sqrt_depth&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set True, activations are scaled&#39;</span>
        <span class="s1">&#39; with sqrt(embedding_dim) in EmbLookup.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;emb_with_matmul&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;use one-hot vector to perform matmul.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

<div class="viewcode-block" id="SingleShardSharedEmbeddingSoftmax.EmbLookupDefaultTheta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardSharedEmbeddingSoftmax.EmbLookupDefaultTheta">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupDefaultTheta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span></div>

<div class="viewcode-block" id="SingleShardSharedEmbeddingSoftmax.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.SingleShardSharedEmbeddingSoftmax.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for ids.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: Named tuple with the weight matrix for the embedding.</span>
<span class="sd">      ids: A rank-N int32 tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A rank-(N+1) params.dtype tensor.</span>
<span class="sd">      embs[indices, :] is the embedding vector for ids[indices].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab_id_validation&#39;</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">emb_with_matmul</span><span class="p">:</span>
      <span class="c1"># [b, t, vocab_size]</span>
      <span class="n">one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">one_hot</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;blv,kv-&gt;blk&#39;</span><span class="p">,</span> <span class="n">one_hot</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;kv,...v-&gt;...k&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">one_hot</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># TODO(yonghui): Get rid of this extra copy (tf.transpose).</span>
      <span class="n">emb_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
      <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">emb_vars</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">scale_sqrt_depth</span><span class="p">:</span>
      <span class="n">embs</span> <span class="o">*=</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">embs</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ConvSoftmax"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSoftmax">[docs]</a><span class="k">class</span> <span class="nc">ConvSoftmax</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A softmax implementation based on 1x1 convolution.</span>

<span class="sd">  On TPU this is much more memory efficient than MatMul after reshaping logits</span>
<span class="sd">  to a matrix.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ConvSoftmax.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSoftmax.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for SoftmaxLayer.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the hidden layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of target classes.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="ConvSoftmax._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSoftmax._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a SimpleFullSoftmax layer.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
      <span class="n">w_proj_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">),</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w_proj&#39;</span><span class="p">,</span> <span class="n">w_proj_pc</span><span class="p">)</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
        <span class="s1">&#39;b&#39;</span><span class="p">,</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">],</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]))</span></div>

<div class="viewcode-block" id="ConvSoftmax.Logits"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSoftmax.Logits">[docs]</a>  <span class="k">def</span> <span class="nf">Logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># [batch, time, depth]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w_proj</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;VALID&#39;</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;VALID&#39;</span><span class="p">),</span> <span class="n">theta</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">logits</span></div></div>


<div class="viewcode-block" id="DropoutLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer">[docs]</a><span class="k">class</span> <span class="nc">DropoutLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply dropout during trainig.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="DropoutLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Keep probability.&#39;</span><span class="p">)</span>
    <span class="c1"># noise_shape is unknown when building layer params.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;noise_shape&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;A 1-D `Tensor` of type `int32`, representing&#39;</span>
        <span class="s1">&#39; the shape for randomly generated keep/drop flags.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;noise_shape_broadcast_dims&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;A list of dimension where the noise shape is broadcasted. For &#39;</span>
        <span class="s1">&#39;example, noise_shape = [n, h, w, 1] when &#39;</span>
        <span class="s1">&#39;noise_shape_broadcast_dims=[-1] &#39;</span><span class="p">)</span>
    <span class="c1"># We typically want to replace dropout by expectation during eval.</span>
    <span class="c1"># However, in certain cases E(f(x)) != f(E(x)), and replacing dropout by its</span>
    <span class="c1"># expectation during eval leads to worse quality.</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_at_eval&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Whether or not to also perform dropout at eval time.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="DropoutLayer._Dropout"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer._Dropout">[docs]</a>  <span class="k">def</span> <span class="nf">_Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">noise_shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">rate</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span>
        <span class="n">noise_shape</span><span class="o">=</span><span class="n">noise_shape</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="DropoutLayer.NumOutputNodes"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer.NumOutputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">NumOutputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="c1"># The layer does element-wise processing thus is input-shape agnostic.</span>
    <span class="k">return</span></div>

<div class="viewcode-block" id="DropoutLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply dropout to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      inputs with dropout applied at training time.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_at_eval</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">inputs</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">noise_shape_broadcast_dims</span><span class="p">:</span>
        <span class="n">noise_shape</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">noise_shape</span> <span class="ow">or</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">noise_shape_broadcast_dims</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">noise_shape</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid broadcasted dim </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
          <span class="n">noise_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">noise_shape</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">noise_shape</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Dropout</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">noise_shape</span><span class="p">)</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
      <span class="k">return</span> <span class="n">ret</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span></div>

<div class="viewcode-block" id="DropoutLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DropoutLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="n">flops_per_element</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Approximately 10 flops per element.</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">*</span> <span class="n">flops_per_element</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="DeterministicDropoutLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeterministicDropoutLayer">[docs]</a><span class="k">class</span> <span class="nc">DeterministicDropoutLayer</span><span class="p">(</span><span class="n">DropoutLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply dropout during trainig.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="DeterministicDropoutLayer._Dropout"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.DeterministicDropoutLayer._Dropout">[docs]</a>  <span class="k">def</span> <span class="nf">_Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">noise_shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">DeterministicDropout</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">keep_prob</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span>
        <span class="n">seeds</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateStepSeedPair</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">),</span>
        <span class="n">noise_shape</span><span class="o">=</span><span class="n">noise_shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm">[docs]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Layer normalization.</span>

<span class="sd">  Implements layer normalization:</span>
<span class="sd">  https://arxiv.org/abs/1607.06450</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LayerNorm.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input to the network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s1">&#39;Tiny value to guard rsqrt.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_fused_layernorm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to use fused layernorm.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;direct_scale&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether to apply scale directly &#39;</span>
        <span class="s1">&#39;without a +1.0.  Var is initialized to 1.0 instead. This makes &#39;</span>
        <span class="s1">&#39;the layer weight-compatible with the implementation in &#39;</span>
        <span class="s1">&#39;contrib.layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_defun&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether to use CallDefun for normalization.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

<div class="viewcode-block" id="LayerNorm._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span> <span class="o">+</span>
        <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">direct_scale</span><span class="p">:</span>
      <span class="n">scale_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span> <span class="o">+</span>
          <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scale_pc</span> <span class="o">=</span> <span class="n">pc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="LayerNorm._GetScaleAndBias"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm._GetScaleAndBias">[docs]</a>  <span class="k">def</span> <span class="nf">_GetScaleAndBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">bias</span></div>

<div class="viewcode-block" id="LayerNorm.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies normalization over the last dimension (layer).</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A tensor of shape [..., hidden_dim].</span>

<span class="sd">    Returns:</span>
<span class="sd">      tensor of the same shape with inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">testonly_skip_norm_layers</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">inputs</span>

    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
          <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)],</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

      <span class="n">cur_scale</span><span class="p">,</span> <span class="n">cur_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetScaleAndBias</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">direct_scale</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">cur_scale</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">cur_scale</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_fused_layernorm</span><span class="p">:</span>
        <span class="n">counts</span><span class="p">,</span> <span class="n">means_ss</span><span class="p">,</span> <span class="n">variance_ss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sufficient_statistics</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">normalize_moments</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">means_ss</span><span class="p">,</span> <span class="n">variance_ss</span><span class="p">,</span>
                                                 <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># Adding a cast here. Sometimes, inputs/mean/variance/p.epsilon are in</span>
        <span class="c1"># float32 while scale and cur_bias are in bf16.</span>
        <span class="n">inputs_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
            <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">scale</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs_norm</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">cur_bias</span>

      <span class="k">def</span> <span class="nf">Normalize</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Normalize `xs.x` w/ `xs.scale` and `xs.bias` gain/shift.&quot;&quot;&quot;</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetShape</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">])</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x_reshaped</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_reshaped</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span>
          <span class="c1"># tf.rsqrt and SquaredDifference are not implemented for bfloat16,</span>
          <span class="c1"># hence we always cast into tf.float32.</span>
          <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_reshaped</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
              <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
              <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">x_norm_den_inv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
              <span class="n">x_reshaped</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_reshaped</span> <span class="o">-</span> <span class="n">mean</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">x_norm_den_inv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span> <span class="n">x_reshaped</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_reshaped</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_norm_den_inv</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_norm</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_norm</span> <span class="o">*</span> <span class="n">xs</span><span class="o">.</span><span class="n">scale</span> <span class="o">+</span> <span class="n">xs</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">use_defun</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span>
          <span class="n">Normalize</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">cur_bias</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">cur_bias</span><span class="p">))</span></div>

<div class="viewcode-block" id="LayerNorm.NumOutputNodes"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm.NumOutputNodes">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">NumOutputNodes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span></div>

<div class="viewcode-block" id="LayerNorm.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LayerNorm.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">flops</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,))</span></div></div>


<span class="c1"># TODO(shibow/wangtao) remove this after b/174094694 is done.</span>
<div class="viewcode-block" id="ReshapedLayerNorm"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ReshapedLayerNorm">[docs]</a><span class="k">class</span> <span class="nc">ReshapedLayerNorm</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Customized LayerNorm with model dim D reshaped as Md.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="ReshapedLayerNorm.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ReshapedLayerNorm.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies normalization over the last two dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A tensor of shape [..., dim_reshape_segments, hidden_dim //</span>
<span class="sd">        dim_reshape_segments].</span>

<span class="sd">    Returns:</span>
<span class="sd">      tensor of the same shape with inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

      <span class="n">cur_scale</span><span class="p">,</span> <span class="n">cur_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_GetScaleAndBias</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">direct_scale</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">cur_scale</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">cur_scale</span>

      <span class="n">axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
      <span class="n">counts</span><span class="p">,</span> <span class="n">means_ss</span><span class="p">,</span> <span class="n">variance_ss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sufficient_statistics</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">normalize_moments</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">means_ss</span><span class="p">,</span> <span class="n">variance_ss</span><span class="p">,</span>
                                               <span class="kc">None</span><span class="p">)</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
      <span class="n">cur_bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cur_bias</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
      <span class="c1"># Adding a cast here. Sometimes, inputs/mean/variance/p.epsilon are in</span>
      <span class="c1"># float32 while scale and cur_bias are in bf16.</span>
      <span class="n">inputs_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">scale</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">inputs_norm</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">cur_bias</span></div></div>


<div class="viewcode-block" id="CategoricalLayerNorm"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm">[docs]</a><span class="k">class</span> <span class="nc">CategoricalLayerNorm</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Categorical layer normalization.</span>

<span class="sd">  Allow dynamic switch of normalization params based on given class_index.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="CategoricalLayerNorm.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="s1">&#39;Number of privatized copies of layer norm params.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="CategoricalLayerNorm._BiasVarName"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm._BiasVarName">[docs]</a>  <span class="k">def</span> <span class="nf">_BiasVarName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;bias_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span></div>

<div class="viewcode-block" id="CategoricalLayerNorm._ScaleVarName"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm._ScaleVarName">[docs]</a>  <span class="k">def</span> <span class="nf">_ScaleVarName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;scale_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span></div>

<div class="viewcode-block" id="CategoricalLayerNorm._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Skip LayerNorm&#39;s _CreateLayerVariables() as bias and scale variables will</span>
    <span class="c1"># be created in this function.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>  <span class="c1"># pylint: disable=bad-super-call</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span> <span class="o">+</span>
        <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_BiasVarName</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">pc</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ScaleVarName</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">pc</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">AddExtraTheta</span><span class="p">(</span><span class="s1">&#39;class_index&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

<div class="viewcode-block" id="CategoricalLayerNorm._GetScaleAndBias"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CategoricalLayerNorm._GetScaleAndBias">[docs]</a>  <span class="k">def</span> <span class="nf">_GetScaleAndBias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">assert_between</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">class_index</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)]):</span>
      <span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_BiasVarName</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)]</span>
      <span class="n">cur_bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">biases</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">class_index</span><span class="p">)</span>
      <span class="n">scales</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ScaleVarName</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)]</span>
      <span class="n">cur_scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">class_index</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">cur_scale</span><span class="p">,</span> <span class="n">cur_bias</span></div></div>


<div class="viewcode-block" id="ConvSetLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSetLayer">[docs]</a><span class="k">class</span> <span class="nc">ConvSetLayer</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set of Convolutions with different filter sizes in a single layer.</span>

<span class="sd">    Applies a set of convolutions with different filter shapes to the inputs and</span>
<span class="sd">    returns the concatenated outputs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ConvSetLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSetLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;cnn_tpl&#39;</span><span class="p">,</span>
             <span class="n">ConvLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">filter_stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
             <span class="s1">&#39;Conv layer template for the set of conv layers.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_shapes&#39;</span><span class="p">,</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span>
        <span class="s1">&#39;Must be a list of sequences of 4. Elements are in order of height&#39;</span>
        <span class="s1">&#39; (time), width (frequency), in_channel, out_channel&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

    <span class="n">filter_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Asserting kernel sizes are different and input sizes are the same.</span>
    <span class="k">for</span> <span class="n">filter_shape</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shapes</span><span class="p">:</span>
      <span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">filter_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">assert</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">filter_set</span>
      <span class="n">filter_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
      <span class="k">assert</span> <span class="n">input_shape</span> <span class="o">==</span> <span class="n">filter_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="n">params_conv_set</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">filter_shape</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shapes</span><span class="p">:</span>
      <span class="n">conv_p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cnn_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">conv_p</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">filter_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">filter_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># Important: combined quantization will be done pre-concat versus</span>
      <span class="c1"># by each layer on its output. Otherwise, inherit quantization params</span>
      <span class="c1"># from this layer.</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">conv_p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">qdomain</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
      <span class="n">conv_p</span><span class="o">.</span><span class="n">disable_activation_quantization</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">conv_p</span><span class="o">.</span><span class="n">filter_shape</span> <span class="o">=</span> <span class="n">filter_shape</span>
      <span class="n">params_conv_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_p</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;conv_set&#39;</span><span class="p">,</span> <span class="n">params_conv_set</span><span class="p">)</span>

<div class="viewcode-block" id="ConvSetLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSetLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="c1"># The same QTensor is used for all inputs to the concat.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">TrackQTensor</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="ConvSetLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ConvSetLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply all convolution sets to inputs and concatenate outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: The inputs tensor. It is expected to be of shape [batch, time,</span>
<span class="sd">        frequency, channel]. The time dimension corresponds to the height</span>
<span class="sd">        dimension as in images and the frequency dimension corresponds to the</span>
<span class="sd">        width dimension as in images.</span>
<span class="sd">      paddings: The paddings tensor. It is expected to be of shape [batch,</span>
<span class="sd">        time].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (out, output_paddings).</span>

<span class="sd">      - out: output tensor. Expected to be of shape [batch, time_mod,</span>
<span class="sd">        frequency_mod, out_channel_1 + out_channel_2 ...] where time_mod and</span>
<span class="sd">        frequency_mod depend on the conv layer strides and out_channel_i is</span>
<span class="sd">        the output channel size of the i-th conv layer in the set.</span>
<span class="sd">      - output_paddings: Modified paddings generated within `ConvLayer.FProp`.</span>
<span class="sd">        Expected to be of the shape [batch, time_mod].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]]],</span> <span class="mi">0</span><span class="p">))</span>
    <span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="n">conv_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_paddings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># output_padding should be same for all filters for the same stride.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">conv_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_set</span><span class="p">):</span>
      <span class="n">conv_i_output</span><span class="p">,</span> <span class="n">conv_i_padding</span> <span class="o">=</span> <span class="n">conv_i</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">conv_set</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">inputs</span><span class="p">,</span>
                                                   <span class="n">paddings</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">output_paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_paddings</span> <span class="o">=</span> <span class="n">conv_i_padding</span>
      <span class="n">conv_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_i_output</span><span class="p">)</span>

    <span class="c1"># Track for quantization.</span>
    <span class="n">conv_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">QTensor</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">conv_outputs</span><span class="p">]</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">conv_outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">output_paddings</span></div></div>


<div class="viewcode-block" id="LocalizedLabelSmoother"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LocalizedLabelSmoother">[docs]</a><span class="k">class</span> <span class="nc">LocalizedLabelSmoother</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Smooths labels given as class ids.</span>

<span class="sd">  Implements the smoothing from https://arxiv.org/abs/1612.02695. Instead of</span>
<span class="sd">  1-hot class ids the model is trained to predict a distribution over classes</span>
<span class="sd">  that includes the correct class label and with a small probability the labels</span>
<span class="sd">  of tokens that appear nearby in time in the ground truth. This typically acts</span>
<span class="sd">  as a strong regularizer.</span>

<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LocalizedLabelSmoother.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LocalizedLabelSmoother.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of classes&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;offsets&#39;</span><span class="p">,</span> <span class="p">[],</span> <span class="s1">&#39;Offset (over time) for smoothing. At time T the &#39;</span>
        <span class="s1">&#39;smoothed target is class[T] + sum_i weights[i]*class[T+offset[i]]&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="p">[],</span> <span class="s1">&#39;Weight of the smoothing at corresponding offset&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">offsets</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

<div class="viewcode-block" id="LocalizedLabelSmoother.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LocalizedLabelSmoother.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span> <span class="n">target_labels</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert class_ids to 1hot and smooth by neighborhood.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      target_paddings: float32 matrix [bs, seq_len]</span>
<span class="sd">      target_labels: int32 matrix [bs, seq_len]. This stores the target label</span>
<span class="sd">        output at each decoder step as generated by the speech input generator</span>
<span class="sd">        input_batch.tgt.labels</span>
<span class="sd">      target_ids: int32 matrix [bs, seq_len]. This stores the target_id that is</span>
<span class="sd">        fed to the decoder, as generated by the speech input generator</span>
<span class="sd">        input_batch.tgt.ids</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor [bs, seq_len, num_classes] denoting a smoothed distribution over</span>
<span class="sd">      num_classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">target_ids</span>  <span class="c1"># Unused.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">target_labels</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

    <span class="c1"># Start list keeping the scaled class-probabilities at different offsets.</span>
    <span class="n">output_distributions</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_probabilities</span><span class="p">]</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># If offsets &lt; 0 we force a future output_act to be like a past token.</span>
    <span class="c1"># If offsets &gt; 0 we force a past output_act to be like a future token.</span>
    <span class="n">min_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">offsets</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">max_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">offsets</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">,</span>
                                 <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">min_offset</span><span class="p">,</span> <span class="n">max_offset</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="c1"># Shift the weights to the left by one location - we don&#39;t make the</span>
    <span class="c1"># EOS more probable.</span>
    <span class="n">class_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">target_paddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span>
                           <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">min_offset</span><span class="p">,</span> <span class="n">max_offset</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">class_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">class_weights</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">offset</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">offsets</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
      <span class="n">offset_in_padded</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">-</span> <span class="n">min_offset</span>
      <span class="n">output_distributions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">class_probabilities</span><span class="p">[:,</span> <span class="n">offset_in_padded</span><span class="p">:</span><span class="n">offset_in_padded</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
          <span class="o">*</span> <span class="n">class_weights</span><span class="p">[:,</span> <span class="n">offset_in_padded</span><span class="p">:</span><span class="n">offset_in_padded</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span>
          <span class="n">weight</span><span class="p">)</span>
    <span class="n">output_distributions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">output_distributions</span><span class="p">)</span>
    <span class="n">output_distributions</span> <span class="o">/=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
        <span class="n">output_distributions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_distributions</span></div></div>


<div class="viewcode-block" id="UniformLabelSmoother"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.UniformLabelSmoother">[docs]</a><span class="k">class</span> <span class="nc">UniformLabelSmoother</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Smooths labels given as class ids and confidence.</span>

<span class="sd">  Implements the smoothing from https://arxiv.org/abs/1512.00567. Correct class</span>
<span class="sd">  label confidence is dropped by eps and all the other classes are increased</span>
<span class="sd">  by eps/num_classes.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="UniformLabelSmoother.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.UniformLabelSmoother.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_classes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of classes&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;uncertainty&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;Uncertainty of correct label, eps.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;uncertainty_larger&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">&#39;Apply a larger uncertainty to specific tokens, as specified &#39;</span>
        <span class="s1">&#39;by token_from_target_ids.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;token_id_uncertainty_larger&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Id of token from target_ids &#39;</span>
             <span class="s1">&#39;to apply uncertainty_larger to.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">token_id_uncertainty_larger</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">token_id_uncertainty_larger</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

<div class="viewcode-block" id="UniformLabelSmoother.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.UniformLabelSmoother.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">target_paddings</span><span class="p">,</span> <span class="n">target_labels</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert target_labels to 1hot and smooth uniformly.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      target_paddings: float32 matrix [bs, seq_len]</span>
<span class="sd">      target_labels: int32 matrix [bs, seq_len]. This stores the target label</span>
<span class="sd">        output at each decoder step as generated by the speech input generator</span>
<span class="sd">        input_batch.tgt.labels</span>
<span class="sd">      target_ids: int32 matrix [bs, seq_len]. This stores the target_id that is</span>
<span class="sd">        fed to the decoder, as generated by the speech input generator</span>
<span class="sd">        input_batch.tgt.ids</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of float32 [bs, seq_len, num_classes] denoting a smoothed</span>
<span class="sd">      distribution over num_classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">target_paddings</span>  <span class="c1"># Unused by FProp.</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">low_confidence</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">high_confidence</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty</span><span class="p">)</span>

    <span class="n">smooth_targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">target_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="n">depth</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
        <span class="n">on_value</span><span class="o">=</span><span class="n">high_confidence</span><span class="p">,</span>
        <span class="n">off_value</span><span class="o">=</span><span class="n">low_confidence</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">token_id_uncertainty_larger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">target_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">low_confidence_larger</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty_larger</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">high_confidence_larger</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">uncertainty_larger</span><span class="p">)</span>
      <span class="n">smooth_targets_larger</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">target_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
          <span class="n">depth</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
          <span class="n">on_value</span><span class="o">=</span><span class="n">high_confidence_larger</span><span class="p">,</span>
          <span class="n">off_value</span><span class="o">=</span><span class="n">low_confidence_larger</span><span class="p">)</span>
      <span class="n">should_smooth_larger</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">target_ids</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">token_id_uncertainty_larger</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
          <span class="n">multiples</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span>
      <span class="n">smooth_targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">should_smooth_larger</span><span class="p">,</span> <span class="n">smooth_targets_larger</span><span class="p">,</span>
                                <span class="n">smooth_targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">smooth_targets</span></div></div>


<div class="viewcode-block" id="HighwaySkipLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.HighwaySkipLayer">[docs]</a><span class="k">class</span> <span class="nc">HighwaySkipLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A highway skip layer.</span>

<span class="sd">  This class represents a highway skip layer, which takes multiple</span>
<span class="sd">  inputs (from different layers of the network) and gates them.</span>
<span class="sd">  This returns C(x)x + T(x)h, initially biasing C to be open.</span>
<span class="sd">  For some discussion about initialization please see:</span>
<span class="sd">  Section 2.2 in [Srivastava, 2015]: https://arxiv.org/pdf/1505.00387v2.pdf</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="HighwaySkipLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.HighwaySkipLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input to the network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether or not to apply BN to the highway skip layer output. &#39;</span>
        <span class="s1">&#39;Note this is only a single bool.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;carry_bias_init&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;carry gates bias initialization&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;couple_carry_transform_gates&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Boolean on whether to couple the transform and carry gates.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">carry_gate_params</span> <span class="o">=</span> <span class="n">ProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span>
        <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">bias_init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">carry_bias_init</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_carry_gate&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;carry_gate&#39;</span><span class="p">,</span> <span class="n">carry_gate_params</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">couple_carry_transform_gates</span><span class="p">:</span>
      <span class="n">transform_gate_params</span> <span class="o">=</span> <span class="n">ProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
          <span class="n">batch_norm</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span>
          <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span>
          <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
          <span class="n">bias_init</span><span class="o">=-</span><span class="n">p</span><span class="o">.</span><span class="n">carry_bias_init</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_transform_gate&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;transform_gate&#39;</span><span class="p">,</span> <span class="n">transform_gate_params</span><span class="p">)</span>

<div class="viewcode-block" id="HighwaySkipLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.HighwaySkipLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">transformed_x</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fprop for Highway Skip layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      x: feature at the lower layer.</span>
<span class="sd">      transformed_x: transformation of x at a higher layer.</span>
<span class="sd">      paddings: padding applied to the features.</span>

<span class="sd">    Returns:</span>
<span class="sd">      layer_out - activations after forward propagation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">carry_gate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">carry</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">carry_gate</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">carry_gate</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">couple_carry_transform_gates</span><span class="p">:</span>
      <span class="n">transform</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">carry</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_gate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_gate</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">transform_gate</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">layer_out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">carry</span> <span class="o">+</span> <span class="n">transformed_x</span> <span class="o">*</span> <span class="n">transform</span>
    <span class="k">return</span> <span class="n">layer_out</span></div></div>


<div class="viewcode-block" id="GatingLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatingLayer">[docs]</a><span class="k">class</span> <span class="nc">GatingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A gating layer.</span>

<span class="sd">  This class represents a gating layer, which takes 2 inputs of the same shape</span>
<span class="sd">  and gates them.</span>

<span class="sd">  The output is: carry * x + (1 - carry) * y where, carry is given by</span>
<span class="sd">  sigmoid(x @ w_1 + y @ w_2 + bias).</span>

<span class="sd">  This is different from the HighwaySkipLayer above in that carry is also a</span>
<span class="sd">  function of y (named transformed_x in HighwaySkipLayer).</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GatingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input to the network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;has_bias&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether carry has a bias term.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;carry_bias_init&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;carry gates bias initialization&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">carry_gate_params</span> <span class="o">=</span> <span class="n">ProjectionLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">has_bias</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">bias_init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">carry_bias_init</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;carry&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;carry_gate&#39;</span><span class="p">,</span> <span class="n">carry_gate_params</span><span class="p">)</span>

<div class="viewcode-block" id="GatingLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatingLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fprop for the gating layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      x: An input feature, the last dimension must match p.input_dim.</span>
<span class="sd">      y: Another input feature. Must have the same shape as &#39;x&#39;.</span>
<span class="sd">      paddings: padding applied to the features. When x and y have shape [...,</span>
<span class="sd">        input_dim], &#39;paddings&#39;, when specified, must have shaped [..., 1], where</span>
<span class="sd">        all but the last dimension match.</span>

<span class="sd">    Returns:</span>
<span class="sd">      layer_out - activations after forward propagation. Same shape as x and y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">))],</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">carry</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">carry_gate</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">carry_gate</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
                                  <span class="n">paddings</span><span class="p">)</span>
    <span class="n">layer_out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">carry</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">carry</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_out</span></div></div>


<div class="viewcode-block" id="GradNormTracker"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GradNormTracker">[docs]</a><span class="k">class</span> <span class="nc">GradNormTracker</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper class to keep track of gradient norm stats.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="GradNormTracker.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GradNormTracker.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;decay&#39;</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">,</span>
             <span class="s1">&#39;Decay in updating the moving avgs in grad norm stats&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;grad_norm_lower_cap&#39;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="s1">&#39;The minimal gradient norm value.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;clip_threshold&#39;</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="s1">&#39;Distance threshold at which gradients are clipped to 0.0.&#39;</span>
        <span class="s1">&#39; Distance is measured in the number of standard deviations a&#39;</span>
        <span class="s1">&#39; given gradient norm is from the mean gradient norm. The&#39;</span>
        <span class="s1">&#39; default value of 4.0 means we are throwing away roughly&#39;</span>
        <span class="s1">&#39; 0.15</span><span class="si">% o</span><span class="s1">f steps.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;grad_norm_clip_cap_min&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;We stop clipping if grad norm is already smaller than this&#39;</span>
        <span class="s1">&#39; value.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dry_run&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If True, always return 1.0 in FProp() to signify &#39;</span>
        <span class="s1">&#39;no grad clipping suggested, in which case the class only collects &#39;</span>
        <span class="s1">&#39;stats and summaries.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_decay</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">decay</span>

<div class="viewcode-block" id="GradNormTracker._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GradNormTracker._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>

    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;log_mean&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;log_mean_squared&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;total_weight&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;total_rejections&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradNormTracker.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GradNormTracker.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">has_nan</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update gradient norm moving avgs, and returns whether or not ...</span>

<span class="sd">    to clip gradients to 0.0. If the current batch has NaN grads, does not</span>
<span class="sd">    update the moving avgs and forces to clip the gradients to 0.0.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      grad_norm: A float scalar tensor.</span>
<span class="sd">      has_nan: A boolean scalar tensor to indicate if the current batch has nan.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A scalar float tensor with value of either 1.0 or 0.0. The value of 0.0</span>
<span class="sd">      means the gradient norm is excessively large or contains NaN, and the step</span>
<span class="sd">      should be aborted completely.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">grad_norm_lower_cap</span><span class="p">)</span>

      <span class="c1"># Exponentially decayed moving avg of log(grad_norm) mean.</span>
      <span class="n">mean</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">log_mean</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">total_weight</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
      <span class="c1"># Exponentially decayed moving avg of log(grad_norm) variance.</span>
      <span class="n">var</span> <span class="o">=</span> <span class="p">((</span><span class="n">theta</span><span class="o">.</span><span class="n">log_mean_squared</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">total_weight</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">))</span> <span class="o">-</span>
             <span class="n">mean</span> <span class="o">*</span> <span class="n">mean</span><span class="p">)</span>
      <span class="n">std</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">))</span>

      <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;log_grad_norm_mean&#39;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
      <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;log_grad_norm_std&#39;</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
      <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;clip_ratio_threshold&#39;</span><span class="p">,</span>
                           <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">std</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">clip_threshold</span><span class="p">))</span>
      <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;clip_threshold&#39;</span><span class="p">,</span>
                           <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">clip_threshold</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
      <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;total_rejections&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">total_rejections</span><span class="p">)</span>

      <span class="n">log_grad_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">grad_norm</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
      <span class="n">log_grad_norm_cap</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">clip_threshold</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">log_grad_norm_cap_min</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad_norm_clip_cap_min</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
      <span class="n">log_grad_norm_cap</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">log_grad_norm_cap</span><span class="p">,</span> <span class="n">log_grad_norm_cap_min</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">UpdateExpMovingAvg</span><span class="p">(</span><span class="n">ref_var</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">ignore</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ignore</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">delta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ignore</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([]),</span>
                           <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">decay</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">val</span> <span class="o">-</span> <span class="n">ref_var</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">decay</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">val</span> <span class="o">-</span> <span class="n">ref_var</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">ref_var</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>

      <span class="c1"># We trigger when total_weight is at least half of max weight or the</span>
      <span class="c1"># current batch contains NaNs.</span>
      <span class="n">trigger</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">log_grad_norm</span> <span class="o">&gt;</span> <span class="n">log_grad_norm_cap</span><span class="p">,</span>
                                    <span class="n">theta</span><span class="o">.</span><span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mf">0.75</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">has_nan</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trigger</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">trigger</span><span class="p">,</span> <span class="n">has_nan</span><span class="p">)</span>

      <span class="n">log_grad_norm_capped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">log_grad_norm</span><span class="p">,</span> <span class="n">log_grad_norm_cap</span><span class="p">)</span>

      <span class="n">update_moving_avg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
          <span class="n">UpdateExpMovingAvg</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">log_mean</span><span class="p">,</span> <span class="n">log_grad_norm_capped</span><span class="p">,</span> <span class="n">has_nan</span><span class="p">),</span>
          <span class="n">UpdateExpMovingAvg</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">log_mean_squared</span><span class="p">,</span>
                             <span class="n">log_grad_norm_capped</span> <span class="o">*</span> <span class="n">log_grad_norm_capped</span><span class="p">,</span>
                             <span class="n">has_nan</span><span class="p">),</span>
          <span class="n">UpdateExpMovingAvg</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">has_nan</span><span class="p">),</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="o">.</span><span class="n">total_rejections</span><span class="p">,</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">trigger</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>

      <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span><span class="n">update_moving_avg</span><span class="p">],</span>
                                        <span class="mf">1.0</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dry_run</span> <span class="k">else</span> <span class="mf">1.0</span> <span class="o">-</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">trigger</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="WeightedSumLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.WeightedSumLayer">[docs]</a><span class="k">class</span> <span class="nc">WeightedSumLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the weighted sum of a list of input tensors.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="WeightedSumLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.WeightedSumLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Params for this MergerLayer class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_sources&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of input sources to combine.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;weighted_merger_dropout_prob&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span>
             <span class="s1">&#39;Applies dropout to the weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weighted_merger_softmax&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;If set, applies a softmax &#39;</span>
        <span class="s1">&#39;layer on top of the weights for normalization.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;global_weight_scale&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;A global scale put on weights.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;minimal_prob&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;The minimal weight for each component.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;add_weight_summaries&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;If set, creates summaries for the &#39;</span>
             <span class="s1">&#39;sum weights.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Layer must have a specified name!&#39;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Must specify num_sources &gt; 0.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weighted_merger_dropout_prob</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
      <span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
      <span class="n">dropout_tpl</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="o">.</span><span class="n">weighted_merger_dropout_prob</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;weighted_merger_dropout&#39;</span><span class="p">,</span> <span class="n">dropout_tpl</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;weighted_merger_dropout&#39;</span><span class="p">,</span> <span class="n">IdentityLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">())</span>

<div class="viewcode-block" id="WeightedSumLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.WeightedSumLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">params_init</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># Weights to be learned.</span>
    <span class="n">pw</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;sum_weight&#39;</span><span class="p">,</span> <span class="n">pw</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightedSumLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.WeightedSumLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the list of input tensors into a single tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      inputs: A list of tensors of shape [time, batch, hidden_dim]</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor of the same shape with input tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">n_sources</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_sources</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Weighted sum of all sources, all dims must match.</span>
    <span class="c1"># For weighted_sum, assume input is a list of rank 3 tensors</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasRank</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="c1"># The constant factor is just meant to support the non-normalized scenario.</span>
    <span class="c1"># If softmax is applied, this factor will cancel out.</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">sum_weight</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">global_weight_scale</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_merger_dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">weighted_merger_dropout</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">weighted_merger_softmax</span><span class="p">:</span>
      <span class="n">residual_weights</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">minimal_prob</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_sources</span>
      <span class="k">assert</span> <span class="n">residual_weights</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>
      <span class="k">assert</span> <span class="n">residual_weights</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
      <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">residual_weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">minimal_prob</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">add_weight_summaries</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">):</span>
        <span class="n">summary_utils</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;weight_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_sources</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="GatedAverageLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatedAverageLayer">[docs]</a><span class="k">class</span> <span class="nc">GatedAverageLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gated combination of n input vectors.</span>

<span class="sd">  Given n inputs, x_1 ... x_n. First learns a gate g in a single layer.</span>
<span class="sd">  Returns g_1 * x_1 + ... g_n * x_n.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GatedAverageLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatedAverageLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_nodes&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of nodes in each input vector.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_inputs&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of input vectors to combine.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes GatedAverageLayer.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_nodes</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of dimensions should be greater than 0.&#39;</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of inputs should be greater than 0.&#39;</span>

<div class="viewcode-block" id="GatedAverageLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatedAverageLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">in_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_nodes</span>

    <span class="c1"># Weight matrix for scalar gates</span>
    <span class="n">gm_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">in_size</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_VariableCollections</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;gm&#39;</span><span class="p">,</span> <span class="n">gm_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="GatedAverageLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GatedAverageLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gates, then merges a list of n input vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: gm (gate matrix)</span>
<span class="sd">      inputs: List of inputs, each of shape [..., num_nodes]</span>

<span class="sd">    Returns:</span>
<span class="sd">      a gated output vector [..., num_nodes]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">,</span> <span class="s1">&#39;Number of inputs should match params.&#39;</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">([</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inp</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">]),</span>
          <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
      <span class="p">],</span> <span class="n">inp</span><span class="p">)</span>

    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">reshaped_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">])</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="n">concat_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">reshaped_inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">xmg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(</span><span class="n">concat_inputs</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">gm</span><span class="p">))</span>
    <span class="n">xmg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">xmg</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">concat_inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">])</span>
    <span class="n">gated_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">xmg</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">gated_sum</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LHUCLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LHUCLayer">[docs]</a><span class="k">class</span> <span class="nc">LHUCLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;`Learning Hidden Unit Contribution (LHUC)` layer.</span>

<span class="sd">  This paper proposes to use LHUC layer for NMT adaptation:</span>
<span class="sd">      http://aclweb.org/anthology/N18-2080</span>

<span class="sd">  During base model training, LHUC layer is fixed to 1.0 (no-op in</span>
<span class="sd">  multiplication). During adaptation, only LHUC layer is trained, and all other</span>
<span class="sd">  parameters in the model are frozen.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LHUCLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LHUCLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input and output.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>

<div class="viewcode-block" id="LHUCLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LHUCLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_VariableCollections</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="LHUCLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.LHUCLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add learnt gate for adaptation.&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">inp</span>
    <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="ResidualAdapterLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ResidualAdapterLayer">[docs]</a><span class="k">class</span> <span class="nc">ResidualAdapterLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Residual Adapter layer for NLP tasks.</span>

<span class="sd">  This paper proposes using residual adapters for fine-tuning new tasks on BERT.</span>
<span class="sd">  https://arxiv.org/pdf/1902.00751.pdf</span>

<span class="sd">  During adaptation, residual adapter layers can be added to a pre-trained</span>
<span class="sd">  model and trained, while all other parameters are frozen.</span>
<span class="sd">  In terms of operations, the layer is identical to a vanilla Transformer</span>
<span class="sd">  feedforward layer. Separate implementation is meant to distinguish function.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ResidualAdapterLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ResidualAdapterLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input to the adapter.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bottleneck_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the feedforward inner layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Layer norm default params.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

    <span class="n">bottleneck_params</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bottleneck&#39;</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;RELU&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">],</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">hidden_layer_dims</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;bottleneck&#39;</span><span class="p">,</span> <span class="n">bottleneck_params</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;adapter_ln&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="ResidualAdapterLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.ResidualAdapterLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fprop for Residual Adapter.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A `.NestedMap` object containing weights&#39; values of this layer and</span>
<span class="sd">        its children layers.</span>
<span class="sd">      x: [..., input_dim].</span>
<span class="sd">      paddings: padding applied to the features.</span>

<span class="sd">    Returns:</span>
<span class="sd">      layer_out - [..., input_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">normalized_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">bottleneck_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">,</span> <span class="n">normalized_x</span><span class="p">,</span>
                                         <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">bottleneck_x</span></div></div>


<div class="viewcode-block" id="Conv2DFlops"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DFlops">[docs]</a><span class="k">def</span> <span class="nf">Conv2DFlops</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filter_shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns number of float operations (mult/adds) for a Conv2D op.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: the input shape. Must have four elements.</span>
<span class="sd">    filter_shape: the convolution filter shape. Must have four elements.</span>
<span class="sd">    stride: the strides along height and width, respectively.</span>
<span class="sd">    padding: &#39;SAME&#39; or &#39;VALID&#39;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Number of multiplications and additions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
  <span class="n">fh</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">ic</span><span class="p">,</span> <span class="n">oc</span> <span class="o">=</span> <span class="n">filter_shape</span>
  <span class="n">sh</span><span class="p">,</span> <span class="n">sw</span> <span class="o">=</span> <span class="n">stride</span>

  <span class="k">def</span> <span class="nf">_CeilDiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">floordiv</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s1">&#39;SAME&#39;</span><span class="p">:</span>
    <span class="n">oh</span> <span class="o">=</span> <span class="n">_CeilDiv</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sh</span><span class="p">)</span>
    <span class="n">ow</span> <span class="o">=</span> <span class="n">_CeilDiv</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sw</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">padding</span> <span class="o">==</span> <span class="s1">&#39;VALID&#39;</span>
    <span class="n">oh</span> <span class="o">=</span> <span class="n">_CeilDiv</span><span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">fh</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sh</span><span class="p">)</span>
    <span class="n">ow</span> <span class="o">=</span> <span class="n">_CeilDiv</span><span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="n">fw</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sw</span><span class="p">)</span>
  <span class="c1"># Mul/add counts as 2 flops.</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">oh</span> <span class="o">*</span> <span class="n">ow</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">fh</span> <span class="o">*</span> <span class="n">fw</span> <span class="o">*</span> <span class="n">ic</span> <span class="o">*</span> <span class="n">oc</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DLayerNoPadding"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayerNoPadding">[docs]</a><span class="k">class</span> <span class="nc">Conv2DLayerNoPadding</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;2-D Convolution layer w/o padding.</span>

<span class="sd">  TODO(laurenzo): Dedup in favor of SeparableConv2DLayer where possible.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Conv2DLayerNoPadding.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayerNoPadding.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter shape. Must be a sequence of length 4. Elements are in&#39;</span>
        <span class="s1">&#39; the order of height (time), width (frequency), in_channel,&#39;</span>
        <span class="s1">&#39; out_channel. &#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;filter_stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;Filter stride to use. Must be a pair of ints. The first int&#39;</span>
        <span class="s1">&#39; specifies the stride on the height dimension. The second int&#39;</span>
        <span class="s1">&#39; specifies the stride on the width dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39; An optional list of ints. Defaults to [1, 1]. &#39;</span>
        <span class="s1">&#39;1-D tensor of length 2. The dilation factor for each dimension &#39;</span>
        <span class="s1">&#39;of input. If set to k &gt; 1, there will be k-1 skipped cells &#39;</span>
        <span class="s1">&#39;between each filter element on that dimension.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME|VALID&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">padding</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;VALID&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dilations</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">)</span>

<div class="viewcode-block" id="Conv2DLayerNoPadding._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayerNoPadding._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="Conv2DLayerNoPadding.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayerNoPadding.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply convolution to inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap object containing weights&#39; values of this layer and its</span>
<span class="sd">        children layers.</span>
<span class="sd">      x: The inputs tensor. It is expected to be of shape [batch, height, width,</span>
<span class="sd">        channel].</span>

<span class="sd">    Returns:</span>
<span class="sd">      Convolution output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
      <span class="n">computation_cost</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span>
          <span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;flops&#39;</span><span class="p">,</span>
          <span class="n">Conv2DFlops</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
              <span class="n">filter_shape</span><span class="o">=</span><span class="n">symbolic</span><span class="o">.</span><span class="n">EvalExpr</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">TENSOR_VALUES</span><span class="p">,</span>
                                             <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span><span class="p">),</span>
              <span class="n">stride</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">,</span>
              <span class="n">padding</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
          <span class="n">filters</span><span class="o">=</span><span class="n">theta</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
          <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span>
          <span class="n">padding</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
          <span class="n">dilations</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">dilations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">dilations</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">],</span>
          <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Conv2DLayerNoPadding.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.Conv2DLayerNoPadding.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">fh</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">ic</span><span class="p">,</span> <span class="n">oc</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_shape</span>
    <span class="k">assert</span> <span class="n">ic</span> <span class="o">==</span> <span class="n">c</span>
    <span class="n">sh</span><span class="p">,</span> <span class="n">sw</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">filter_stride</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s1">&#39;SAME&#39;</span><span class="p">:</span>
      <span class="n">oh</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">ceiling</span><span class="p">(</span><span class="n">h</span> <span class="o">/</span> <span class="n">sh</span><span class="p">)</span>
      <span class="n">ow</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">ceiling</span><span class="p">(</span><span class="n">w</span> <span class="o">/</span> <span class="n">sw</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">oh</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">ceiling</span><span class="p">((</span><span class="n">h</span> <span class="o">-</span> <span class="n">fh</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sh</span><span class="p">)</span>
      <span class="n">ow</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">ceiling</span><span class="p">((</span><span class="n">w</span> <span class="o">-</span> <span class="n">fw</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sw</span><span class="p">)</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">oh</span> <span class="o">*</span> <span class="n">ow</span> <span class="o">*</span> <span class="n">fh</span> <span class="o">*</span> <span class="n">fw</span> <span class="o">*</span> <span class="n">ic</span> <span class="o">*</span> <span class="n">oc</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># mul/add counts as 2 flop.</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="n">b</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">oc</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">outputs</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="FetchLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FetchLayer">[docs]</a><span class="k">class</span> <span class="nc">FetchLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A layer facilitating fetching activations and their gradients.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activations</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="FetchLayer.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FetchLayer.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="n">args</span><span class="p">)</span></div>

<div class="viewcode-block" id="FetchLayer._ReturnSingleValueOrList"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FetchLayer._ReturnSingleValueOrList">[docs]</a>  <span class="k">def</span> <span class="nf">_ReturnSingleValueOrList</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">lst</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lst</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReturnSingleValueOrList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_activations</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReturnSingleValueOrList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span><span class="p">)</span>

<div class="viewcode-block" id="FetchLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.FetchLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">theta</span>
    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activations</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">FetchBak</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">i</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">dys</span>
        <span class="k">return</span> <span class="n">dys</span>

      <span class="k">def</span> <span class="nf">FetchFwd</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_activations</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">CallDefun</span><span class="p">(</span><span class="n">FetchFwd</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">bak</span><span class="o">=</span><span class="n">FetchBak</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_activations</span><span class="p">)</span> <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div></div>


<div class="viewcode-block" id="GluLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GluLayer">[docs]</a><span class="k">class</span> <span class="nc">GluLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gated Linear Unit.</span>

<span class="sd">  See https://arxiv.org/abs/1612.08083 for more details.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GluLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GluLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the layer input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the layer output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ln_tpl&#39;</span><span class="p">,</span> <span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Layer norm default params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dense_tpl&#39;</span><span class="p">,</span> <span class="n">FCLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(),</span> <span class="s1">&#39;Fully connected layer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;RELU&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Non-linearity applied after the dense layer in the value branch.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dropout_tpl&#39;</span><span class="p">,</span> <span class="n">DropoutLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Dropout applied to output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;apply_residual&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Whether or not to add inputs to outputs.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span><span class="p">:</span>
      <span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">apply_residual</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">output_dim</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>

    <span class="c1"># Initialize value feed-forward layer.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dense_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;value_layer&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">params</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">activation</span>
    <span class="n">params</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;value_layer&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize gate feed-forward layer.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dense_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;gate_layer&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">params</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;SIGMOID&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;gate_layer&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize layer norm.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ln_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer_norm&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Initialize dropout.</span>
    <span class="n">dropout_tpl</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dropout_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">dropout_tpl</span><span class="p">)</span>

<div class="viewcode-block" id="GluLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.GluLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
    <span class="n">inputs_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">value_layer</span><span class="p">,</span> <span class="n">inputs_normalized</span><span class="p">,</span>
                                    <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">gates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_layer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">gate_layer</span><span class="p">,</span> <span class="n">inputs_normalized</span><span class="p">,</span>
                                  <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">glu_output</span> <span class="o">=</span> <span class="n">values</span> <span class="o">*</span> <span class="n">gates</span>
    <span class="n">glu_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">glu_output</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">apply_residual</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">glu_output</span>
    <span class="k">return</span> <span class="n">glu_output</span></div></div>


<div class="viewcode-block" id="MultitaskAdapterBaseLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterBaseLayer">[docs]</a><span class="k">class</span> <span class="nc">MultitaskAdapterBaseLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Residual adapter layer for multilingual models.</span>

<span class="sd">  Residual adapters can be used to fine-tune a single model to multiple</span>
<span class="sd">  domains, tasks, or languages: https://arxiv.org/pdf/1902.00751.pdf</span>

<span class="sd">  Each adapter consists of a &quot;down&quot; projection to a smaller dimension followed</span>
<span class="sd">  by an &quot;up&quot; projection, the result of which is added back to the input</span>
<span class="sd">  activation.  The projection weights and biases are task-specific.</span>

<span class="sd">  Whereas ResidualAdapterLayer learns and applies the parameters for a single</span>
<span class="sd">  task, this layer learns and applies the parameters for multiple tasks so that</span>
<span class="sd">  we have a single model serving the different tasks. The parameters can be</span>
<span class="sd">  trained for all tasks at the same time, or in one-off per-task training jobs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultitaskAdapterBaseLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterBaseLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_tasks&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of tasks.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the input to the adapter.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;bottleneck_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Dimension of the bottleneck.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;layer_norm_tpl&#39;</span><span class="p">,</span> <span class="n">LayerNorm</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Layer norm default params.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s1">&#39;TBC&#39;</span><span class="p">,</span> <span class="s1">&#39;String(enum) specifying the input and output &#39;</span>
        <span class="s1">&#39;data format for this layer. Supported formats: &#39;</span>
        <span class="s1">&#39;&quot;TBC&quot;: [time, batch, input_dim] and &quot;BTC&quot;: [batch, time, input_dim].&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_task_ids&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;If True, clips the given task ids to [0, p.num_tasks - 1].&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div></div>


<div class="viewcode-block" id="MultitaskAdapterLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterLayer">[docs]</a><span class="k">class</span> <span class="nc">MultitaskAdapterLayer</span><span class="p">(</span><span class="n">MultitaskAdapterBaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;MultitaskAdapterBaseLayer implemented with EmbeddingLayers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultitaskAdapterLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;projection_params_init&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Weight initialization for up and down projections. Only used for &#39;</span>
        <span class="s1">&#39;weights, not biases.  If None, uses default weight init, which is &#39;</span>
        <span class="s1">&#39;typically Xavier with scale of 1.0.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># Data format is either &#39;TBC&#39; (time-major) or &#39;BTC&#39; (batch-major).</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;TBC&#39;</span><span class="p">,</span> <span class="s1">&#39;BTC&#39;</span><span class="p">)</span>
    <span class="n">base_emb_params</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">max_num_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">down_proj_w_params</span> <span class="o">=</span> <span class="n">base_emb_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">down_proj_w_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;down_proj_w&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">projection_params_init</span><span class="p">:</span>
      <span class="n">down_proj_w_params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">projection_params_init</span>
    <span class="n">down_proj_b_params</span> <span class="o">=</span> <span class="n">base_emb_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">down_proj_b_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;down_proj_b&#39;</span><span class="p">)</span>
    <span class="n">up_proj_w_params</span> <span class="o">=</span> <span class="n">base_emb_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">up_proj_w_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">embedding_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;up_proj_w&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">projection_params_init</span><span class="p">:</span>
      <span class="n">up_proj_w_params</span><span class="o">.</span><span class="n">params_init</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">projection_params_init</span>
    <span class="n">up_proj_b_params</span> <span class="o">=</span> <span class="n">base_emb_params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">up_proj_b_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;up_proj_b&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;down_proj_w&#39;</span><span class="p">,</span> <span class="n">down_proj_w_params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;down_proj_b&#39;</span><span class="p">,</span> <span class="n">down_proj_b_params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;up_proj_w&#39;</span><span class="p">,</span> <span class="n">up_proj_w_params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;up_proj_b&#39;</span><span class="p">,</span> <span class="n">up_proj_b_params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_norm_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;adapter_ln&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="MultitaskAdapterLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tasks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fprop for multitask adapter.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap object containing weights&#39; values of this layer and its</span>
<span class="sd">        children layers.</span>
<span class="sd">      inputs: A tensor containing the activations from the previous layer. For</span>
<span class="sd">        &#39;TBC&#39;, the shape is [time, batch, input_dim] and for &#39;BTC&#39;, it&#39;s [batch,</span>
<span class="sd">        time, input_dim].</span>
<span class="sd">      tasks: An int32 tensor containing the task ID for each input.  If &#39;tasks&#39;</span>
<span class="sd">        is of rank 2, we assume it to be of shape [time, batch] if &#39;BTC&#39; and</span>
<span class="sd">        [batch, time] if &#39;TBC&#39;, indicating a different task for each timestep.</span>
<span class="sd">        In this case we look up adapter params for each timestep.  If &#39;tasks&#39; is</span>
<span class="sd">        of rank 1, we assume it to be of shape [batch], indicating a single task</span>
<span class="sd">        for all timesteps of a sequence. This latter setup uses substantially</span>
<span class="sd">        less memory and is generally preferred.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor containing the adapted activations with shape</span>
<span class="sd">      [time, batch, input_dim] for &#39;TBC&#39; and [batch, time, input_dim] for &#39;BTC&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">per_timestep_task</span> <span class="o">=</span> <span class="p">(</span><span class="n">tasks</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;TBC&#39;</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">time_index</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">batch_index</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="c1"># Checks that inputs has 3 dimensions, last is hidden dim.</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">]),</span>
            <span class="c1"># Checks that inputs and tasks have same batch dimension.</span>
            <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">([</span><span class="n">inputs_shape</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]],</span> <span class="p">[</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tasks</span><span class="p">)[</span><span class="n">batch_index</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">per_timestep_task</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tasks</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">])</span>
        <span class="p">],</span>
        <span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">clip_task_ids</span><span class="p">:</span>
      <span class="n">tasks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># To support different task for each timetstep, flatten inputs and</span>
    <span class="c1"># tasks.  Below, &#39;batch&#39; now refers to flattened batch size, time * batch.</span>
    <span class="k">if</span> <span class="n">per_timestep_task</span><span class="p">:</span>
      <span class="n">tasks</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">with_dependencies</span><span class="p">(</span>
          <span class="p">[</span>
              <span class="c1"># Checks that inputs and tasks have same time dimension.</span>
              <span class="n">py_utils</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>
                                          <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tasks</span><span class="p">)[:</span><span class="mi">1</span><span class="p">])</span>
          <span class="p">],</span>
          <span class="n">tasks</span><span class="p">)</span>
      <span class="n">tasks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;TBC&#39;</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>

    <span class="c1"># Lookup all weights and biases</span>
    <span class="c1"># [batch] -&gt; [batch, hidden * k] -&gt; [batch, hidden, k]</span>
    <span class="n">down_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj_w</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">down_proj_w</span><span class="p">,</span> <span class="n">tasks</span><span class="p">),</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">])</span>
    <span class="c1"># [batch] -&gt; [batch, k] -&gt; [1, batch, k] if &#39;TBC&#39; else [batch, 1, k]</span>
    <span class="n">down_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj_b</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">down_proj_b</span><span class="p">,</span> <span class="n">tasks</span><span class="p">),</span> <span class="n">time_index</span><span class="p">)</span>
    <span class="c1"># [batch] -&gt; [batch, k * hidden] -&gt; [batch, k, hidden]</span>
    <span class="n">up_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj_w</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">up_proj_w</span><span class="p">,</span> <span class="n">tasks</span><span class="p">),</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">])</span>
    <span class="c1"># [batch] -&gt; [batch, h] -&gt; [1, batch, h] if &#39;TBC&#39; else [batch, 1, h]</span>
    <span class="n">up_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj_b</span><span class="o">.</span><span class="n">EmbLookup</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">up_proj_b</span><span class="p">,</span> <span class="n">tasks</span><span class="p">),</span> <span class="n">time_index</span><span class="p">)</span>

    <span class="c1"># Layer norm -&gt; down-projection -&gt; non-linearity -&gt; up-projection</span>
    <span class="n">norm_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># If per_timestep_task, t = 1, b = time * batch.</span>
    <span class="c1"># Otherwise, t = time, b = batch.</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;TBC&#39;</span><span class="p">:</span>
      <span class="n">down_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;tbh,bhk-&gt;tbk&#39;</span><span class="p">,</span> <span class="n">norm_inputs</span><span class="p">,</span> <span class="n">down_weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">down_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bth,bhk-&gt;btk&#39;</span><span class="p">,</span> <span class="n">norm_inputs</span><span class="p">,</span> <span class="n">down_weights</span><span class="p">)</span>
    <span class="n">down_projected</span> <span class="o">+=</span> <span class="n">down_biases</span>
    <span class="n">down_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">down_projected</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;TBC&#39;</span><span class="p">:</span>
      <span class="n">up_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;tbk,bkh-&gt;tbh&#39;</span><span class="p">,</span> <span class="n">down_projected</span><span class="p">,</span> <span class="n">up_weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">up_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;btk,bkh-&gt;bth&#39;</span><span class="p">,</span> <span class="n">down_projected</span><span class="p">,</span> <span class="n">up_weights</span><span class="p">)</span>
    <span class="n">up_projected</span> <span class="o">+=</span> <span class="n">up_biases</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">up_projected</span>

    <span class="c1"># Unflatten output:</span>
    <span class="c1">#   for &#39;TBC&#39;: [1, time * batch, hidden] -&gt; [time, batch, hidden]</span>
    <span class="c1">#   for &#39;BTC&#39;: [1, batch * time, hidden] -&gt; [batch, time, hidden]</span>
    <span class="k">if</span> <span class="n">per_timestep_task</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="MultitaskAdapterEinsumLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterEinsumLayer">[docs]</a><span class="k">class</span> <span class="nc">MultitaskAdapterEinsumLayer</span><span class="p">(</span><span class="n">MultitaskAdapterBaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;MultitaskAdapterBaseLayer implemented with Einsum.</span>

<span class="sd">  The embedding-based solution sometimes triggers b/175464137.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MultitaskAdapterEinsumLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterEinsumLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="s1">&#39;BTC&#39;</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;BTC&#39;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">layer_norm_tpl</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="MultitaskAdapterEinsumLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterEinsumLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">down_w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;down_w&#39;</span><span class="p">,</span> <span class="n">down_w_pc</span><span class="p">)</span>
    <span class="n">down_b_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;down_b&#39;</span><span class="p">,</span> <span class="n">down_b_pc</span><span class="p">)</span>
    <span class="n">up_w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;up_w&#39;</span><span class="p">,</span> <span class="n">up_w_pc</span><span class="p">)</span>
    <span class="n">up_b_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="s1">&#39;up_b&#39;</span><span class="p">,</span> <span class="n">up_b_pc</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultitaskAdapterEinsumLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.MultitaskAdapterEinsumLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tasks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fprop for multitask adapter.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: A NestedMap object containing weights&#39; values of this layer and its</span>
<span class="sd">        children layers.</span>
<span class="sd">      inputs: A tensor containing the activations from the previous layer.</span>
<span class="sd">        [batch, time, input_dim].</span>
<span class="sd">      tasks: An int32 tensor containing the task ID for each input. [batch].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor containing the adapted activations with the same shape as inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CastToFPropDtype</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">tasks</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">clip_task_ids</span><span class="p">:</span>
      <span class="n">tasks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># [batch, num_tasks].</span>
    <span class="n">tasks_onehot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tasks</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Einsum axis names:</span>
    <span class="c1"># b - batch</span>
    <span class="c1"># t - time</span>
    <span class="c1"># k - task</span>
    <span class="c1"># i - input_dim</span>
    <span class="c1"># n - bottleneck_dim</span>

    <span class="c1"># [batch, input_dim, bottleneck_dim].</span>
    <span class="n">down_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bk,kin-&gt;bin&#39;</span><span class="p">,</span> <span class="n">tasks_onehot</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">down_w</span><span class="p">)</span>
    <span class="c1"># [batch, 1, bottleneck_dim].</span>
    <span class="n">down_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bk,kn-&gt;bn&#39;</span><span class="p">,</span> <span class="n">tasks_onehot</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">down_b</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="c1"># [batch, bottleneck_dim, input_dim].</span>
    <span class="n">up_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bk,kni-&gt;bni&#39;</span><span class="p">,</span> <span class="n">tasks_onehot</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">up_w</span><span class="p">)</span>
    <span class="c1"># [batch, 1, input_dim].</span>
    <span class="n">up_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bk,ki-&gt;bi&#39;</span><span class="p">,</span> <span class="n">tasks_onehot</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">up_b</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># Layer norm -&gt; down-projection -&gt; non-linearity -&gt; up-projection</span>
    <span class="n">norm_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># [batch, time, bottleneck_dim].</span>
    <span class="n">down_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bti,bin-&gt;btn&#39;</span><span class="p">,</span> <span class="n">norm_inputs</span><span class="p">,</span> <span class="n">down_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">down_b</span>
    <span class="c1"># ReLU.</span>
    <span class="n">down_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">down_projected</span><span class="p">)</span>
    <span class="c1"># [batch, time, input_dim].</span>
    <span class="n">up_projected</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;btn,bni-&gt;bti&#39;</span><span class="p">,</span> <span class="n">down_projected</span><span class="p">,</span> <span class="n">up_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">up_b</span>
    <span class="c1"># Residual.</span>
    <span class="k">return</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">up_projected</span></div></div>


<div class="viewcode-block" id="CCTGatingNetwork"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CCTGatingNetwork">[docs]</a><span class="k">class</span> <span class="nc">CCTGatingNetwork</span><span class="p">(</span><span class="n">quant_utils</span><span class="o">.</span><span class="n">QuantizableLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A gating network that is continous for training and discrete for eval.</span>

<span class="sd">  Based on the gating network from https://arxiv.org/abs/2002.07106.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="CCTGatingNetwork.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CCTGatingNetwork.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input to the network.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;hidden_layer_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the hidden layer outputs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_outputs&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Number of scalar gate outputs.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;noise_std&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Standard deviation for gating noise.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;noise_warmup_steps&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Steps to full noise.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">schedule</span><span class="o">.</span><span class="n">PolynomialSchedule</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">noise_warmup_steps</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">noise_std</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;noise_std&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;gating_layer&#39;</span>
    <span class="n">params</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">params</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;RELU&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">]</span>
    <span class="n">params</span><span class="o">.</span><span class="n">hidden_layer_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dim</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;gatingfflayer&#39;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="CCTGatingNetwork.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CCTGatingNetwork.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">p_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gatingfflayer</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">gatingfflayer</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="n">ones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p_c</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
      <span class="n">zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p_c</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
      <span class="n">p_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">p_c</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">FPropDtype</span><span class="p">(</span><span class="n">p</span><span class="p">))),</span>
          <span class="n">ones</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">noise_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_std</span><span class="o">.</span><span class="n">Value</span><span class="p">()</span>
      <span class="n">noise</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">DeterministicVN</span><span class="p">(</span>
          <span class="n">p</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p_c</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="n">noise_std</span><span class="p">)</span>
      <span class="n">p_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">p_c</span> <span class="o">+</span> <span class="n">noise</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p_c</span></div>

<div class="viewcode-block" id="CCTGatingNetwork.FPropMeta"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CCTGatingNetwork.FPropMeta">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">FPropMeta</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">py_utils</span><span class="o">.</span><span class="n">CheckShapes</span><span class="p">((</span><span class="n">inputs</span><span class="p">,))</span>
    <span class="k">assert</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">input_dim</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">in_dim</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">other_dims</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">num_elements</span><span class="p">()</span> <span class="o">/</span> <span class="n">in_dim</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">other_dims</span> <span class="o">*</span> <span class="n">in_dim</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dim</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">other_dims</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">hidden_layer_dim</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">(</span><span class="n">flops</span><span class="o">=</span><span class="n">flops</span><span class="p">,</span> <span class="n">out_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">out_shape</span><span class="p">,))</span></div></div>


<div class="viewcode-block" id="CondScaleShiftFFNLayer"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CondScaleShiftFFNLayer">[docs]</a><span class="k">class</span> <span class="nc">CondScaleShiftFFNLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Feature Modulation layer.</span>

<span class="sd">  https://distill.pub/2018/feature-wise-transformations/</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="CondScaleShiftFFNLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CondScaleShiftFFNLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;ffn&#39;</span><span class="p">,</span> <span class="n">FeedForwardNet</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span> <span class="s1">&#39;Projection layer params&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;scale_fn&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span>
             <span class="s1">&#39;The activation function to use for scale output&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;shift_fn&#39;</span><span class="p">,</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span>
             <span class="s1">&#39;The activation function to use for shift output&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

    <span class="n">output_dim</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># 1st split for shift, 2nd split for scale</span>
    <span class="n">params_ffn</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ffn</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_ffn&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="n">params_fcout</span> <span class="o">=</span> <span class="n">FCLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">params_ffn</span><span class="o">.</span><span class="n">hidden_layer_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;NONE&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_fcout&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;ffn&#39;</span><span class="p">,</span> <span class="n">params_ffn</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;fcout&#39;</span><span class="p">,</span> <span class="n">params_fcout</span><span class="p">)</span>

<div class="viewcode-block" id="CondScaleShiftFFNLayer.FProp"><a class="viewcode-back" href="../../../lingvo.core.layers.html#lingvo.core.layers.CondScaleShiftFFNLayer.FProp">[docs]</a>  <span class="k">def</span> <span class="nf">FProp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate scale shift and modify input.</span>

<span class="sd">    Args:</span>
<span class="sd">      theta: params.</span>
<span class="sd">      inputs: The input tensor. Shaped [..., input_dim].</span>
<span class="sd">      paddings: The input padding tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output after calculating shift and scale (2 tensors).</span>
<span class="sd">      Shaped [..., output_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">ffn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">fcout_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fcout</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">fcout</span><span class="p">,</span> <span class="n">ffn_output</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">scale_output</span><span class="p">,</span> <span class="n">shift_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">fcout_output</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">OpWrapper</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Wrapper for retrieve tf operations.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">activations</span><span class="o">.</span><span class="n">IsSupported</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">GetFn</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;EXP&#39;</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;NONE&#39;</span><span class="p">:</span>
          <span class="n">op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">op</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="n">scale_output</span> <span class="o">=</span> <span class="n">OpWrapper</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">,</span> <span class="n">scale_output</span><span class="p">)</span>
    <span class="n">shift_output</span> <span class="o">=</span> <span class="n">OpWrapper</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shift_fn</span><span class="p">,</span> <span class="n">shift_output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale_output</span><span class="p">,</span> <span class="n">shift_output</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>