

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.core.py_utils &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.py_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.py_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Common utilities.&quot;&quot;&quot;</span>

<span class="c1"># ==============================================================================</span>
<span class="c1"># Note: Avoid adding dependencies to py_utils beyond standard python packages</span>
<span class="c1">#       and tensorflow.</span>
<span class="c1"># ==============================================================================</span>

<span class="kn">import</span> <span class="nn">collections</span> <span class="k">as</span> <span class="nn">py_collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pkgutil</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">cluster_factory</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">gshard_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">nested_map</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils_flags</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">retry</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">thread_local_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">tshape</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="c1"># pylint: disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="kn">import</span> <span class="n">rewriter_config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">func_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">init_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">stateless_random_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tf2</span> <span class="kn">import</span> <span class="n">enabled</span> <span class="k">as</span> <span class="n">tf2_enabled</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="kn">import</span> <span class="n">topology</span> <span class="k">as</span> <span class="n">tf_topology</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="kn">import</span> <span class="n">tpu_function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">deprecation</span>
<span class="c1"># pylint: enable=g-direct-tensorflow-import</span>


<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>


<span class="c1"># pylint: disable=protected-access</span>
<span class="n">_FromGlobal</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">_FromGlobal</span>
<span class="c1"># pylint: enable=protected-access</span>
<span class="n">use_xla</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">use_xla</span>
<span class="n">use_tpu</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">use_tpu</span>
<span class="n">testonly_skip_norm_layers</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">testonly_skip_norm_layers</span>
<span class="n">tpu_compat</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">tpu_compat</span>
<span class="n">use_stateless_vars_init</span> <span class="o">=</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">use_stateless_vars_init</span>

<span class="n">ENQUEUE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_enqueue_ops&#39;</span>

<span class="c1"># pylint: disable=protected-access</span>
<span class="n">deprecation</span><span class="o">.</span><span class="n">_PRINT_DEPRECATION_WARNINGS</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># pylint: enable=protected-access</span>

<span class="n">ThreadLocalStack</span> <span class="o">=</span> <span class="n">thread_local_utils</span><span class="o">.</span><span class="n">ThreadLocalStack</span>
<span class="n">ThreadLocalDict</span> <span class="o">=</span> <span class="n">thread_local_utils</span><span class="o">.</span><span class="n">ThreadLocalDict</span>
<span class="n">NestedMap</span> <span class="o">=</span> <span class="n">nested_map</span><span class="o">.</span><span class="n">NestedMap</span>


<div class="viewcode-block" id="Assert"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Assert">[docs]</a><span class="k">def</span> <span class="nf">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_greater_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_greater_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_greater"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_greater">[docs]</a><span class="k">def</span> <span class="nf">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_less_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_less_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_less"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_less">[docs]</a><span class="k">def</span> <span class="nf">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_between"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_between">[docs]</a><span class="k">def</span> <span class="nf">assert_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">l</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">([</span>
      <span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
      <span class="n">assert_less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="p">])</span></div>


<div class="viewcode-block" id="assert_shape_match"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_shape_match">[docs]</a><span class="k">def</span> <span class="nf">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;msg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
        <span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_same_dim0"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_same_dim0">[docs]</a><span class="k">def</span> <span class="nf">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_even_divide"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_even_divide">[docs]</a><span class="k">def</span> <span class="nf">assert_even_divide</span><span class="p">(</span><span class="n">denorm</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Asserts that denorm is evenly divided by num.&quot;&quot;&quot;</span>
  <span class="n">denorm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">denorm</span><span class="p">)</span>
  <span class="n">num</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">denorm</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;denorminator.dtype is not tf.int32 or tf.int64.&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">num</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;numerator.dtype is not tf.int32 or tf.int64.&#39;</span><span class="p">)</span>

  <span class="n">num</span> <span class="o">=</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">denorm</span><span class="p">))</span>

  <span class="n">quo</span> <span class="o">=</span> <span class="n">denorm</span> <span class="o">//</span> <span class="n">num</span>
  <span class="k">return</span> <span class="n">assert_equal</span><span class="p">(</span><span class="n">quo</span> <span class="o">*</span> <span class="n">num</span><span class="p">,</span> <span class="n">denorm</span><span class="p">)</span></div>


<div class="viewcode-block" id="AssertIdShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AssertIdShape">[docs]</a><span class="k">def</span> <span class="nf">AssertIdShape</span><span class="p">(</span><span class="n">expected_ids_shape_pattern</span><span class="p">,</span> <span class="n">ids_shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Asserts shape expected_ids_shape_pattern matches all other input shapes.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">AssertFn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">assert_shape_match</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">ids_shape</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">expected_ids_shape_pattern</span><span class="p">)</span>
    <span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
        <span class="n">assert_shape_match</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">ids_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_shape</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">args</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ids_shape</span><span class="p">)</span>

  <span class="n">inputs</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span>
      <span class="n">expected_ids_shape_pattern</span><span class="o">=</span><span class="n">expected_ids_shape_pattern</span><span class="p">,</span>
      <span class="n">ids_shape</span><span class="o">=</span><span class="n">ids_shape</span><span class="p">,</span>
      <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">CallDefun</span><span class="p">(</span><span class="n">AssertFn</span><span class="p">,</span> <span class="n">Transform</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span></div>


<div class="viewcode-block" id="_CheckNumerics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CheckNumerics">[docs]</a><span class="k">def</span> <span class="nf">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
    <span class="n">x_name</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;[eager]&#39;</span>
    <span class="k">if</span> <span class="s1">&#39;name&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;:\d+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x_name</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_CheckNumerics&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">check_numerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span> <span class="k">if</span> <span class="n">message</span> <span class="k">else</span> <span class="n">x_name</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                                       <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="CheckNumerics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CheckNumerics">[docs]</a><span class="k">def</span> <span class="nf">CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Check numerics for tensors in inp.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_check_numerics</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">inp</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="with_dependencies"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.with_dependencies">[docs]</a><span class="k">def</span> <span class="nf">with_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="_PrintOptions"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._PrintOptions">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_PrintOptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">get_printoptions</span><span class="p">()</span>
  <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">**</span><span class="n">original</span><span class="p">)</span></div>


<div class="viewcode-block" id="_Print"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Print">[docs]</a><span class="k">def</span> <span class="nf">_Print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">_PrintOptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> = </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">x</span><span class="p">))</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Log">[docs]</a><span class="k">def</span> <span class="nf">Log</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Prints out values of tensors.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Log(z, &#39;debug compute()&#39;, x=x, y=y)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Log happens after this tensor&#39;s computed.</span>
<span class="sd">    prefix: Every tensor is logged with this prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Ensures tensors are printed in order.</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Print</span><span class="p">,</span> <span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39; : &#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="Debug"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Debug">[docs]</a><span class="k">def</span> <span class="nf">Debug</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">more</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper around tf.Print() and tf.logging.info() to simplify debug printing.</span>

<span class="sd">  x = py_utils.Debug(x)</span>

<span class="sd">  When the graph is built a regular log info line will be printed:</span>
<span class="sd">  -DBG- py_utils_test.py:429 x=Tensor(...</span>

<span class="sd">  Then when the tensor node is evaluated it will print lines like:</span>
<span class="sd">  -DBG- py_utils_test.py:429 x Const:0[x.shape=][2 2][x=][[1 2][3 4]]</span>

<span class="sd">  WARNING: The code that parses local variable names can fail. E.g. don&#39;t write</span>
<span class="sd">  two Debug() calls on one line or a Debug() call that spans more than one line.</span>

<span class="sd">  Args:</span>
<span class="sd">     tensor: A tensor to print.</span>
<span class="sd">     message: A message to print.</span>
<span class="sd">     enabled: To enable the debugging.</span>
<span class="sd">     summarize: Integer with number of tensor values to print.</span>
<span class="sd">     more: An optional list of additional tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">enabled</span> <span class="ow">or</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;disable_py_utils_debug&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tensor</span>

  <span class="k">if</span> <span class="n">more</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">more</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">stack</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">stack</span><span class="p">()[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">caller</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getframeinfo</span><span class="p">(</span><span class="n">stack</span><span class="p">)</span>

  <span class="n">caller_var</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
  <span class="n">caller_more_vars</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">:</span>
    <span class="c1"># Rough and likely to fail. But better than nothing.</span>
    <span class="n">caller_var</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Debug\((.*?)(\)|,).*$&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">search</span><span class="p">(</span>
        <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">more</span><span class="p">:</span>
      <span class="n">more_vars</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;more=\[(.*?)\].*$&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">search</span><span class="p">(</span>
          <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">caller_more_vars</span> <span class="o">=</span> <span class="n">more_vars</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

  <span class="n">the_class</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
  <span class="k">if</span> <span class="s1">&#39;self&#39;</span> <span class="ow">in</span> <span class="n">stack</span><span class="o">.</span><span class="n">f_locals</span><span class="p">:</span>
    <span class="n">the_class</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">f_locals</span><span class="p">[</span><span class="s1">&#39;self&#39;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
  <span class="n">header</span> <span class="o">=</span> <span class="s1">&#39;-DBG- </span><span class="si">{}</span><span class="s1">:</span><span class="si">{}</span><span class="s1">:</span><span class="si">{}</span><span class="s1">:</span><span class="si">{}</span><span class="s1"> </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
      <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">caller</span><span class="o">.</span><span class="n">filename</span><span class="p">),</span> <span class="n">the_class</span><span class="p">,</span> <span class="n">caller</span><span class="o">.</span><span class="n">function</span><span class="p">,</span>
      <span class="n">caller</span><span class="o">.</span><span class="n">lineno</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

  <span class="n">info</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1">=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">header</span><span class="p">,</span> <span class="n">caller_var</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">caller_more_vars</span><span class="p">,</span> <span class="n">more</span><span class="p">):</span>
    <span class="n">info</span> <span class="o">+=</span> <span class="s1">&#39; </span><span class="si">{}</span><span class="s1">=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">val</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tensors</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.shape=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">caller_var</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">caller_more_vars</span><span class="p">,</span> <span class="n">more</span><span class="p">):</span>
      <span class="n">tensors</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.shape=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">())),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">val</span><span class="p">)]</span>

    <span class="n">tensors</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">caller_var</span><span class="p">)),</span> <span class="n">tensor</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">caller_more_vars</span><span class="p">,</span> <span class="n">more</span><span class="p">):</span>
      <span class="n">tensors</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">())),</span> <span class="n">val</span><span class="p">]</span>

    <span class="n">name</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">name</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;[eager]&#39;</span>
    <span class="n">info</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1"> </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">header</span><span class="p">,</span> <span class="n">caller_var</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">Print</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">),</span>
        <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;:.*$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="_Save"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Save">[docs]</a><span class="k">def</span> <span class="nf">_Save</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.</span><span class="si">%08d</span><span class="s1">.</span><span class="si">%s</span><span class="s1">.npy&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">ensure_text</span><span class="p">(</span><span class="n">prefix</span><span class="p">),</span> <span class="n">steps</span><span class="p">,</span>
                                 <span class="n">six</span><span class="o">.</span><span class="n">ensure_text</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span></div>


<div class="viewcode-block" id="Save"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Save">[docs]</a><span class="k">def</span> <span class="nf">Save</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Saves values of tensors into files.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Save(z, &#39;/path/tmp&#39;, x=x, y=y, z=z)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Saving happens after this tensor is computed.</span>
<span class="sd">    filename_prefix: Every tensor is saved with this filename prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="n">steps</span> <span class="o">=</span> <span class="n">GetGlobalStep</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Save</span><span class="p">,</span> <span class="p">[</span><span class="n">steps</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="HasRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasRank">[docs]</a><span class="k">def</span> <span class="nf">HasRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected rank.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="n">expected_rank</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Ranks did not match, got </span><span class="si">%d</span><span class="s1">, &#39;</span>
        <span class="s1">&#39;expected </span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">expected_rank</span><span class="p">)],</span>
                             <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="HasAtLeastRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasAtLeastRank">[docs]</a><span class="k">def</span> <span class="nf">HasAtLeastRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has rank &gt;= expected_rank.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">&gt;=</span> <span class="n">expected_rank</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Rank of tensor </span><span class="si">%d</span><span class="s1"> did not exceed the expected value </span><span class="si">%d</span><span class="s1">.&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">if</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">expected_rank</span><span class="p">)],</span>
        <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="GetRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetRank">[docs]</a><span class="k">def</span> <span class="nf">GetRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns tensor&#39;s rank as an int if it&#39;s available, otherwise a Tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: The input tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Either an int or a Tensor for the rank of the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>  <span class="c1"># int</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>  <span class="c1"># Tensor</span></div>


<div class="viewcode-block" id="GetShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetShape">[docs]</a><span class="k">def</span> <span class="nf">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns tensor&#39;s shape as a list which can be unpacked, unlike tf.shape.</span>

<span class="sd">  Tries to return static shape if it&#39;s available. Note that this means</span>
<span class="sd">  some of the outputs will be ints while the rest will be Tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: The input tensor.</span>
<span class="sd">    ndims: If not None, returns the shapes for the first `ndims` dimensions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">dynamic_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

  <span class="c1"># Early exit for unranked tensor.</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">dynamic_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)]</span>

  <span class="c1"># Ranked tensor.</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ndims</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)</span>

  <span class="c1"># Return mixture of static and dynamic dims.</span>
  <span class="n">static_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">if</span> <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="k">return</span> <span class="n">shapes</span></div>


<div class="viewcode-block" id="HasShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasShape">[docs]</a><span class="k">def</span> <span class="nf">HasShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A Tensor.</span>
<span class="sd">    expected_shape: A Python list or a 1D tensor. Elements of expected_shape can</span>
<span class="sd">      be -1 which indicate that any size is valid for that dimension.</span>
<span class="sd">    ndims: If not None, check only the first `ndims` dimensions of `tensor`.</span>
<span class="sd">      Must be equal to the length of `expected_shape` if not None.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The input `tensor` with control dependencies that will raise a runtime</span>
<span class="sd">    error if dynamic shape checks fail.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: A value error if the assertion fails at static shape checks.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">py_utils_flags</span><span class="o">.</span><span class="n">enable_asserts</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tensor</span>

  <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                               <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>

  <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="p">[:</span><span class="n">ndims</span><span class="p">]</span>

  <span class="c1"># TODO(jngiam): Attempt to switch back to tf.Assert after it has better</span>
  <span class="c1"># support on GPUs.</span>
  <span class="n">assert_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

  <span class="c1"># If expected_shape is a Tensor, then we are unable to perform static checks.</span>
  <span class="c1"># In this case, we can do a dynamic check and return.</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">assert_op</span><span class="p">],</span> <span class="n">tensor</span><span class="p">)</span>

  <span class="c1"># Infer ranks from the inputs.</span>
  <span class="n">expected_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_shape</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">tensor_rank</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">tensor_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">)</span>

  <span class="c1"># If ndims is None, then either one of the ranks should not be None, or they</span>
  <span class="c1"># should both match. If both ranks are None, then they are both tensors and</span>
  <span class="c1"># should be caught by the earlier short-circuit.</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tensor_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">expected_rank</span> <span class="o">!=</span> <span class="n">tensor_rank</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Tensor does not match rank of expected shape.</span><span class="se">\n</span><span class="s1">&#39;</span>
                       <span class="s1">&#39;Tensor shape: </span><span class="si">{}</span><span class="s1"> Expected shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="n">tensor_shape</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">))</span>
    <span class="c1"># Both tensors can be assumed to be of same rank.</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="n">expected_rank</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tensor_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">tensor_rank</span> <span class="o">&lt;</span> <span class="n">ndims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Tensor has fewer dimensions than ndims.</span><span class="se">\n</span><span class="s1">&#39;</span>
                       <span class="s1">&#39;Tensor shape: </span><span class="si">{}</span><span class="s1"> ndims: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">ndims</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">expected_rank</span> <span class="o">!=</span> <span class="n">ndims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Expected shape must have number of dimensions equal to ndims.</span><span class="se">\n</span><span class="s1">&#39;</span>
          <span class="s1">&#39;Expected shape: </span><span class="si">{}</span><span class="s1"> ndims: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">expected_shape</span><span class="p">,</span> <span class="n">ndims</span><span class="p">))</span>

  <span class="c1"># Ensure that both tensor_shape and expected_shape are both lists.</span>
  <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="p">[:</span><span class="n">ndims</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">ndims</span><span class="p">)</span>

  <span class="c1"># Map tf.Dimension values to their held values.</span>
  <span class="n">tensor_shape</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tensor_shape</span>
  <span class="p">]</span>
  <span class="n">expected_shape</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">expected_shape</span>
  <span class="p">]</span>

  <span class="n">all_static_checks</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">expected_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">)):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">all_static_checks</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="n">expected_dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="k">continue</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="n">all_static_checks</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">expected_dim</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Tensor does not match expected shape on dimension </span><span class="si">{}</span><span class="s1">.</span><span class="se">\n</span><span class="s1">&#39;</span>
                       <span class="s1">&#39;Tensor shape: </span><span class="si">{}</span><span class="s1"> Expected shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="n">idx</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">all_static_checks</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">assert_op</span><span class="p">],</span> <span class="n">tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="HasSameShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasSameShape">[docs]</a><span class="k">def</span> <span class="nf">HasSameShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ref</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">ref</span><span class="p">))</span></div>


<div class="viewcode-block" id="GetSize"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetSize">[docs]</a><span class="k">def</span> <span class="nf">GetSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span>
      <span class="nb">any</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">])):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="CausalSelfAttenPadding"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CausalSelfAttenPadding">[docs]</a><span class="k">def</span> <span class="nf">CausalSelfAttenPadding</span><span class="p">(</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps tf.linalg.band_part() for tflite compatibility.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">tflite_compatible</span><span class="p">:</span>
    <span class="c1"># [N, 1]</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># [1, N]</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">row_cols</span> <span class="o">=</span> <span class="n">rows</span> <span class="o">-</span> <span class="n">cols</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">row_cols</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">],</span> <span class="n">dtype</span><span class="p">),</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="outside_all_rewrites"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.outside_all_rewrites">[docs]</a><span class="k">def</span> <span class="nf">outside_all_rewrites</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span></div>


<span class="c1"># TODO(jamesqin): remove once b/147439702 is fixed.</span>
<span class="n">_OUTSIDE_COMPILATION</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>


<div class="viewcode-block" id="RunOnTpuHost"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RunOnTpuHost">[docs]</a><span class="k">def</span> <span class="nf">RunOnTpuHost</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Runs the given function call on TPU host.</span>

<span class="sd">  Invokes func(\*args, \*\*kwargs) directly if not running on tpu.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to invoke.</span>
<span class="sd">    *args: args of func</span>
<span class="sd">    **kwargs: kwargs of func</span>

<span class="sd">  Returns:</span>
<span class="sd">    The function return value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_OUTSIDE_COMPILATION</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
    <span class="n">_OUTSIDE_COMPILATION</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">outside_compilation</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">_OUTSIDE_COMPILATION</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="tpu_host"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.tpu_host">[docs]</a><span class="k">def</span> <span class="nf">tpu_host</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Decorates a python function to only run on TPU hosts.</span>

<span class="sd">  This function has no effect when running on CPU/GPU.</span>

<span class="sd">  Example::</span>

<span class="sd">    @py_utils.tpu_host()</span>
<span class="sd">    def ComputeWER(self):</span>
<span class="sd">      # Call a custom op computing WER.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to invoke</span>

<span class="sd">  Returns:</span>
<span class="sd">    A TPU-host only function</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">RunOnTpuHost</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Wrapped</span></div>

<span class="c1"># Maps a TPU job name (&#39;/job:xxx&#39;) to the job&#39;s DeviceAssignment object.</span>
<span class="c1"># When there is only a single TPU job, the key could be None.</span>
<span class="n">_tpu_device_assignment_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>


<div class="viewcode-block" id="SetTpuDeviceAssignment"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SetTpuDeviceAssignment">[docs]</a><span class="k">def</span> <span class="nf">SetTpuDeviceAssignment</span><span class="p">(</span><span class="n">tpu_device_assignment</span><span class="p">,</span> <span class="n">job</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">job</span> <span class="ow">in</span> <span class="n">_tpu_device_assignment_dict</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;tpu_device_assignment was already set, &#39;</span>
                       <span class="s1">&#39;overwriting with new assignment.&#39;</span><span class="p">)</span>
  <span class="n">_tpu_device_assignment_dict</span><span class="p">[</span><span class="n">job</span><span class="p">]</span> <span class="o">=</span> <span class="n">tpu_device_assignment</span></div>


<span class="c1"># This function should called in unittest only.</span>
<div class="viewcode-block" id="ClearTpuDevice"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ClearTpuDevice">[docs]</a><span class="k">def</span> <span class="nf">ClearTpuDevice</span><span class="p">():</span>
  <span class="k">global</span> <span class="n">_tpu_device_assignment_dict</span>
  <span class="n">_tpu_device_assignment_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetTpuDeviceAssignment"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetTpuDeviceAssignment">[docs]</a><span class="k">def</span> <span class="nf">GetTpuDeviceAssignment</span><span class="p">(</span><span class="n">job</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_tpu_device_assignment_dict</span><span class="p">[</span><span class="n">job</span><span class="p">]</span></div>


<div class="viewcode-block" id="SessionConfig"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SessionConfig">[docs]</a><span class="k">def</span> <span class="nf">SessionConfig</span><span class="p">(</span><span class="n">soft_placement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">cluster_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">disable_meta_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a session config proto.</span>

<span class="sd">  Args:</span>
<span class="sd">    soft_placement: Turns allow_soft_placement on iff True.</span>
<span class="sd">    inline: Turns do_function_inlining on iff True.</span>
<span class="sd">    cluster_def: A tf.train.ClusterDef describing the cluster.</span>
<span class="sd">    disable_meta_optimizer: Turns off grappler/metagraph optimizer.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A TF session config proto.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">session_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config_pb2</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
      <span class="n">allow_soft_placement</span><span class="o">=</span><span class="n">soft_placement</span><span class="p">,</span>
      <span class="n">graph_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphOptions</span><span class="p">(</span>
          <span class="n">optimizer_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="p">(</span>
              <span class="n">opt_level</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span> <span class="n">do_function_inlining</span><span class="o">=</span><span class="n">inline</span><span class="p">)),</span>
      <span class="n">cluster_def</span><span class="o">=</span><span class="n">cluster_def</span><span class="p">)</span>
  <span class="n">session_config</span><span class="o">.</span><span class="n">share_cluster_devices_in_session</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">if</span> <span class="n">disable_meta_optimizer</span><span class="p">:</span>
    <span class="c1"># Useful if start-up time is critical.</span>
    <span class="n">session_config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="o">.</span><span class="n">disable_meta_optimizer</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="c1"># Disable layout optimizer which increases GPU memory usage.</span>
  <span class="n">session_config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="o">.</span><span class="n">layout_optimizer</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">OFF</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">session_config</span></div>


<div class="viewcode-block" id="AssertIsCompatible"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AssertIsCompatible">[docs]</a><span class="k">def</span> <span class="nf">AssertIsCompatible</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">IsCompatible</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> vs </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span></div>


<div class="viewcode-block" id="SetShapes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SetShapes">[docs]</a><span class="k">def</span> <span class="nf">SetShapes</span><span class="p">(</span><span class="n">dst_nmap</span><span class="p">,</span> <span class="n">src_nmap</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set shapes in dst_nmap using those in src_nmap.&quot;&quot;&quot;</span>
  <span class="n">AssertIsCompatible</span><span class="p">(</span><span class="n">src_nmap</span><span class="p">,</span> <span class="n">dst_nmap</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">dst</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">src_nmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">dst_nmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()):</span>
    <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dtypes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Dtypes">[docs]</a><span class="k">def</span> <span class="nf">Dtypes</span><span class="p">(</span><span class="n">nmap_list</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all tensors&#39; data types in a list.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">nmap_list</span><span class="p">)]</span></div>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Flatten">[docs]</a><span class="k">def</span> <span class="nf">Flatten</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Flattens &#39;x&#39; by extracting tensors from nested structures to a list.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="Pack"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Pack">[docs]</a><span class="k">def</span> <span class="nf">Pack</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Packs &#39;values&#39; according to &#39;tmpl&#39;.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span></div>


<div class="viewcode-block" id="Transform"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Transform">[docs]</a><span class="k">def</span> <span class="nf">Transform</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">v</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replaces every nested value x in &#39;v&#39; with fn(x) and returns the result.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConvertNoneGradientToZeros"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConvertNoneGradientToZeros">[docs]</a><span class="k">def</span> <span class="nf">ConvertNoneGradientToZeros</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">dxs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sanitize dxs so that None becomes zeros appropriately.</span>

<span class="sd">  Args:</span>
<span class="sd">    xs: A list of tensors.</span>
<span class="sd">    dxs: A list of tensors. dxs[i] corresponds to xs[i]&#39;s gradient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` same as dxs with None replaced by a zero tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">dx</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">dx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dx</span>
  <span class="k">return</span> <span class="n">Transform</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">dxs</span><span class="p">)</span></div>


<div class="viewcode-block" id="IsCompatible"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.IsCompatible">[docs]</a><span class="k">def</span> <span class="nf">IsCompatible</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns true if lhs and rhs are compatible.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="_Unique"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Unique">[docs]</a><span class="k">class</span> <span class="nc">_Unique</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;A helper to uniqify variables in a NestedMap.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
      <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="ToUniqueList"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToUniqueList">[docs]</a><span class="k">def</span> <span class="nf">ToUniqueList</span><span class="p">(</span><span class="n">nmap</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the flattened `nmap` with duplicates removed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span></div>


<div class="viewcode-block" id="ReadOnlyAttrDictView"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadOnlyAttrDictView">[docs]</a><span class="k">def</span> <span class="nf">ReadOnlyAttrDictView</span><span class="p">(</span><span class="n">backing</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a dict to provide a read-only view of its contents.</span>

<span class="sd">  Dict keys can also be accessed by attribute.</span>

<span class="sd">  Args:</span>
<span class="sd">    backing: Dict-like object to wrap.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Read-only Mapping that can be accessed by index ([&#39;foo&#39;]) or attr (d.foo).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">Wrapper</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Wrapper object.&quot;&quot;&quot;</span>

    <span class="c1"># Disable pytype attribute checking.</span>
    <span class="n">_HAS_DYNAMIC_ATTRIBUTES</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__hasattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">backing</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Wrapper</span><span class="p">()</span></div>


<div class="viewcode-block" id="ToStaticShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToStaticShape">[docs]</a><span class="k">def</span> <span class="nf">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts &#39;shape&#39; to a static shape.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span>
    <span class="p">]</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="n">static_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">static_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">static_shape</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">shape</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">shape</span></div>


<div class="viewcode-block" id="Zeros"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Zeros">[docs]</a><span class="k">def</span> <span class="nf">Zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="UniformSampler"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UniformSampler">[docs]</a><span class="k">class</span> <span class="nc">UniformSampler</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;A reservoir sampler.</span>

<span class="sd">  This class implements reservoir sampling: Given a limit of `num_samples` total</span>
<span class="sd">  samples, this class maintains a uniform probability (1 / `num_samples`) of</span>
<span class="sd">  keeping any item dynamically added to the sampler.</span>

<span class="sd">  See https://en.wikipedia.org/wiki/Reservoir_sampling for details.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">=</span> <span class="n">num_samples</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="UniformSampler.Add"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UniformSampler.Add">[docs]</a>  <span class="k">def</span> <span class="nf">Add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add item to sampler.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">samples</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fetch the current samples from the sampler.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span></div>


<div class="viewcode-block" id="RNNCellStateInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit">[docs]</a><span class="k">class</span> <span class="nc">RNNCellStateInit</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;State initialization functions for RNN cell init state.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="RNNCellStateInit._Params"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit._Params">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span>
             <span class="s1">&#39;Initialization method. Should be one of zeros, random_normal.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="RNNCellStateInit.Zeros"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit.Zeros">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Zeros</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;tf.zeros().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></div>

<div class="viewcode-block" id="RNNCellStateInit.RandomNormal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit.RandomNormal">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">RandomNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;tf.random.normal().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DefaultRNNCellStateInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefaultRNNCellStateInit">[docs]</a><span class="k">def</span> <span class="nf">DefaultRNNCellStateInit</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()</span></div>


<div class="viewcode-block" id="InitRNNCellState"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.InitRNNCellState">[docs]</a><span class="k">def</span> <span class="nf">InitRNNCellState</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Initial state definitions for RNN cell implementations.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: A array of ints/symbols for specifying the shape of the state.</span>
<span class="sd">    init: Hyperparameters as returned by one of the static implemetaitons in</span>
<span class="sd">      RNNCellStateInit.</span>
<span class="sd">    dtype: The dype of the states. Defaults to tf.float32.</span>
<span class="sd">    name: A name for the operation. If --stateless_vars_init is set, this name</span>
<span class="sd">      is used to generate a seed on a per-variable basis. Otherwise, this name</span>
<span class="sd">      is optional.</span>
<span class="sd">    is_eval: Bool, set to True if we need special behavior in eval mode.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor of the specified shape, and sampled from the distribution as</span>
<span class="sd">    defined by the init parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">DefaultRNNCellStateInit</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>

  <span class="n">method</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="k">if</span> <span class="p">((</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;zeros&#39;</span><span class="p">])</span> <span class="ow">or</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_eval</span><span class="p">)):</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">use_stateless_vars_init</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;InitRNNCellState() requires a `name` argument when &#39;</span>
                         <span class="s1">&#39;--stateless_vars_init is enabled.&#39;</span><span class="p">)</span>
      <span class="n">seed</span> <span class="o">=</span> <span class="n">_GenerateStatelessRngSeed</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">init</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
      <span class="n">init_state</span> <span class="o">=</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_normal</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Initialization method (</span><span class="si">%s</span><span class="s1">) not supported.&#39;</span> <span class="o">%</span> <span class="n">method</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">init_state</span></div>


<div class="viewcode-block" id="WeightInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit">[docs]</a><span class="k">class</span> <span class="nc">WeightInit</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Static class providing weight initialization config params.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="WeightInit._Params"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit._Params">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parameters of this class.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s1">&#39;Initialization scale.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="WeightInit.Gaussian"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Gaussian">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Gaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Uniform"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Uniform">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Uniform</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.uniform(-1.0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformPositive"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformPositive">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformPositive</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.uniform(0., 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Category"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Category">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Category</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;tf.floor(scale * tf.random.uniform(0., 1.0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Xavier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Xavier">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.XavierWithFixupParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.XavierWithFixupParams">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">XavierWithFixupParams</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">depth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">layers_per_residual_block</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization with Fixup.&quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">layers_per_residual_block</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GeoMeanXavier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GeoMeanXavier">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GeoMeanXavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A variant of Xavier (x = sqrt(3. / sqrt(in * out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Constant"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Constant">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Constant</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussian"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussian">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.truncated_normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtFanIn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtFanIn">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtFanOut">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtFanAvg"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtFanAvg">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanAvg</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;tf.random.normal(0, sqrt(2.0 / (in + out))).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanavg&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.uniform(-1 / sqrt(dim0), 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformUnitScaling"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformUnitScaling">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformUnitScaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * sqrt(3) / sqrt(dim0) * tf.uniform(-1, 1).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformUnitScalingFanAvg"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformUnitScalingFanAvg">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformUnitScalingFanAvg</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Same as tf.variance_scaling_initializer() ...</span>

<span class="sd">    Samples are drawn from a uniform distribution within [-limit, limit], with</span>
<span class="sd">    limit = sqrt(3 * scale / n)</span>

<span class="sd">    where</span>
<span class="sd">    n = max(1., (fan_in + fan_out) / 2).</span>
<span class="sd">    See tf.keras.initializers.VarianceScaling for details.</span>

<span class="sd">    Args:</span>
<span class="sd">      scale: A Python float.</span>
<span class="sd">      seed: A Python int or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A WeightInit param.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_unit_scaling_fan_avg&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.truncated_normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtFanIn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtFanIn">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.truncated_normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtFanOut">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random.truncated_normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.KaimingUniformFanInRelu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.KaimingUniformFanInRelu">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.KaimingUniformFanInLeakyRelu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.KaimingUniformFanInLeakyRelu">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInLeakyRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div></div>


<span class="n">_DEFAULT_XAVIER_INIT</span> <span class="o">=</span> <span class="mf">1.000001</span>


<div class="viewcode-block" id="DefaultParamInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefaultParamInit">[docs]</a><span class="k">def</span> <span class="nf">DefaultParamInit</span><span class="p">():</span>
  <span class="c1"># Here we use 1.000001 as a signature for user picking up the</span>
  <span class="c1"># default param initializer.</span>
  <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span></div>


<span class="c1"># TODO(rpang, jonathanasdf): explore adding _is_default to hyperparams.Param.</span>
<div class="viewcode-block" id="IsDefaultParamInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.IsDefaultParamInit">[docs]</a><span class="k">def</span> <span class="nf">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span> <span class="ow">and</span>
          <span class="nb">abs</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">scale</span> <span class="o">-</span> <span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-7</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="WeightParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightParams">[docs]</a><span class="k">def</span> <span class="nf">WeightParams</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                 <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">device_mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tensor_split_dims_mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for a weight variable given the shape/init/dtype.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
  <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">device_mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">tensor_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_split_dims_mapping</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;The weight data type.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;The weight shape.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;init&#39;</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span>
           <span class="s1">&#39;Variable collections this weight belongs to.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;device_mesh&#39;</span><span class="p">,</span> <span class="n">device_mesh</span><span class="p">,</span>
      <span class="s1">&#39;A numpy.ndarray describing the topology of a device mesh to partition&#39;</span>
      <span class="s1">&#39; this variable onto. Each element in the np.ndarray is the ID of a&#39;</span>
      <span class="s1">&#39; device in the topology. device_mesh and tensor_split_dims_mapping below&#39;</span>
      <span class="s1">&#39; together specifies how this weight tensor should be sharded across&#39;</span>
      <span class="s1">&#39; different tpu cores. If None, this variable is not sharded.&#39;</span>
      <span class="s1">&#39; Here are examples: np.array([0, 1, 2, 3, 4, 5, 6, 7]) which is a 1d&#39;</span>
      <span class="s1">&#39; mesh with 8 devices, np.array([[0, 1, 2, 3], [4, 5, 6, 7]]) which is&#39;</span>
      <span class="s1">&#39; 2d matrix of 8 devices.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;tensor_split_dims_mapping&#39;</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">,</span>
      <span class="s1">&#39;A list of integers that map each tensor axis to the device mesh axis&#39;</span>
      <span class="s1">&#39; along which it is sharded. Its length is the tensor rank, and&#39;</span>
      <span class="s1">&#39; split_dims_mapping[i] is device mesh axis for tensor dimension i. Use&#39;</span>
      <span class="s1">&#39; -1 for tensor dimensions that are not sharded. If the list is set to&#39;</span>
      <span class="s1">&#39; None and a device_mesh is specified, the sharding will be treated as&#39;</span>
      <span class="s1">&#39; replicated. Here is a concrete examples: &#39;</span>
      <span class="s1">&#39;   device_mesh=np.array([[0, 1, 2, 3] [4, 5, 6, 7]]), of shape [2, 4]&#39;</span>
      <span class="s1">&#39;   shape=[x, y, z], so this is a 3d variable.&#39;</span>
      <span class="s1">&#39;   tensor_split_dims_mapping=[-1, -1, 1], in this case, the third dim&#39;</span>
      <span class="s1">&#39;   of the variable is split along the second dim of the mesh. Each &#39;</span>
      <span class="s1">&#39;   split of the variable is of the shape [x, y, z/4].&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span></div>


<div class="viewcode-block" id="FindNeeded"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FindNeeded">[docs]</a><span class="k">def</span> <span class="nf">FindNeeded</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;List names of tensors and operations required to compute endpoints.&quot;&quot;&quot;</span>
  <span class="n">names_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">queue</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">names_seen</span><span class="p">:</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">o</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">names_seen</span></div>


<div class="viewcode-block" id="_CollectionGetter"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CollectionGetter">[docs]</a><span class="k">class</span> <span class="nc">_CollectionGetter</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Get graph local value from a defined collection.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default_factory</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_key</span> <span class="o">=</span> <span class="n">key</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span> <span class="o">=</span> <span class="n">default_factory</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collection</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collection</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="SanitizeScopeKey"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SanitizeScopeKey">[docs]</a><span class="k">def</span> <span class="nf">SanitizeScopeKey</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes invalid symbols from name_scope keys.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">):</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
  <span class="k">return</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;[&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span></div>


<span class="c1"># Maintain a session for unit tests (initialized in test_utils.py).</span>
<span class="n">_SESSION_SCOPE</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="UnitTestSessionScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UnitTestSessionScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">UnitTestSessionScope</span><span class="p">(</span><span class="n">sess</span><span class="p">):</span>
  <span class="n">_SESSION_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_SESSION_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetUnitTestSession"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetUnitTestSession">[docs]</a><span class="k">def</span> <span class="nf">GetUnitTestSession</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the current variable reuse setting.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_SESSION_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">_SESSION_SCOPE</span><span class="o">.</span><span class="n">stack</span> <span class="k">else</span> <span class="kc">None</span></div>


<span class="c1"># Global variable to control multitask variable reuse</span>
<span class="c1"># If False (default) the default tf.get_variable is used, that is:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables</span>
<span class="c1"># - Non-reusing scopes only allow getting new variables</span>
<span class="c1"># With GetOpportunisticVariableReuse() == True:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables, as usual</span>
<span class="c1"># - Non-reusing scopes reuse new variables or get new ones</span>
<span class="n">_OPPORTUNISTIC_VARIABLE_REUSE</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="OpportunisticVariableReuseScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OpportunisticVariableReuseScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">OpportunisticVariableReuseScope</span><span class="p">(</span><span class="n">enable_opportunistic_reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="n">_OPPORTUNISTIC_VARIABLE_REUSE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">enable_opportunistic_reuse</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_OPPORTUNISTIC_VARIABLE_REUSE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetOpportunisticVariableReuse"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetOpportunisticVariableReuse">[docs]</a><span class="k">def</span> <span class="nf">GetOpportunisticVariableReuse</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the current variable reuse setting.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">_OPPORTUNISTIC_VARIABLE_REUSE</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
          <span class="k">if</span> <span class="n">_OPPORTUNISTIC_VARIABLE_REUSE</span><span class="o">.</span><span class="n">stack</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span></div>


<span class="n">_VARIABLE_RENAME_RULES</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>

<span class="c1"># Global variable to track task calling scope.</span>
<span class="c1"># Currently only used for TPU Embedding purposes as a TPUEmbeddinglayer</span>
<span class="c1"># may be shared across tasks and the calling task needs to be known</span>
<span class="c1"># for tracking embedding activations for backprop.</span>
<span class="n">_TASK_CALL_SCOPE</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="TaskCallScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TaskCallScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">TaskCallScope</span><span class="p">(</span><span class="n">task_name</span><span class="p">):</span>
  <span class="n">_TASK_CALL_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">task_name</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_TASK_CALL_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetTaskCallScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetTaskCallScope">[docs]</a><span class="k">def</span> <span class="nf">GetTaskCallScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the current task call scope.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_TASK_CALL_SCOPE</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">_TASK_CALL_SCOPE</span><span class="o">.</span><span class="n">stack</span> <span class="k">else</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="VariableRenameScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariableRenameScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableRenameScope</span><span class="p">(</span><span class="n">renames</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Append the renaming rules to the stack of renames.</span>

<span class="sd">  Args:</span>
<span class="sd">    renames: pairs of (regexp, new_name_format). If the regexp matches, the</span>
<span class="sd">      new_name_format will be interpolated using the matched groups.</span>

<span class="sd">  Yields:</span>
<span class="sd">    scope in which the renaming rules are applied</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_VARIABLE_RENAME_RULES</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">renames</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_VARIABLE_RENAME_RULES</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetVariableName"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetVariableName">[docs]</a><span class="k">def</span> <span class="nf">GetVariableName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get variable name after application of all renaming rules.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: untransformed variable name with scope_name prepended</span>

<span class="sd">  Returns:</span>
<span class="sd">    name possibly modified using renaming rules</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">new_name</span> <span class="o">=</span> <span class="n">name</span>
  <span class="k">for</span> <span class="n">renames</span> <span class="ow">in</span> <span class="n">_VARIABLE_RENAME_RULES</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">renames</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">matched</span><span class="p">:</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Multiple matches for: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">new_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">new_name</span> <span class="o">!=</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;WARNING!!! Renaming variable &#39;</span><span class="si">%s</span><span class="s2">&#39; to &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_name</span></div>


<div class="viewcode-block" id="GenerateSeedFromName"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GenerateSeedFromName">[docs]</a><span class="k">def</span> <span class="nf">GenerateSeedFromName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate a random seed from a name string.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An integer seed in the range [0, 2**31 - 1).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">md5</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">()</span>
  <span class="n">md5</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">ensure_binary</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">md5</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span></div>


<div class="viewcode-block" id="MaybeGenerateSeedFromScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MaybeGenerateSeedFromScope">[docs]</a><span class="k">def</span> <span class="nf">MaybeGenerateSeedFromScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Generate a random seed from the current name of the scope.</span>

<span class="sd">  If running in eager mode, this returns 0.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An integer seed in the range [0, 2**31 - 1).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;new_step_seed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="GenerateSeedFromId"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GenerateSeedFromId">[docs]</a><span class="k">def</span> <span class="nf">GenerateSeedFromId</span><span class="p">(</span><span class="n">obj_id</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate a random seed from the id of an object.</span>

<span class="sd">  If deterministic execution (i.e. unit test), generate the seed from a fixed</span>
<span class="sd">  unique name instead.</span>

<span class="sd">  Args:</span>
<span class="sd">    obj_id: id(object).</span>

<span class="sd">  Returns:</span>
<span class="sd">    An integer seed in the range [0, 2**31 - 1).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We are in a program/test which need determistic randomization.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;new_step_seed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="n">md5</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">()</span>
  <span class="n">md5</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">obj_id</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">md5</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span></div>


<span class="c1"># To keep track of all the variables ever gets created by the CreateVariable</span>
<span class="c1"># routine below.</span>
<span class="n">_ALL_VARS_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_all_vars&#39;</span><span class="p">,)</span>

<span class="n">_get_all_vars</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_ALL_VARS_KEY</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>

<span class="n">_VARIABLE_SHAPE_PREFIXES</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="VariableShapePrefixContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariableShapePrefixContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableShapePrefixContext</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a shape prefix to variable created by CreateVariable().</span>

<span class="sd">  Args:</span>
<span class="sd">    shape_prefix: a positive integer of shape prefix.</span>

<span class="sd">  Yields:</span>
<span class="sd">    None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">shape_prefix</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">shape_prefix</span><span class="p">)</span>
  <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetVariableShapePrefixes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetVariableShapePrefixes">[docs]</a><span class="k">def</span> <span class="nf">GetVariableShapePrefixes</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the list of shape prefixes for CreateVariable().&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">stack</span></div>


<div class="viewcode-block" id="GetFanInFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetFanInFanOut">[docs]</a><span class="k">def</span> <span class="nf">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns (fan_in, fan_out) of a weight variable of the give shape.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Following _compute_fans() from TF&#39;s init_ops.py.</span>
    <span class="k">return</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">receptive_field_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
      <span class="n">receptive_field_size</span> <span class="o">*=</span> <span class="n">s</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="n">fan_out</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="k">return</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span></div>


<span class="n">_VARIABLE_CREATOR_STACK</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span><span class="o">.</span><span class="n">stack</span>


<div class="viewcode-block" id="_DefaultVariableCreator"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DefaultVariableCreator">[docs]</a><span class="k">def</span> <span class="nf">_DefaultVariableCreator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;var_name&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;var_params&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="_GetVariableCreator"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetVariableCreator">[docs]</a><span class="k">def</span> <span class="nf">_GetVariableCreator</span><span class="p">():</span>
  <span class="n">fn</span> <span class="o">=</span> <span class="n">_DefaultVariableCreator</span>
  <span class="k">for</span> <span class="n">wrapper</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">_VARIABLE_CREATOR_STACK</span><span class="p">):</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">fn</span></div>


<div class="viewcode-block" id="VariableCreatorScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariableCreatorScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableCreatorScope</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Yields a context around a variable_creator, used by `CreateVariable()`.</span>

<span class="sd">  The function must have the following signature::</span>

<span class="sd">    def variable_creator(next_creator, **kwargs)</span>

<span class="sd">  The function may delegate variable creation to the next variable creator, or</span>
<span class="sd">  return its own tf.Variable.</span>

<span class="sd">  This differs from tf.variable_creator_scope in that tf.variable_creator_scope</span>
<span class="sd">  modifies a tf.Variable() call while this modifies a tf.get_variable() call. As</span>
<span class="sd">  the code is migrated to TF2 and tf.get_variable() is deprecated, this may be</span>
<span class="sd">  upgraded to using tf.variable_creator_scope instead.</span>

<span class="sd">  This differs from tf.variable_scope(custom_getter=variable_creator) in that</span>
<span class="sd">  the kwargs passed can be manipulated.</span>

<span class="sd">  Variable creators are resolved from the outermost towards the innermost.</span>

<span class="sd">  The innermost variable creator function is tf.get_variable.</span>

<span class="sd">  The passed in kwargs must conform to what tf.get_variable accepts, with the</span>
<span class="sd">  addition of `var_name` and `var_params`.</span>

<span class="sd">  Args:</span>
<span class="sd">    variable_creator: A variable creator function.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_VARIABLE_CREATOR_STACK</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable_creator</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_VARIABLE_CREATOR_STACK</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="PlaceOnTpuCore"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PlaceOnTpuCore">[docs]</a><span class="k">def</span> <span class="nf">PlaceOnTpuCore</span><span class="p">(</span><span class="n">core_id</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a VariableCreatorScope that places variables on a given tpu core.</span>

<span class="sd">  Only applies when running with TPUs.</span>

<span class="sd">  Does not yet properly support model parallelism.</span>

<span class="sd">  Args:</span>
<span class="sd">    core_id: The tpu core id.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Creator</span><span class="p">(</span><span class="n">next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">cluster_factory</span><span class="o">.</span><span class="n">Current</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">WorkerDeviceInModelSplit</span><span class="p">(</span><span class="n">core_id</span><span class="p">)</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="n">tpu_compat</span><span class="p">()</span> <span class="ow">and</span>
        <span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">job</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;controller&#39;</span><span class="p">,</span> <span class="s1">&#39;trainer_client&#39;</span><span class="p">,</span> <span class="s1">&#39;executor_tpu&#39;</span><span class="p">)):</span>
      <span class="c1"># The job is running in a fleet that uses tpu, but does not itself have</span>
      <span class="c1"># access to the tpu, e.g. controller job. In this case, the returned</span>
      <span class="c1"># device needs to be the cpu device on the tpu host for the given core.</span>
      <span class="c1"># FIXME: the current implementation is wrong for large values of core_id.</span>
      <span class="n">device</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">ListDevices</span><span class="p">(</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">VariableCreatorScope</span><span class="p">(</span><span class="n">Creator</span><span class="p">)</span></div>


<span class="c1"># TODO(yonghui): Add support for partitioned Variables.</span>
<div class="viewcode-block" id="CreateVariable"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CreateVariable">[docs]</a><span class="k">def</span> <span class="nf">CreateVariable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                   <span class="n">params</span><span class="p">,</span>
                   <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">default_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                   <span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates tf.Variable according to param_config.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string, name of the variable.</span>
<span class="sd">    params: A WeightParams specifying the details of how this variable should be</span>
<span class="sd">      constructed and initialized.</span>
<span class="sd">    reuse: Whether or not to reuse an existing variable. It has the same</span>
<span class="sd">      semantics as the reuse arg in tf.variable_scope.</span>
<span class="sd">    trainable: Whether or not the variable is trainable.</span>
<span class="sd">    collections: Override the default variable collection (</span>
<span class="sd">      tf.GraphKeys.GLOBAL_VARIABLES).</span>
<span class="sd">    default_seed: Seed to use for initialization if not specified in params.</span>
<span class="sd">      Used for deterministic initialization in tests.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      tf.VariableSynchronization. By default the synchronization is set to AUTO</span>
<span class="sd">      and the current DistributionStrategy chooses when to synchronize.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class tf.VariableAggregation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The created variable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_stateless_vars_init</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">_CreateVariableStateless</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">reuse</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span>
                                    <span class="n">default_seed</span><span class="p">,</span> <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_CreateVariableStateful</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">reuse</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span>
                                   <span class="n">default_seed</span><span class="p">,</span> <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">)</span></div>


<div class="viewcode-block" id="_CreateVariableStateful"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CreateVariableStateful">[docs]</a><span class="k">def</span> <span class="nf">_CreateVariableStateful</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                            <span class="n">params</span><span class="p">,</span>
                            <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">default_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                            <span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates tf.Variable using TF stateful RNGs according to param_config.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string, name of the variable.</span>
<span class="sd">    params: A WeightParams specifying the details of how this variable should be</span>
<span class="sd">      constructed and initialized.</span>
<span class="sd">    reuse: Whether or not to reuse an existing variable. It has the same</span>
<span class="sd">      semantics as the reuse arg in tf.variable_scope.</span>
<span class="sd">    trainable: Whether or not the variable is trainable.</span>
<span class="sd">    collections: Override the default variable collection (</span>
<span class="sd">      tf.GraphKeys.GLOBAL_VARIABLES).</span>
<span class="sd">    default_seed: Seed to use for initialization if not specified in params.</span>
<span class="sd">      Used for deterministic initialization in tests.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      tf.VariableSynchronization. By default the synchronization is set to AUTO</span>
<span class="sd">      and the current DistributionStrategy chooses when to synchronize.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class tf.VariableAggregation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The created variable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">ToStaticShape</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">dim_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]),</span> <span class="n">shape</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">method</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span>

  <span class="k">if</span> <span class="n">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;WARNING!!! var </span><span class="si">%s</span><span class="s1"> is using the default xavier initializer.&#39;</span>
        <span class="s1">&#39; Make sure this is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We are in a program/test which need determistic randomization.</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">default_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">default_seed</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We are not given a per-variable random seed. We use hash of</span>
        <span class="c1"># variable name as a stable random seed.</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">var_name</span><span class="p">)</span>

  <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>

  <span class="c1"># TODO(b/172827074): we do not natively support var initialization for</span>
  <span class="c1"># int8 type except for constant initialization.</span>
  <span class="c1"># NOTE: For int8, we initialize by scaling float32 random values to integer.</span>
  <span class="k">if</span> <span class="n">init_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
    <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>

  <span class="n">v_init</span> <span class="o">=</span> <span class="n">_CreateVarInitStateful</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
                                  <span class="n">init_dtype</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">ComplexWrapper</span><span class="p">(</span><span class="n">init</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_Wrapper</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">dtype</span>
        <span class="c1"># A more complex alternative may be to use the init function for</span>
        <span class="c1"># magnitudes and uniform random for phases instead.</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">_Wrapper</span>

    <span class="n">v_init</span> <span class="o">=</span> <span class="n">ComplexWrapper</span><span class="p">(</span><span class="n">v_init</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">FloatToInt8Wrapper</span><span class="p">(</span><span class="n">init</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_Wrapper</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">dtype</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="mi">127</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">/</span> <span class="mi">127</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">_Wrapper</span>

    <span class="n">v_init</span> <span class="o">=</span> <span class="n">FloatToInt8Wrapper</span><span class="p">(</span><span class="n">v_init</span><span class="p">)</span>

  <span class="c1"># Variable creators.</span>
  <span class="k">def</span> <span class="nf">MaybePinVarsToCpu</span><span class="p">(</span><span class="n">next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;pin_vars_to_cpu&#39;</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/cpu:0&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">MaybeOpportunisticVariableReuse</span><span class="p">(</span><span class="n">next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>  <span class="c1"># Possibly the variable already exists</span>
      <span class="k">if</span> <span class="n">GetOpportunisticVariableReuse</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">(),</span> <span class="n">reuse</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AUTO_REUSE</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span>

  <span class="k">def</span> <span class="nf">LingvoVariableCreator</span><span class="p">(</span><span class="n">next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lingvo variable creator.&quot;&quot;&quot;</span>
    <span class="c1"># TODO(yonghui): Possibly get away from variable_scope and implement our own</span>
    <span class="c1"># variable sharing mechanism.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="n">var_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">VariableScope</span><span class="p">(</span>
          <span class="n">scope</span><span class="o">.</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_scope</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">var_ref</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">experimental_ref</span><span class="p">()</span>  <span class="c1"># For key in dict/set.</span>
    <span class="n">all_vars</span> <span class="o">=</span> <span class="n">_get_all_vars</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">var_ref</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Reusing var </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">cached</span> <span class="o">=</span> <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span>
      <span class="k">assert</span> <span class="n">cached</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;Cached config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> vs new config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                                    <span class="p">(</span><span class="n">cached</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Creating var </span><span class="si">%s</span><span class="s1"> shape=</span><span class="si">%s</span><span class="s1"> on device </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                      <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">collections</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">var</span>

  <span class="k">with</span> <span class="n">VariableCreatorScope</span><span class="p">(</span><span class="n">LingvoVariableCreator</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">VariableCreatorScope</span><span class="p">(</span><span class="n">MaybeOpportunisticVariableReuse</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">VariableCreatorScope</span><span class="p">(</span><span class="n">MaybePinVarsToCpu</span><span class="p">):</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">_GetVariableCreator</span><span class="p">()(</span>
            <span class="n">var_name</span><span class="o">=</span><span class="n">var_name</span><span class="p">,</span>
            <span class="n">var_params</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;var&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">GetVariableShapePrefixes</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">v_init</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

  <span class="c1"># Shard the variable according to the sharding spec.</span>
  <span class="n">tensor_split_dims_mapping</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tensor_split_dims_mapping</span>
  <span class="k">if</span> <span class="n">tensor_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tensor_split_dims_mapping</span> <span class="o">=</span> <span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">GetVariableShapePrefixes</span><span class="p">())</span> <span class="o">+</span>
                                 <span class="n">tensor_split_dims_mapping</span><span class="p">)</span>
  <span class="n">var</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span>
      <span class="n">var</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">,</span> <span class="n">use_sharding_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var</span></div>


<div class="viewcode-block" id="_CreateVariableStateless"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CreateVariableStateless">[docs]</a><span class="k">def</span> <span class="nf">_CreateVariableStateless</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">,</span>
                             <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">default_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                             <span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates tf.Variable using TF stateless RNGs according to `params`.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string, name of the variable.</span>
<span class="sd">    params: A WeightParams specifying the details of how this variable should be</span>
<span class="sd">      constructed and initialized.</span>
<span class="sd">    reuse: Whether or not to reuse an existing variable. It has the same</span>
<span class="sd">      semantics as the reuse arg in tf.variable_scope.</span>
<span class="sd">    trainable: Whether or not the variable is trainable.</span>
<span class="sd">    collections: Override the default variable collection (</span>
<span class="sd">      tf.GraphKeys.GLOBAL_VARIABLES).</span>
<span class="sd">    default_seed: Seed to use for initialization if not specified in params.</span>
<span class="sd">      Used for deterministic initialization in tests.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      tf.VariableSynchronization. By default the synchronization is set to AUTO</span>
<span class="sd">      and the current DistributionStrategy chooses when to synchronize.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class tf.VariableAggregation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The created variable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">ToStaticShape</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">dim_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]),</span> <span class="n">shape</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">method</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span>

  <span class="k">if</span> <span class="n">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;WARNING!!! var </span><span class="si">%s</span><span class="s1"> is using the default xavier initializer.&#39;</span>
        <span class="s1">&#39; Make sure this is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="n">user_seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">default_seed</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">_GenerateStatelessRngSeed</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">user_seed</span><span class="p">)</span>

  <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
  <span class="n">v_init</span> <span class="o">=</span> <span class="n">_CreateVarInitStateless</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
                                   <span class="n">init_dtype</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s1">&#39;Stateless variable initialization does not support tf.complex64.&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LingvoVariableCreator</span><span class="p">(</span><span class="n">next_creator</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lingvo variable creator.&quot;&quot;&quot;</span>
    <span class="c1"># TODO(yonghui): Possibly get away from variable_scope and implement our own</span>
    <span class="c1"># variable sharing mechanism.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="n">var_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">VariableScope</span><span class="p">(</span>
          <span class="n">scope</span><span class="o">.</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_scope</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">next_creator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">var_ref</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">experimental_ref</span><span class="p">()</span>  <span class="c1"># For key in dict/set.</span>
    <span class="n">all_vars</span> <span class="o">=</span> <span class="n">_get_all_vars</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">var_ref</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Reusing var </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">cached</span> <span class="o">=</span> <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span>
      <span class="k">assert</span> <span class="n">cached</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;Cached config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> vs new config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                                    <span class="p">(</span><span class="n">cached</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Creating var </span><span class="si">%s</span><span class="s1"> shape=</span><span class="si">%s</span><span class="s1"> on device </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                      <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">collections</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">var</span>

  <span class="k">with</span> <span class="n">VariableCreatorScope</span><span class="p">(</span><span class="n">LingvoVariableCreator</span><span class="p">):</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">_GetVariableCreator</span><span class="p">()(</span>
        <span class="n">var_name</span><span class="o">=</span><span class="n">var_name</span><span class="p">,</span>
        <span class="n">var_params</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;var&#39;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">GetVariableShapePrefixes</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">v_init</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

  <span class="c1"># Shard the variable according to the sharding spec.</span>
  <span class="n">tensor_split_dims_mapping</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tensor_split_dims_mapping</span>
  <span class="k">if</span> <span class="n">tensor_split_dims_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tensor_split_dims_mapping</span> <span class="o">=</span> <span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">GetVariableShapePrefixes</span><span class="p">())</span> <span class="o">+</span>
                                 <span class="n">tensor_split_dims_mapping</span><span class="p">)</span>
  <span class="n">var</span> <span class="o">=</span> <span class="n">gshard_utils</span><span class="o">.</span><span class="n">MeshSplit</span><span class="p">(</span>
      <span class="n">var</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">device_mesh</span><span class="p">,</span> <span class="n">tensor_split_dims_mapping</span><span class="p">,</span> <span class="n">use_sharding_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var</span></div>


<div class="viewcode-block" id="_RandomXavierUniformInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._RandomXavierUniformInitializer">[docs]</a><span class="k">def</span> <span class="nf">_RandomXavierUniformInitializer</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a random Xavier uniform initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">XavierUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); scale*[-x, x]).&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">shape</span><span class="se">\&#39;</span><span class="s1"> must not be </span><span class="se">\&#39;</span><span class="s1">None</span><span class="se">\&#39;</span><span class="s1"> or 0 for XavierUniform&#39;</span><span class="p">)</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">:</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">*</span> <span class="n">fan_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">XavierUniform</span></div>


<div class="viewcode-block" id="_CreateVarInitStateful"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CreateVarInitStateful">[docs]</a><span class="k">def</span> <span class="nf">_CreateVarInitStateful</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates variable initialization function for a stateful RNG.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span>
  <span class="p">]):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="c1"># This is probably not the right method to use when len(shape) &gt; 2,</span>
      <span class="c1"># e.g. dim0 will be 3 with a 3x3 conv2d kernel.</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s1">&#39;Initializing </span><span class="si">%s</span><span class="s1"> of shape </span><span class="si">%s</span><span class="s1"> with method </span><span class="si">%s</span><span class="s1">: dim0=</span><span class="si">%s</span><span class="s1">. &#39;</span>
          <span class="s1">&#39;Make sure that it is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">dim0</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim0</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">]:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanavg&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span>
      <span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanavg&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">scale</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;category&#39;</span><span class="p">:</span>
    <span class="n">uniform_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">uniform_init</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">uniform_unit_scaling_initializer</span><span class="p">(</span>
        <span class="n">factor</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling_fan_avg&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">(</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_avg&#39;</span><span class="p">,</span>
        <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span>
      <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">]:</span>

    <span class="k">def</span> <span class="nf">XavierUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); scale*[-x, x]).&quot;&quot;&quot;</span>
      <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">shape</span><span class="se">\&#39;</span><span class="s1"> must not be </span><span class="se">\&#39;</span><span class="s1">None</span><span class="se">\&#39;</span><span class="s1"> or 0 for XavierUniform&#39;</span><span class="p">)</span>
      <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">*</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

    <span class="n">v_init</span> <span class="o">=</span> <span class="n">XavierUniform</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span>
  <span class="p">]:</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">:</span>
      <span class="c1"># Assume the &#39;a&#39; parameter is the &#39;scale&#39; argument.</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_dev</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">bound</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">bound</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;init_type `</span><span class="si">%s</span><span class="s1">` not supported.&#39;</span> <span class="o">%</span> <span class="n">method</span>

  <span class="k">return</span> <span class="n">v_init</span></div>


<div class="viewcode-block" id="_GenerateStatelessRngSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GenerateStatelessRngSeed">[docs]</a><span class="k">def</span> <span class="nf">_GenerateStatelessRngSeed</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates a 2-tuple seed for a stateless variable initializer.</span>

<span class="sd">  We want to ensure that different variables end up with different random values</span>
<span class="sd">  even when they are passed the same seed and shape. To this aim, this function</span>
<span class="sd">  generates a pseudo-unique seed by hashing the variable name and mapping it</span>
<span class="sd">  into a scalar seed. More specifically, the returned value is a 2-tuple of</span>
<span class="sd">  tf.int32 scalar, where the first element is the user-provided seed and the</span>
<span class="sd">  second element is obtained by hashing the variable name.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: The variable name for which to generate a stateless-like seed.</span>
<span class="sd">    seed: The user-specified scalar seed.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A 2-tuple seed of tf.int32 values (for TPU compatibility).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">seed0</span> <span class="o">=</span> <span class="n">seed</span> <span class="ow">or</span> <span class="mi">0</span>
  <span class="n">seed1</span> <span class="o">=</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">seed0</span><span class="p">,</span> <span class="n">seed1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span></div>


<div class="viewcode-block" id="_DeterministicRandomNormalInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomNormalInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomNormalInitializer</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a random normal initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">DeterministicNormal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
    <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_normal</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">DeterministicNormal</span></div>


<div class="viewcode-block" id="_DeterministicRandomUniformInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomUniformInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomUniformInitializer</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a random uniform initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">DeterministicUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
    <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_uniform</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">maxval</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">DeterministicUniform</span></div>


<div class="viewcode-block" id="_DeterministicRandomTruncatedNormalInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomTruncatedNormalInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomTruncatedNormalInitializer</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a random truncated normal initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">DeterministicTruncatedNormal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
    <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_truncated_normal</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">DeterministicTruncatedNormal</span></div>


<div class="viewcode-block" id="_DeterministicRandomUniformUnitScalingInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomUniformUnitScalingInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomUniformUnitScalingInitializer</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a random uniform unit scaling initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">DeterministicUniformUnitScaling</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="c1"># The following logic is originally from (UniformUnitScaling.__call__())</span>
    <span class="c1"># in TensorFlow: python/ops/init_ops.py</span>
    <span class="n">scale_shape</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="k">if</span> <span class="n">partition_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale_shape</span> <span class="o">=</span> <span class="n">partition_info</span><span class="o">.</span><span class="n">full_shape</span>

    <span class="n">input_size</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="c1"># Estimating input size is not possible to do perfectly, but we try.</span>
    <span class="c1"># The estimate, obtained by multiplying all dimensions but the last one,</span>
    <span class="c1"># is the right thing for matrix multiply and convolutions (see above).</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">scale_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
      <span class="n">input_size</span> <span class="o">*=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="c1"># Avoid errors when initializing zero-size tensors.</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">maxval</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span> <span class="o">/</span> <span class="n">input_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">factor</span>
    <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_uniform</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="n">maxval</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">maxval</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">DeterministicUniformUnitScaling</span></div>


<div class="viewcode-block" id="_DeterministicRandomVarianceScalingInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomVarianceScalingInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomVarianceScalingInitializer</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span>
                                                   <span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a variance scaling initializer.&quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">scale</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`scale` must be positive float.&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="s1">&#39;fan_avg&#39;</span><span class="p">}:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid `mode` argument:&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
  <span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">distribution</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span>
      <span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_normal&#39;</span><span class="p">,</span> <span class="s1">&#39;untruncated_normal&#39;</span>
  <span class="p">}:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid `distribution` argument:&#39;</span><span class="p">,</span> <span class="n">distribution</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">DeterministicVarianceScaling</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="c1"># This is originally from TensorFlow: python/ops/init_ops.py</span>
    <span class="n">scale_shape</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="k">if</span> <span class="n">partition_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale_shape</span> <span class="o">=</span> <span class="n">partition_info</span><span class="o">.</span><span class="n">full_shape</span>
    <span class="c1"># Handle special case of empty list as shape, since fan_in and fan_out</span>
    <span class="c1"># are numerically added below. Without this, GetFanInFanOut() would</span>
    <span class="c1"># return None, None instead.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">scale_shape</span><span class="p">:</span>
      <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">scale_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;fan_in&#39;</span><span class="p">:</span>
      <span class="n">scale_inner</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;fan_out&#39;</span><span class="p">:</span>
      <span class="n">scale_inner</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scale_inner</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span> <span class="ow">or</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;truncated_normal&#39;</span><span class="p">:</span>
      <span class="c1"># constant taken from scipy.stats.truncnorm.std(</span>
      <span class="c1">#                         a=-2, b=2, loc=0., scale=1.)</span>
      <span class="n">stddev</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scale_inner</span><span class="p">)</span> <span class="o">/</span> <span class="mf">.87962566103423978</span>
      <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_truncated_normal</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;untruncated_normal&#39;</span><span class="p">:</span>
      <span class="n">stddev</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scale_inner</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_normal</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">*</span> <span class="n">scale_inner</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_uniform</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="n">limit</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">DeterministicVarianceScaling</span></div>


<div class="viewcode-block" id="_DeterministicRandomXavierUniformInitializer"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DeterministicRandomXavierUniformInitializer">[docs]</a><span class="k">def</span> <span class="nf">_DeterministicRandomXavierUniformInitializer</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a variance scaling initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">XavierUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); scale*[-x, x]).&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">shape</span><span class="se">\&#39;</span><span class="s1"> must not be </span><span class="se">\&#39;</span><span class="s1">None</span><span class="se">\&#39;</span><span class="s1"> or 0 for XavierUniform&#39;</span><span class="p">)</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">:</span>
      <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">*</span> <span class="n">fan_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">stateless_random_ops</span><span class="o">.</span><span class="n">stateless_random_uniform</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">XavierUniform</span></div>


<div class="viewcode-block" id="_CreateVarInitStateless"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CreateVarInitStateless">[docs]</a><span class="k">def</span> <span class="nf">_CreateVarInitStateless</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates variable initialization function for a stateless RNG.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span>
  <span class="p">]):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="c1"># This is probably not the right method to use when len(shape) &gt; 2,</span>
      <span class="c1"># e.g. dim0 will be 3 with a 3x3 conv2d kernel.</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s1">&#39;Initializing </span><span class="si">%s</span><span class="s1"> of shape </span><span class="si">%s</span><span class="s1"> with method </span><span class="si">%s</span><span class="s1">: dim0=</span><span class="si">%s</span><span class="s1">. &#39;</span>
          <span class="s1">&#39;Make sure that it is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">dim0</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim0</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">]:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanavg&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span>
      <span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanavg&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomNormalInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomUniformInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="n">scale</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomUniformInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomUniformUnitScalingInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling_fan_avg&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomVarianceScalingInitializer</span><span class="p">(</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_avg&#39;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span>
      <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomTruncatedNormalInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomXavierUniformInitializer</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span>
  <span class="p">]:</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">:</span>
      <span class="c1"># Assume the &#39;a&#39; parameter is the &#39;scale&#39; argument.</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_dev</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">_DeterministicRandomUniformInitializer</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="n">bound</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">bound</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;init_type </span><span class="si">%s</span><span class="s1"> not supported.&#39;</span> <span class="o">%</span> <span class="n">method</span>

  <span class="k">return</span> <span class="n">v_init</span></div>


<span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="GetGlobalVariableScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetGlobalVariableScope">[docs]</a><span class="k">def</span> <span class="nf">GetGlobalVariableScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets the global variable scope (as if no variable_scope has been set).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The VariableScope corresponding to as if no tf.variable_scope is in effect.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_global_variable_scope</span><span class="p">:</span>
    <span class="c1"># Each thread gets its own default global variable scope, and we take</span>
    <span class="c1"># advantage of that in order to get a top-level scope. This avoids the</span>
    <span class="c1"># need to call tf.get_variable_scope() at the module level, which allows</span>
    <span class="c1"># this module to be imported without modifying global state (i.e. creating</span>
    <span class="c1"># the default graph). It is important to not mutate the global state at</span>
    <span class="c1"># module load time, because it let&#39;s us flip flags after import that affect</span>
    <span class="c1"># core TensorFlow behavior.</span>
    <span class="k">def</span> <span class="nf">Initialize</span><span class="p">():</span>
      <span class="k">global</span> <span class="n">_global_variable_scope</span>
      <span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>

    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">Initialize</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">_global_variable_scope</span></div>


<span class="n">_GLOBAL_STEP_STACK</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="GlobalStepContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GlobalStepContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">GlobalStepContext</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">):</span>
  <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetGlobalStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetGlobalStep">[docs]</a><span class="k">def</span> <span class="nf">GetGlobalStep</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetOrCreateGlobalStepVar"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetOrCreateGlobalStepVar">[docs]</a><span class="k">def</span> <span class="nf">GetOrCreateGlobalStepVar</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step variable, creating it if it does not exist.</span>

<span class="sd">  Prefer GetGlobalStep if a tensor rather than a tf.Variable is sufficient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The global_step variable, or a new created one if it does not exist.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">GetGlobalVariableScope</span><span class="p">(),</span> <span class="n">use_resource</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;pin_vars_to_cpu&#39;</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/cpu:0&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span></div>


<div class="viewcode-block" id="LogMultiLines"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.LogMultiLines">[docs]</a><span class="k">def</span> <span class="nf">LogMultiLines</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">lines</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span></div>


<div class="viewcode-block" id="_LogPlacement"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._LogPlacement">[docs]</a><span class="k">def</span> <span class="nf">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Logs theta and its copy&#39;s device placement.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">GetDevices</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten a `.NestedMap` m and extracts each value&#39;s device.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()]</span>

  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;=== </span><span class="si">%s</span><span class="s1"> ===&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
  <span class="n">LogMultiLines</span><span class="p">(</span>
      <span class="n">label</span><span class="p">,</span>
      <span class="n">theta</span><span class="o">.</span><span class="n">Pack</span><span class="p">([(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> -&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">GetDevices</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">GetDevices</span><span class="p">(</span><span class="n">copy</span><span class="p">))</span>
                 <span class="p">])</span><span class="o">.</span><span class="n">DebugString</span><span class="p">())</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;==========&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="CreateLocalTheta"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CreateLocalTheta">[docs]</a><span class="k">def</span> <span class="nf">CreateLocalTheta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">device_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates local copy of theta and shards across devices device list.</span>

<span class="sd">  Leaves variables intact.</span>

<span class="sd">  Args:</span>
<span class="sd">    theta: a `.NestedMap` of variables.</span>
<span class="sd">    device_list: list of devices to shard across. If None, defaults to a list</span>
<span class="sd">      [&#39;&#39;].</span>
<span class="sd">    label: Logging label.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of identity() wrapped theta</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">AddIdentity</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Helper class.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_list</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_list</span> <span class="o">=</span> <span class="n">device_list</span> <span class="k">if</span> <span class="n">device_list</span> <span class="k">else</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">)]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">copy</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">AddIdentity</span><span class="p">(</span><span class="n">device_list</span><span class="p">))</span>
  <span class="n">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">copy</span></div>


<div class="viewcode-block" id="_GetVarsToLoad"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetVarsToLoad">[docs]</a><span class="k">def</span> <span class="nf">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">,</span>
                   <span class="n">ckpt_path</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Determines variables to load and their names in checkpoint.&quot;&quot;&quot;</span>
  <span class="c1"># This list contains mappings from var names as they appear in the checkpoint</span>
  <span class="c1"># to the vars in our model they correspond to.</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">model_var</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
    <span class="n">loaded</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">variable_loading_rules</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># Skip if var doesn&#39;t match the loading rules, or if it should be ignored.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Loading rules do not match </span><span class="si">%s</span><span class="s1">.&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">continue</span>
      <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Ignoring </span><span class="si">%s</span><span class="s1"> from loading.&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">continue</span>
      <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">checkpoint_var_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;:0&#39;</span><span class="p">):</span>
        <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">checkpoint_var_name</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Loading </span><span class="si">%s</span><span class="s1"> from </span><span class="si">%s</span><span class="s1"> with regexp: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                      <span class="n">checkpoint_var_name</span><span class="p">,</span> <span class="n">regexp</span><span class="p">)</span>
      <span class="n">vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">checkpoint_var_name</span><span class="p">,</span> <span class="n">model_var</span><span class="p">))</span>
      <span class="n">loaded</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">loaded</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
          <span class="s1">&#39;Not loading model variable </span><span class="si">%s</span><span class="s1"> from </span><span class="si">%s</span><span class="s1"> as it does not match any rules&#39;</span>
          <span class="s1">&#39; or matches ignored&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vars_to_load</span></div>


<div class="viewcode-block" id="OverrideVarsFromCheckpoint"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OverrideVarsFromCheckpoint">[docs]</a><span class="k">def</span> <span class="nf">OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span>
                               <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add TF graph ops to override variables from a provided checkpoint.</span>

<span class="sd">  Args:</span>
<span class="sd">    all_vars: List of all the parameters in the model.</span>
<span class="sd">    checkpoint_path: A path to the checkpoints of a pretrained model.</span>
<span class="sd">    variable_loading_rules: A list of tuples of strings defining (regex to match</span>
<span class="sd">      parameter names in the model to override, format string to determine the</span>
<span class="sd">      corresponding var in the checkpoint).</span>
<span class="sd">    var_ignore_rules: A list consisting of a list of regexes to match parameter</span>
<span class="sd">      names in the model which should not be overridden, even if they match</span>
<span class="sd">      those in the loading rules.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A callable that, when called with a tf.Session, will restore the variables</span>
<span class="sd">    from the provided checkpoint.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span>
                                <span class="n">var_ignore_rules</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="n">all_rules_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1"> --&gt; </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable_loading_rules</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variable loading rules </span><span class="si">{</span><span class="n">all_rules_text</span><span class="si">}</span><span class="s1"> &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;did not match any of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_vars</span><span class="p">)</span><span class="si">}</span><span class="s1"> vars.&#39;</span><span class="p">)</span>
  <span class="n">load_var_names</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">]))</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Overriding </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vars_to_load</span><span class="p">)</span><span class="si">}</span><span class="s1"> vars from &#39;</span>
                  <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s1">:</span><span class="se">\n</span><span class="si">{</span><span class="n">load_var_names</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="n">savers</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">while</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="c1"># When restoring, it&#39;s possible the same value in the checkpoint</span>
    <span class="c1"># can be restored to multiple variables (e.g. during</span>
    <span class="c1"># distillation).  However, tf.train.Saver, since it&#39;s used for</span>
    <span class="c1"># both saving and restoring, requires the name in the checkpoint</span>
    <span class="c1"># to be unique for each variable.  So, we call it multiple times</span>
    <span class="c1"># with a unique set of names each time.</span>
    <span class="n">unique_vars_to_load</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">remaining_vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_vars_to_load</span><span class="p">:</span>
        <span class="n">unique_vars_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">remaining_vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="n">savers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="n">unique_vars_to_load</span><span class="p">,</span> <span class="n">sharded</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">remaining_vars_to_load</span>

  <span class="k">def</span> <span class="nf">_Restore</span><span class="p">(</span><span class="n">sess</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">saver</span> <span class="ow">in</span> <span class="n">savers</span><span class="p">:</span>
      <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_Restore</span></div>


<div class="viewcode-block" id="OverrideVarsFromCheckpoints"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OverrideVarsFromCheckpoints">[docs]</a><span class="k">def</span> <span class="nf">OverrideVarsFromCheckpoints</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpts_loading_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add TF graph ops to override model variables from checkpoints.</span>

<span class="sd">  Args:</span>
<span class="sd">    all_vars: List of all the parameters in the model.</span>
<span class="sd">    ckpts_loading_rules: A dictionary of checkpoint path: loading rules.</span>
<span class="sd">      Checkpoint path must be a path to a pretrained model, and loading rules is</span>
<span class="sd">      expected to be a tuple of two lists. The first consisting of tuples of</span>
<span class="sd">      strings defining (regex to match parameter names in the model to override,</span>
<span class="sd">      format string to determine the corresponding var in the checkpoint), and</span>
<span class="sd">      the second list consisting of a list of regexes to match parameter names</span>
<span class="sd">      in the model which should not be overridden, even if they match those in</span>
<span class="sd">      the loading rules.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A callable that, when called with a tf.Session, will restore the variables</span>
<span class="sd">    from checkpoint and return a list of overwritten variables.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if colliding vars exist or loading rules is not a list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ckpts_loading_rules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from multiple checkpoints.&#39;</span><span class="p">)</span>

  <span class="n">var_refs_overridden</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">var_names_overridden</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">restore_fns</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span> <span class="ow">in</span> <span class="n">ckpts_loading_rules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from checkpoint: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">loading_rules</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="c1"># Filter the model variables to be overridden.</span>
    <span class="n">to_load_vars</span> <span class="o">=</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="n">ckpt_path</span><span class="p">)</span>
    <span class="n">var_refs_to_override</span> <span class="o">=</span> <span class="p">[</span><span class="n">var</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">experimental_ref</span><span class="p">()</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">to_load_vars</span><span class="p">]</span>
    <span class="n">var_names_to_override</span> <span class="o">=</span> <span class="p">[</span><span class="n">var</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">to_load_vars</span><span class="p">]</span>

    <span class="n">overlap_refs</span> <span class="o">=</span> <span class="nb">set</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">var_refs_overridden</span><span class="p">,</span> <span class="n">var_refs_to_override</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">overlap_refs</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Colliding variables to override: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">overlap_refs</span><span class="p">)</span>

    <span class="n">restore_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                   <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">var_refs_overridden</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">var_refs_to_override</span><span class="p">)</span>
    <span class="n">var_names_overridden</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">var_names_to_override</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Model variables overridden: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var_refs_overridden</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_Restore</span><span class="p">(</span><span class="n">sess</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">restore_fns</span><span class="p">:</span>
      <span class="n">fn</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">var_names_overridden</span>

  <span class="k">return</span> <span class="n">_Restore</span></div>


<div class="viewcode-block" id="ComputeGradientsSimple"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeGradientsSimple">[docs]</a><span class="k">def</span> <span class="nf">ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss_or_activations</span><span class="p">,</span>
                           <span class="n">all_vars</span><span class="p">,</span>
                           <span class="n">grad_aggregation_method</span><span class="p">,</span>
                           <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                           <span class="n">gate_gradients</span><span class="p">,</span>
                           <span class="n">activations_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span>
      <span class="n">loss_or_activations</span><span class="p">,</span>
      <span class="n">all_vars</span><span class="p">,</span>
      <span class="n">grad_ys</span><span class="o">=</span><span class="n">activations_grad</span><span class="p">,</span>
      <span class="n">aggregation_method</span><span class="o">=</span><span class="n">grad_aggregation_method</span><span class="p">,</span>
      <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
      <span class="n">gate_gradients</span><span class="o">=</span><span class="n">gate_gradients</span><span class="p">)</span></div>


<div class="viewcode-block" id="ComputeTpuEmbeddingGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeTpuEmbeddingGradients">[docs]</a><span class="k">def</span> <span class="nf">ComputeTpuEmbeddingGradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">activation_dict</span><span class="p">,</span> <span class="n">tpu_embedding</span><span class="p">,</span>
                                 <span class="n">gradient_multiplier_schedule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a TpuEmbedding SendGradient op.</span>

<span class="sd">  Args:</span>
<span class="sd">   loss: The loss to backprop from.</span>
<span class="sd">   activation_dict: String feature -&gt; embedding activations dict.</span>
<span class="sd">   tpu_embedding: TPUEmbedding instance.</span>
<span class="sd">   gradient_multiplier_schedule: Values from this schedule will be multiplied to</span>
<span class="sd">     the TPUEmbedding gradients.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Scale the loss to account for the full batch size.</span>
  <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
  <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

  <span class="c1"># Apply gradient multiplier schedule.</span>
  <span class="n">grad_multiplier</span> <span class="o">=</span> <span class="n">gradient_multiplier_schedule</span><span class="o">.</span><span class="n">Value</span><span class="p">()</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span> <span class="o">*</span> <span class="n">grad_multiplier</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">]</span>

  <span class="n">feature_to_gradient_dict</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span>
      <span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="n">grads</span><span class="p">))</span>
  <span class="n">send_gradient_op</span> <span class="o">=</span> <span class="n">tpu_embedding</span><span class="o">.</span><span class="n">generate_send_gradients_op</span><span class="p">(</span>
      <span class="n">feature_to_gradient_dict</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">GetGlobalStep</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">send_gradient_op</span></div>


<div class="viewcode-block" id="_ComputeGradientsTpu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._ComputeGradientsTpu">[docs]</a><span class="k">def</span> <span class="nf">_ComputeGradientsTpu</span><span class="p">(</span><span class="n">loss_or_activations</span><span class="p">,</span>
                         <span class="n">all_vars</span><span class="p">,</span>
                         <span class="n">grad_aggregation_method</span><span class="p">,</span>
                         <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                         <span class="n">gate_gradients</span><span class="p">,</span>
                         <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">defer_crs_to_apply_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">activations_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">is_activations</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients for local loss across whole TPU cluster.</span>

<span class="sd">  This implementation specializes for the case where weight params maybe used</span>
<span class="sd">  for different number of times in the forward computation, so that gradients</span>
<span class="sd">  should be normalized by the actual number of times they are being computed.</span>

<span class="sd">  TODO(yonghui): Maybe merge this implementation with the _ComputeGradientsTpu</span>
<span class="sd">  one.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss_or_activations: The loss or activations to backprop from.</span>
<span class="sd">    all_vars: Vars with respect to which gradients are to be computed.</span>
<span class="sd">    grad_aggregation_method: aggregation method to use when calling</span>
<span class="sd">      tf.gradients.</span>
<span class="sd">    colocate_gradients_with_ops: boolean, whether or not to colocate gradient op</span>
<span class="sd">      with the original op.</span>
<span class="sd">    gate_gradients: boolean, flag to be passed to tf.gradients.</span>
<span class="sd">    skip_zero_gradients: whether to skip zero gradients during aggregation.</span>
<span class="sd">    use_bf16_gradients_ar: Whether to use bfloat16 dtype for gradients</span>
<span class="sd">      all-reduce.</span>
<span class="sd">    defer_crs_to_apply_grad: Whether to defer gradient cross replica sum to</span>
<span class="sd">      apply_gradient. This helps reducing the number of gradient all-reduces</span>
<span class="sd">      when doing gradient accumulation, which does gradient cross replica sum</span>
<span class="sd">      only every k steps in a tf.cond. Currently this works only when</span>
<span class="sd">      skip_zero_gradients is None.</span>
<span class="sd">    activations_grad: The gradients computed for activations.</span>
<span class="sd">    is_activations: A boolean, whether the input is loss or activations.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Gradients to be passed back.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: upon invalid arguments.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">is_activations</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">activations_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_zero_gradients</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_activations</span><span class="p">:</span>
    <span class="c1"># Scale the loss to account for the full batch size.</span>
    <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
    <span class="k">assert</span> <span class="n">shards</span>
    <span class="n">loss_or_activations</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
        <span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_or_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="c1"># Computes the gradients.</span>
  <span class="c1"># Sum the grads so that we can compute statistics across the whole batch.</span>
  <span class="n">all_grads</span> <span class="o">=</span> <span class="n">ComputeGradientsSimple</span><span class="p">(</span>
      <span class="n">loss_or_activations</span><span class="o">=</span><span class="n">loss_or_activations</span><span class="p">,</span>
      <span class="n">all_vars</span><span class="o">=</span><span class="n">all_vars</span><span class="p">,</span>
      <span class="n">grad_aggregation_method</span><span class="o">=</span><span class="n">grad_aggregation_method</span><span class="p">,</span>
      <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
      <span class="n">gate_gradients</span><span class="o">=</span><span class="n">gate_gradients</span><span class="p">,</span>
      <span class="n">activations_grad</span><span class="o">=</span><span class="n">activations_grad</span><span class="p">)</span>

  <span class="c1"># NOTE: We can&#39;t use tpu_optimizer.CrossShardOptimizer since</span>
  <span class="c1"># we need to scale the grads *after* the cross_replica_sum to</span>
  <span class="c1"># match GPU version!</span>

  <span class="c1"># TODO(cwhipkey): should we do something different here? - we could do</span>
  <span class="c1"># some operations on the gradients before the aggregation (see comments in</span>
  <span class="c1"># tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py - see compute_gradients -</span>
  <span class="c1"># for some more details).</span>

  <span class="n">aggregated_grads</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">all_grads</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">continue</span>
    <span class="k">if</span> <span class="n">use_bf16_gradients_ar</span><span class="p">:</span>
      <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">g</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">skip_zero_gradients</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># loss is already scaled by 1/shards.</span>
        <span class="k">if</span> <span class="n">defer_crs_to_apply_grad</span><span class="p">:</span>
          <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute the cross-replica mean of &#39;g&#39;, skipping zero gradients.</span>

        <span class="c1"># Q(yonghui): Is there a better way to detect a non-zero gradient?</span>
        <span class="c1"># Note(yonghui): gradient of a weight can be zero if that</span>
        <span class="c1"># weight is not used in the forward computation, e.g. as in</span>
        <span class="c1"># switchable layers in neural architecture search, pruned by channel</span>
        <span class="c1"># mask, or sparsified.</span>
        <span class="k">if</span> <span class="n">skip_zero_gradients</span> <span class="o">==</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span>
          <span class="c1"># Same shape as &#39;g&#39;.</span>
          <span class="n">g_is_non_zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">skip_zero_gradients</span> <span class="o">==</span> <span class="s1">&#39;variable&#39;</span><span class="p">:</span>
          <span class="c1"># A variable-wide 0/1 scalar.</span>
          <span class="n">g_is_non_zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mf">1e-24</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown skip_zero_gradients: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                           <span class="n">skip_zero_gradients</span><span class="p">)</span>
        <span class="n">num_updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g_is_non_zero</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_updates</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">normalized_g</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">aggregated_grads</span></div>


<div class="viewcode-block" id="VarGrad"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VarGrad">[docs]</a><span class="k">class</span> <span class="nc">VarGrad</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;A class that holds a variable and a gradient.&quot;&quot;&quot;</span>

  <span class="n">_VAR_GRAD</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;VarGradNamedTuple&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">])</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_VAR_GRAD</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;VarGrad(</span><span class="si">%r</span><span class="s1">, </span><span class="si">%r</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span></div>


<div class="viewcode-block" id="SkipNoneGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SkipNoneGradients">[docs]</a><span class="k">def</span> <span class="nf">SkipNoneGradients</span><span class="p">(</span><span class="n">var_grads</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes pairs whose grad is None.&quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;ComputeGradients drops </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">var_grad</span><span class="p">:</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="ComputeGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeGradients">[docs]</a><span class="k">def</span> <span class="nf">ComputeGradients</span><span class="p">(</span>
    <span class="n">loss_or_activations</span><span class="p">,</span>
    <span class="n">vmap</span><span class="p">,</span>
    <span class="n">grad_aggregation_method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AggregationMethod</span><span class="o">.</span><span class="n">EXPERIMENTAL_TREE</span><span class="p">,</span>
    <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">compute_gradients_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">skip_none_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">defer_crs_to_apply_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">activations_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">is_activations</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss_or_activations: either the loss, which is a scalar tensor, or</span>
<span class="sd">      activations, which could be a tensor or a list of tensors.</span>
<span class="sd">    vmap: A `.NestedMap` of variables.</span>
<span class="sd">    grad_aggregation_method: Specifies the method used to combine gradient</span>
<span class="sd">      terms. Accepted values are constants defined in the class</span>
<span class="sd">      AggregationMethod.</span>
<span class="sd">    colocate_gradients_with_ops: If True, try colocating gradients with the</span>
<span class="sd">      corresponding op.</span>
<span class="sd">    gate_gradients: If True, add a tuple around the gradients returned for an</span>
<span class="sd">      operations. This avoids some race conditions.</span>
<span class="sd">    compute_gradients_fn: Function to use to compute gradients. If None, use</span>
<span class="sd">      default. compute_gradients_fn should have the same signature as this</span>
<span class="sd">      function, but without the last argument.</span>
<span class="sd">    skip_zero_gradients: Whether to skip aggregating zero gradients. This helps</span>
<span class="sd">      in case where some weights may not be used in forward computation, e.g.,</span>
<span class="sd">      sparsely activated networks or switchable layers in neural architectural</span>
<span class="sd">      search. Only applicable on TPU.</span>
<span class="sd">      Possible values are:</span>

<span class="sd">        - None: do not skip zero gradients;</span>
<span class="sd">        - `variable`: skip if the entire variable&#39;s gradients are almost zero;</span>
<span class="sd">          reduce_sum(abs(grads)) &lt; 1e-8.</span>
<span class="sd">        - `weight`: skip if the individual weight&#39;s gradients are almost zero:</span>
<span class="sd">          abs(grad) &lt; 1e-8.</span>

<span class="sd">    use_bf16_gradients_ar: Whether to use bfloat16 dtype for gradients</span>
<span class="sd">      all-reduce. This applies to TPU only.</span>
<span class="sd">    skip_none_gradients: Whether to skip gradients that are None.</span>
<span class="sd">    defer_crs_to_apply_grad: Whether to defer gradient cross replica sum to</span>
<span class="sd">      apply_gradient. This applies to TPU only.</span>
<span class="sd">    activations_grad: The gradients computed for activations.</span>
<span class="sd">    is_activations: A boolean, whether the input is loss or activations.</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of VarGrad. You can view</span>
<span class="sd">    var_grad as an ordered list of (key, (var, grad)) tuples. Every</span>
<span class="sd">    key of var_grad exists in vmap. Every variable in vmap that</span>
<span class="sd">    contributes to loss must exist in var_grad. Every var of var_grad</span>
<span class="sd">    must exist in vmap.  grad is the corresponding gradient computed</span>
<span class="sd">    for var. grad is guaranteed to be not None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">is_activations</span><span class="p">:</span>
    <span class="n">loss_or_activations</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">loss_or_activations</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vmap</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">skip_zero_gradients</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>

  <span class="c1"># Uniqify and remove None.</span>
  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="c1"># Filter out variables not contributing to &#39;loss_or_activations&#39;.</span>
  <span class="n">trainable_variables</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">Needed</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trainable_variables</span><span class="p">:</span>
        <span class="c1"># Skip non-trainable variables. Otherwise,</span>
        <span class="c1"># tf.Optimizer.apply_gradients throws up an exception instead</span>
        <span class="c1"># of skipping the update.</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">Needed</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">filtered_vlist</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

  <span class="c1"># Use caller-supplied gradient function if supplied.</span>
  <span class="k">if</span> <span class="n">compute_gradients_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">take_grad</span> <span class="o">=</span> <span class="n">compute_gradients_fn</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># tpu vs non-tpu is slightly different.</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
          <span class="n">_ComputeGradientsTpu</span><span class="p">,</span>
          <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="n">skip_zero_gradients</span><span class="p">,</span>
          <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="n">use_bf16_gradients_ar</span><span class="p">,</span>
          <span class="n">defer_crs_to_apply_grad</span><span class="o">=</span><span class="n">defer_crs_to_apply_grad</span><span class="p">,</span>
          <span class="n">activations_grad</span><span class="o">=</span><span class="n">activations_grad</span><span class="p">,</span>
          <span class="n">is_activations</span><span class="o">=</span><span class="n">is_activations</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">ComputeGradientsSimple</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">take_grad</span><span class="p">(</span><span class="n">loss_or_activations</span><span class="p">,</span> <span class="n">filtered_vlist</span><span class="p">,</span>
                    <span class="n">grad_aggregation_method</span><span class="p">,</span> <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                    <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># Formulate pairs of (var, grad) and pack them into the same</span>
  <span class="c1"># structure as filtered_vmap.</span>
  <span class="n">var_grads</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span>
      <span class="p">[</span><span class="n">VarGrad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">filtered_vlist</span><span class="p">,</span> <span class="n">grads</span><span class="p">)])</span>

  <span class="k">if</span> <span class="n">skip_none_gradients</span><span class="p">:</span>
    <span class="n">var_grads</span> <span class="o">=</span> <span class="n">SkipNoneGradients</span><span class="p">(</span><span class="n">var_grads</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var_grads</span></div>


<div class="viewcode-block" id="MaskGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MaskGradients">[docs]</a><span class="k">def</span> <span class="nf">MaskGradients</span><span class="p">(</span><span class="n">var_grad</span><span class="p">,</span> <span class="n">grad_mask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of non-masked variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grad: A `.NestedMap` of (variable, gradient)</span>
<span class="sd">    grad_mask: A dict of (variable name, mask).</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of (variable, mask * gradient).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">ApplyMask</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">entry</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">grad_mask</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">ApplyMask</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyGradMultiplier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyGradMultiplier">[docs]</a><span class="k">def</span> <span class="nf">ApplyGradMultiplier</span><span class="p">(</span><span class="n">vs_gs</span><span class="p">,</span> <span class="n">grad_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scale gradients by grad_scale on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs: A `.NestedMap` of VarGrad.</span>
<span class="sd">    grad_scale: If None, each vs_gs entry has the scale. Otherwise, grad_scale</span>
<span class="sd">      applies to every entry.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of (variable, gradient * grad_scale). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="mf">0.</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Scale</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad_scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">scale</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">grad_scale</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="HasNanOrInfGradient"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasNanOrInfGradient">[docs]</a><span class="k">def</span> <span class="nf">HasNanOrInfGradient</span><span class="p">(</span><span class="n">var_grads</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a bool tensor to indicate if `var_grads` contains NaNs or Infs.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: A `.NestedMap` with (var, grad) tuple as the map value.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A bool scalar tensor to indicate if the `var_grads` contains NaNs or Infs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">HasNanOrInf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">(</span>
            <span class="p">[</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span>
             <span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">x</span><span class="p">))])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_inf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">([</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()])</span></div>


<div class="viewcode-block" id="ApplyGradNormClipping"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyGradNormClipping">[docs]</a><span class="k">def</span> <span class="nf">ApplyGradNormClipping</span><span class="p">(</span><span class="n">vs_gs</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Clip gradients to norm on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs: A `.NestedMap` of VarGrad.</span>
<span class="sd">    norm: Each tensor&#39;s gradient will be scaled down to have a maximum L2-norm</span>
<span class="sd">      value of `norm`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of VarGrad(variable, scaled_gradient). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Clip</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">norm</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Clip</span><span class="p">)</span></div>


<span class="n">SKIP_LP_REGULARIZATION</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_skip_lp_regularization&#39;</span>


<div class="viewcode-block" id="AdjustGradientsWithLpLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AdjustGradientsWithLpLoss">[docs]</a><span class="k">def</span> <span class="nf">AdjustGradientsWithLpLoss</span><span class="p">(</span><span class="n">var_grads</span><span class="p">,</span> <span class="n">lp_regularizer_weight</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adjusts the map of (var, grad) with Lp regularization, where p=1.0 or 2.0.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: a `.NestedMap` or list of (variable, gradient).</span>
<span class="sd">    lp_regularizer_weight: Lp regularization weight.</span>
<span class="sd">    p: For now we support 1.0 or 2.0.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (lp_loss, var_grads).</span>

<span class="sd">    - lp_loss: A scalar. The lp loss.</span>
<span class="sd">    - var_grads: a `.NestedMap` or list of (variable, gradient) regulated by Lp.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(yuancao): For now we support p=1 or 2, but this can be extended to</span>
  <span class="c1"># lp-norm in general.</span>

  <span class="k">assert</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="s1">&#39;For now we only support L1/L2 regularization.&#39;</span>

  <span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">uniq_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="o">.</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">uniq_ids</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">var</span>

  <span class="k">def</span> <span class="nf">ShouldAdjust</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">)</span>

  <span class="n">filtered_var_grads</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">var_grad</span> <span class="k">for</span> <span class="n">var_grad</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">var_grads</span><span class="p">)</span> <span class="k">if</span> <span class="n">ShouldAdjust</span><span class="p">(</span><span class="n">var_grad</span><span class="o">.</span><span class="n">var</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="n">filtered_vars</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">GetVar</span><span class="p">,</span> <span class="n">filtered_var_grads</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">filtered_vars</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;AdjustGradientsWithLpLoss: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumSquared</span><span class="p">(</span><span class="n">filtered_vars</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumAbs</span><span class="p">(</span><span class="n">filtered_vars</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LpGrad</span><span class="p">(</span><span class="n">var_grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adjusts item&#39;s grad w/ Lp loss term.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">var_grad</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="c1"># Question(rpang): do we apply Lp loss here even if &#39;var&#39; is in</span>
      <span class="c1"># SKIP_LP_REGULARIZATION?</span>
      <span class="c1">#</span>
      <span class="c1"># Note: IndexedSlces appears for embedding lookups.</span>
      <span class="c1"># Embedding lookup ids can have duplicate. For duplicated ids, we</span>
      <span class="c1"># only want to consider once for each ids.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>  <span class="c1"># [#ids, dims]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="c1"># Counts is a vector of size vocab_size. counts[i] is i-th words</span>
        <span class="c1"># occurrences in &#39;ids&#39;.</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ids</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

        <span class="c1"># Gradients for duplicated ids will be summed when they get</span>
        <span class="c1"># applied, and hence we account for that by first dividing</span>
        <span class="c1"># gradient resulting from lp loss by how many times the id is</span>
        <span class="c1"># duplicated.</span>
        <span class="c1">#</span>
        <span class="c1"># For each id in &#39;ids&#39;, we know counts[id] is non-zero,</span>
        <span class="c1"># hence, it&#39;s always safe to take reciprocal.</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ids</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [#ids, 1]</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">values</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">grad_v</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">var</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">var</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">grad_v</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">+=</span> <span class="n">delta</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">lp_loss</span><span class="p">,</span> <span class="n">Transform</span><span class="p">(</span><span class="n">LpGrad</span><span class="p">,</span> <span class="n">var_grads</span><span class="p">)</span></div>


<div class="viewcode-block" id="SplitRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SplitRecursively">[docs]</a><span class="k">def</span> <span class="nf">SplitRecursively</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits Tensors in &#39;x&#39; recursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: a Tensor, or a list or NestMap containing Tensors to split.</span>
<span class="sd">    num_splits: number of splits per Tensor.</span>
<span class="sd">    axis: the split axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of split values of length &#39;num_splits&#39;.</span>

<span class="sd">    - If &#39;x&#39; is a Tensor, a list of split Tensors.</span>
<span class="sd">    - If &#39;x&#39; is a list, a list of lists, where each sublist has the same length</span>
<span class="sd">      as &#39;x&#39; and the k&#39;th element in each sublist corresponds to a split of the</span>
<span class="sd">      k&#39;th element from &#39;x&#39;.</span>
<span class="sd">    - If &#39;x&#39; is a `.NestedMap`, a list of `.NestedMap`, where each field</span>
<span class="sd">      corresponds to a split from the same field of &#39;x&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">SplitRecursively</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">splits</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">NestedMap</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">val_splits</span> <span class="o">=</span> <span class="n">SplitRecursively</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for SplitRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span></div>


<div class="viewcode-block" id="ConcatRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatRecursively">[docs]</a><span class="k">def</span> <span class="nf">ConcatRecursively</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates tensors from &#39;splits&#39;.</span>

<span class="sd">  This is the inverse function of SplitRecursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    splits: a list of splits to concatenate, where elements can be Tensors,</span>
<span class="sd">      lists, or `.NestedMap`. The elements must share the same type and</span>
<span class="sd">      structure.  For example, list elements must have the same length;</span>
<span class="sd">      `.NestedMap` must have the same set of fields.</span>
<span class="sd">    axis: the concatenation axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Concatenated data.</span>

<span class="sd">    - If input &#39;splits&#39; are Tensors, returns a concatenated Tensor.</span>
<span class="sd">    - If input &#39;splits&#39; are lists, returns a list of the same length where the</span>
<span class="sd">      k&#39;th element represents concatenated data of the k&#39;th element from each</span>
<span class="sd">      split.</span>
<span class="sd">    - If input &#39;splits&#39; are `.NestedMap`, returns a `.NestedMap` with each field</span>
<span class="sd">      concatenated from corresponding fields of input splits.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if &#39;splits&#39; is not a list or elements of &#39;splits&#39; do not have</span>
<span class="sd">      known or matching types.</span>
<span class="sd">    ValueError: if &#39;splits&#39; is empty or elements of &#39;splits&#39; do not have</span>
<span class="sd">      matching structures.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Non-list inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">splits</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Empty inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>

  <span class="n">tmpl</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Length mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                           <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">))</span>
    <span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tmpl</span><span class="p">:</span>
      <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span></div>


<div class="viewcode-block" id="WeightedAvg"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightedAvg">[docs]</a><span class="k">def</span> <span class="nf">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">sum_reduction_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes weighted average of values from a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a tensor of values</span>
<span class="sd">    weights: a tensor of weights</span>
<span class="sd">    sum_reduction_fn: called to reduce the values and weights to single value</span>
<span class="sd">    name: name of metric.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (avg, total_weight).</span>

<span class="sd">    - avg: weighted average value</span>
<span class="sd">    - total_weight: sum of all weights</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;shape of values and weights tensors must match for metric &#39;</span> <span class="o">+</span> <span class="n">name</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">(</span>
      <span class="p">[</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)],</span> <span class="n">values</span><span class="p">)</span>
  <span class="n">total_weight</span> <span class="o">=</span> <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
  <span class="c1"># divide_no_nan only supports tf.{float,complex}*.</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
  <span class="n">avg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">divide_no_nan</span><span class="p">(</span>
      <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">total_weight</span></div>


<div class="viewcode-block" id="WeightedAvgOfMetrics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightedAvgOfMetrics">[docs]</a><span class="k">def</span> <span class="nf">WeightedAvgOfMetrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the weighted average of metrics in the list.</span>

<span class="sd">  Args:</span>
<span class="sd">    metrics: list of dictionaries of metrics</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - dictionary of weighted averages of each metrics.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_metrics</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_metrics</span><span class="p">:</span>
        <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values_and_weights</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">lists_of_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span></div>


<div class="viewcode-block" id="ConcatPerExampleTensors"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatPerExampleTensors">[docs]</a><span class="k">def</span> <span class="nf">ConcatPerExampleTensors</span><span class="p">(</span><span class="n">per_example</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenate per-example tensors from many hosts into one large block.</span>

<span class="sd">  Args:</span>
<span class="sd">    per_example: list of dictionaries of per-example tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - string -&gt; concatenated tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_per_example</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">per_example</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_per_example</span><span class="p">:</span>
        <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">lists_of_per_example</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span></div>


<div class="viewcode-block" id="CombineMetrics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CombineMetrics">[docs]</a><span class="k">def</span> <span class="nf">CombineMetrics</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Combines metrics from `loss_metric_weight_pairs` according to weights.</span>

<span class="sd">  Keys must either exist in all metrics, in which it will be processed as a</span>
<span class="sd">  weighted sum, or exist in only one metrics, in which case it will be copied.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss_metric_weight_pairs: a list of (metrics, weight) pairs, where each</span>
<span class="sd">      weight is a float and each metrics is a dict with str keys and</span>
<span class="sd">      (metric_value, target_weight) values.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dict with the same set of keys as input metrics and values of</span>
<span class="sd">    (weighted_sum(metric_value), weighted_sum(target_weight)).</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if there exists a metric that exists in more than one element</span>
<span class="sd">      of `loss_metric_weight_pairs` but not in all of them.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">all_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
      <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">])</span>  <span class="c1"># pylint: disable=g-complex-comprehension</span>
  <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">all_keys</span><span class="p">:</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">count</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Found metric </span><span class="si">%s</span><span class="s1"> which exists in more than one&#39;</span>
                       <span class="s1">&#39;but not all loss metrics.&#39;</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">total_val</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_target_weight</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">target_weight</span> <span class="o">=</span> <span class="n">loss_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="c1"># Single metric, don&#39;t multiply by weight.</span>
          <span class="n">total_val</span> <span class="o">=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">=</span> <span class="n">target_weight</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Total weighted sum of all predictions.</span>
          <span class="n">total_val</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target_weight</span>

    <span class="n">result</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_val</span> <span class="o">/</span> <span class="n">total_target_weight</span><span class="p">,</span> <span class="n">total_target_weight</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="AddVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddVN">[docs]</a><span class="k">def</span> <span class="nf">AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">per_step</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add variational noise to x.</span>

<span class="sd">  Args:</span>
<span class="sd">    p: Layer params, with a `vn` subparam containing `VariationalNoiseParams`.</span>
<span class="sd">    x: Input to add variational noise to.</span>
<span class="sd">    per_step: Whether to add per_step noise.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The input with variational noise added according to params.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">per_step</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;VN scale must be set.&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">=</span> <span class="n">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">GetGlobalStep</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span><span class="p">:</span>
      <span class="c1"># First element of seeds is global step.</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">seeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">seeds</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">noises</span> <span class="o">=</span> <span class="n">DeterministicVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">seed</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span><span class="p">:</span>
      <span class="c1"># TODO(b/171767456): Fix per_step_vn.</span>
      <span class="c1"># seed += GetGlobalStep() * 203984</span>
      <span class="k">pass</span>
    <span class="n">noises</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">noises</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">noises</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noises</span></div>


<div class="viewcode-block" id="VariationalNoiseParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariationalNoiseParams">[docs]</a><span class="k">def</span> <span class="nf">VariationalNoiseParams</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span>
                           <span class="n">global_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">per_step_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for variational noise.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
      <span class="s1">&#39;Std of the variational noise to apply . This can be a scalar,&#39;</span>
      <span class="s1">&#39; or a scalar tensor.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;global_vn&#39;</span><span class="p">,</span> <span class="n">global_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds global variational noise every training setp iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;per_step_vn&#39;</span><span class="p">,</span> <span class="n">per_step_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds per-timesetp variational noise iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate noise.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;deterministic&#39;</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="s1">&#39;If true, generate noise using&#39;</span>
      <span class="s1">&#39;stateless random ops that are compatible with TF functional ops.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span></div>


<div class="viewcode-block" id="DefaultVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefaultVN">[docs]</a><span class="k">def</span> <span class="nf">DefaultVN</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">VariationalNoiseParams</span><span class="p">(</span>
      <span class="n">scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">global_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">per_step_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<span class="c1"># To disable VN of a layer, we use 1.0 in the first input parameter</span>
<span class="c1"># of the following function because otherwise it is the same to DefaultVN()</span>
<span class="c1"># which will be updated by parent configuration in CopyBaseParams()</span>
<div class="viewcode-block" id="DisableVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DisableVN">[docs]</a><span class="k">def</span> <span class="nf">DisableVN</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">VariationalNoiseParams</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span></div>


<span class="c1"># Step seed keyed by graph.</span>
<span class="n">_STEP_SEED_DICT</span> <span class="o">=</span> <span class="n">ThreadLocalDict</span><span class="p">()</span>


<div class="viewcode-block" id="GetStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetStepSeed">[docs]</a><span class="k">def</span> <span class="nf">GetStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets step_seed.&quot;&quot;&quot;</span>
  <span class="n">key</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
  <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_STEP_SEED_DICT</span><span class="o">.</span><span class="n">dict</span><span class="p">:</span>
    <span class="n">ResetStepSeed</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">_STEP_SEED_DICT</span><span class="o">.</span><span class="n">dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span></div>


<div class="viewcode-block" id="ResetStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ResetStepSeed">[docs]</a><span class="k">def</span> <span class="nf">ResetStepSeed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Resets step_seed to specified value.&quot;&quot;&quot;</span>
  <span class="n">key</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
  <span class="n">_STEP_SEED_DICT</span><span class="o">.</span><span class="n">dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaybeResetStepSeedFromScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MaybeResetStepSeedFromScope">[docs]</a><span class="k">def</span> <span class="nf">MaybeResetStepSeedFromScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;In graph mode, resets step_seed according to the current named scope.</span>

<span class="sd">  This is used in graph mode to avoid &quot;tensor is from a different graph&quot;</span>
<span class="sd">  errors that happen when we share random seend tensors too much.</span>
<span class="sd">  See b/129159299 for more context.</span>

<span class="sd">  Eager mode does not have this problem, so in eager mode we do nothing.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;new_step_seed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">))</span></div>


<div class="viewcode-block" id="MaybeResetStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MaybeResetStepSeed">[docs]</a><span class="k">def</span> <span class="nf">MaybeResetStepSeed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;If we&#39;re in graph mode, reset the step seed.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetIncStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetIncStepSeed">[docs]</a><span class="k">def</span> <span class="nf">GetIncStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns and increments the step_seed.&quot;&quot;&quot;</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="c1"># TODO(lepikhin): introduce a routine filling a queue of uint32 random seeds</span>
  <span class="c1"># independent of underlying PRNG used by tensorflow.</span>
  <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">step_seed</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">step_seed</span></div>


<div class="viewcode-block" id="GenerateStepSeedPair"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GenerateStepSeedPair">[docs]</a><span class="k">def</span> <span class="nf">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">op_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates a seed pair for deterministic random operations in ...</span>

<span class="sd">  functional loops.</span>

<span class="sd">  This function retrieves a unique seed pair on each call, based off the current</span>
<span class="sd">  global step and step seed. The step seed ensures this function returns a</span>
<span class="sd">  unique seed pair on each call: calling this function automatically increments</span>
<span class="sd">  the step seed. The step seed is automatically reset at the beginning of each</span>
<span class="sd">  global step in the model&#39;s FProp and works transparently through recurrent.py.</span>

<span class="sd">  Args:</span>
<span class="sd">    p: A hyperparams.Params object, containing keys &#39;random_seed&#39; and</span>
<span class="sd">      &#39;is_inference&#39;.</span>
<span class="sd">    op_seed: An additional operation-level seed to apply.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A size 2 tensor of op seeds to use for stateless_random ops.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">seed_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Ensure GetIncStepSeed is called even inside the shortcut.</span>
    <span class="c1"># This ensures if p.random_seed is set for other ops that use this function</span>
    <span class="c1"># that they will get the same seed pair whether or not p.random_seed is set</span>
    <span class="c1"># for this specific call.</span>
    <span class="n">GetIncStepSeed</span><span class="p">()</span>
    <span class="c1"># Unlike tf.random*, stateless random ops are completely determined by the</span>
    <span class="c1"># passed-in seeds. This means at inference time the same inputs will produce</span>
    <span class="c1"># the same outputs, even if the model is supposed to have randomness such as</span>
    <span class="c1"># dropout during inference. We inject additional randomness only during</span>
    <span class="c1"># inference if the graph is exported with random_seed=None as a workaround.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">2</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">seed_dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">seed_dtype</span><span class="p">)</span>

  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetGlobalStep</span><span class="p">(),</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetIncStepSeed</span><span class="p">(),</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">global_step</span><span class="p">,</span> <span class="n">step_seed</span><span class="p">])</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span>
  <span class="k">if</span> <span class="n">op_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">op_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">op_seed</span><span class="p">,</span> <span class="n">seed_dtype</span><span class="p">)</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">op_seed</span>
  <span class="k">return</span> <span class="n">seeds</span></div>


<div class="viewcode-block" id="DeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DeterministicDropout">[docs]</a><span class="k">def</span> <span class="nf">DeterministicDropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">noise_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Similar to `tf.nn.dropout()`, but fully deterministic.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A float Tensor on which to apply dropout.</span>
<span class="sd">    keep_prob: A scalar `Tensor` of keep probability.</span>
<span class="sd">    seeds: A Tensor of shape [2]. 2 seeds for deterministic random number</span>
<span class="sd">      generator.</span>
<span class="sd">    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for</span>
<span class="sd">      randomly generated keep/drop flags.</span>
<span class="sd">    name: An optional name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same shape as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if keep_prob is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="s1">&#39;keep_prob must be in range (0, 1]. Value: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>
    <span class="c1"># uniform in [keep_prob, 1.0 + keep_prob)</span>
    <span class="c1"># StatelessRandomUniform op does not support non-float (e.g. bfloat16) dtype</span>
    <span class="c1"># and non-int32 seed types.</span>
    <span class="n">noise_shape</span> <span class="o">=</span> <span class="n">noise_shape</span> <span class="ow">or</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_uniform</span><span class="p">(</span>
        <span class="n">noise_shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span>
    <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">binary_tensor</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
    <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="DeterministicVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DeterministicVN">[docs]</a><span class="k">def</span> <span class="nf">DeterministicVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">noise_shape</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Produces Fully deterministic Gaussian noise from shape, mean and std.</span>

<span class="sd">  Args:</span>
<span class="sd">    params: Nested map of params.</span>
<span class="sd">    seeds: A Tensor of shape [2]. 2 seeds for deterministic random number</span>
<span class="sd">      generator.</span>
<span class="sd">    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for</span>
<span class="sd">      randomly generated Gaussian noise.</span>
<span class="sd">    mean: Mean for the Gaussian noise.</span>
<span class="sd">    std: Standard deviation for noise.</span>
<span class="sd">    name: An optional name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the shape noise_shape and type fprop_dtype.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;gaussian_noise&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="p">(</span>
        <span class="n">std</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_normal</span><span class="p">(</span><span class="n">noise_shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">,</span> <span class="n">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">random_tensor</span></div>


<span class="n">BATCH_NORM_UPDATES</span> <span class="o">=</span> <span class="s1">&#39;batch_norm_updates&#39;</span>

<span class="n">_BATCH_NORM_UPDATES_DICT</span> <span class="o">=</span> <span class="s1">&#39;__batch_norm_update_dict&#39;</span>
<span class="n">_get_batch_norm_updates_dict</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_BATCH_NORM_UPDATES_DICT</span><span class="p">,</span>
                                                 <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>


<div class="viewcode-block" id="UpdateBatchNormVars"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateBatchNormVars">[docs]</a><span class="k">def</span> <span class="nf">UpdateBatchNormVars</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Update batch normalization moving averages.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
      <span class="s1">&#39;AssignMovingAvg&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span>
          <span class="n">batch_norm_var</span><span class="p">,</span>
          <span class="n">batch_norm_stats</span><span class="p">,</span>
          <span class="n">decay</span><span class="p">,</span>
      <span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">):</span>
      <span class="n">decay</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="mf">1.0</span> <span class="o">-</span> <span class="n">decay</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">batch_norm_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
      <span class="n">update_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
          <span class="n">batch_norm_stats</span><span class="p">,</span> <span class="n">batch_norm_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">))</span> <span class="o">*</span> <span class="n">decay</span>
      <span class="n">has_nan_or_inf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">update_delta</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_inf</span><span class="p">(</span><span class="n">update_delta</span><span class="p">)))</span>
      <span class="n">update_delta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">has_nan_or_inf</span><span class="p">,</span>
                             <span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">update_delta</span><span class="p">),</span>
                             <span class="k">lambda</span><span class="p">:</span> <span class="n">update_delta</span><span class="p">)</span>
      <span class="n">bn_update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">update_delta</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">BATCH_NORM_UPDATES</span><span class="p">,</span> <span class="n">bn_update</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bn_update_dict</span>
    <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">bn_update</span></div>


<div class="viewcode-block" id="FindRelevantBatchNormUpdates"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FindRelevantBatchNormUpdates">[docs]</a><span class="k">def</span> <span class="nf">FindRelevantBatchNormUpdates</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">batch_norm_updates</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Finds and returns a list of relevant batch-normalization updates.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: The loss that is being optimized for. A tensor or a list of tensors.</span>
<span class="sd">    batch_norm_updates: A list of batch normalization updates.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pair of lists. The first list contains all the batch normalization updates</span>
<span class="sd">    that are relevant to the loss being optimized, and the second list contains</span>
<span class="sd">    all in batch_norm_updates but not in the first list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">dependent_ops_and_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">FindNeeded</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
  <span class="n">relevant_updates</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">irrelevant_updates</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">bn_update</span> <span class="ow">in</span> <span class="n">batch_norm_updates</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">bn_update_dict</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is probably not a valid batch normalization update op.&#39;</span>
        <span class="s1">&#39; Make sure batch normalization is done through calling&#39;</span>
        <span class="s1">&#39; the py_utils.UpdateBatchNormVars helper routine.&#39;</span><span class="p">)</span>
    <span class="n">bn_stat_name</span> <span class="o">=</span> <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">bn_stat_name</span> <span class="ow">in</span> <span class="n">dependent_ops_and_tensors</span><span class="p">:</span>
      <span class="c1"># If a batch normalization stat is computed in the forward pass in</span>
      <span class="c1"># computing loss, then the corresponding batch normalization update is</span>
      <span class="c1"># relevant. Otherwise, it is not.</span>
      <span class="n">relevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">irrelevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">relevant_updates</span><span class="p">,</span> <span class="n">irrelevant_updates</span></div>


<span class="n">_SAMPLE_STEP_STACK</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="SampleStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SampleStep">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">SampleStep</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A context for a sample step during decoding.</span>

<span class="sd">  Example usage::</span>

<span class="sd">      with py_utils.SampleStep(step):</span>
<span class="sd">        sample = self.DecodeOneStep()</span>

<span class="sd">  Args:</span>
<span class="sd">    step: the step tensor.</span>

<span class="sd">  Yields:</span>
<span class="sd">    a context manager for the step scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">_SAMPLE_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">step</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_SAMPLE_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="_GetSampleStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetSampleStep">[docs]</a><span class="k">def</span> <span class="nf">_GetSampleStep</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">_SAMPLE_STEP_STACK</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">_SAMPLE_STEP_STACK</span><span class="o">.</span><span class="n">stack</span> <span class="k">else</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="AddDebugTensor"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddDebugTensor">[docs]</a><span class="k">def</span> <span class="nf">AddDebugTensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds `tensor` to the debug collection.</span>

<span class="sd">  Prints the tensor if `--print_debug_tensors` is True.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A tensor.</span>
<span class="sd">    summarize: Only print this many entries of each tensor. If None, then a</span>
<span class="sd">      maximum of 3 elements are printed per input tensor.</span>
<span class="sd">    name: An optional name for the tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor that evaluates to the same value as the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;print_debug_tensors&#39;</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">_GetSampleStep</span><span class="p">()</span>
    <span class="n">tensors_to_print</span> <span class="o">=</span> <span class="p">([]</span> <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="n">step</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">s</span><span class="p">:</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Print</span><span class="p">(</span>
          <span class="n">tensor</span><span class="p">,</span>
          <span class="n">tensors_to_print</span><span class="p">,</span>
          <span class="n">message</span><span class="o">=</span><span class="s1">&#39;DEBUG tensor </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">s</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="ArgMax"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ArgMax">[docs]</a><span class="k">def</span> <span class="nf">ArgMax</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.argmax wrapper.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A tensor, whose last dimension is being reduced on.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of rank tf.rank(logits)-1. If i == ret[indices],</span>
<span class="sd">    logits[indices, i] is the maximum among logits[indices, :].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="_EnsureMatrixShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._EnsureMatrixShape">[docs]</a><span class="k">def</span> <span class="nf">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Matmul"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Matmul">[docs]</a><span class="k">def</span> <span class="nf">Matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.matmul wrapper expecting x and y are actually matrices.&quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="clip_by_value"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.clip_by_value">[docs]</a><span class="k">def</span> <span class="nf">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_real&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_imag&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="_TransformAndSum"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._TransformAndSum">[docs]</a><span class="k">def</span> <span class="nf">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">transform</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;TransformAndSum&#39;</span><span class="p">):</span>
    <span class="n">sum_transform</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">))]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">sum_transform</span><span class="p">)</span></div>


<div class="viewcode-block" id="SumSquared"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SumSquared">[docs]</a><span class="k">def</span> <span class="nf">SumSquared</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="SumAbs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SumAbs">[docs]</a><span class="k">def</span> <span class="nf">SumAbs</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReduceRms"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReduceRms">[docs]</a><span class="k">def</span> <span class="nf">ReduceRms</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Computes root mean square of tensor x with numerical stability.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Shape of x must be fully defined.&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">x</span>

  <span class="n">denom</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
  <span class="k">if</span> <span class="n">denom</span> <span class="o">&lt;=</span> <span class="mf">1e8</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;reduce_rms </span><span class="si">%s</span><span class="s1"> denom=</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
  <span class="n">sum_square_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">avg_square_x</span> <span class="o">=</span> <span class="n">sum_square_x</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sum_square_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">avg_square_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="PiecewiseConstant"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PiecewiseConstant">[docs]</a><span class="k">def</span> <span class="nf">PiecewiseConstant</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">vdtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the piecewise value of x_in.&quot;&quot;&quot;</span>
  <span class="n">x_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_in</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span>
  <span class="n">bs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">vs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="c1"># The following is equivalent to &#39;return vs[index]&#39;.</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
  <span class="n">one_hot_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">depth</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">one_hot_vec</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="PadSequenceDimension"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadSequenceDimension">[docs]</a><span class="k">def</span> <span class="nf">PadSequenceDimension</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads x to `length` using `pad_val` along the axis dim.</span>

<span class="sd">  Assumes `x` is a tensor with rank &gt;= 2, and it only pads `x` to `length`</span>
<span class="sd">  along the axis dim. Explicitly sets the returned tensor shape to `shape` if</span>
<span class="sd">  given. Raises runtime errors if x.shape[axis] &gt; length or</span>
<span class="sd">  x.shape[i] != shape[i] where i != axis.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: the tensor to be padded with axis dimension being the time. E.g., x</span>
<span class="sd">      usually has shape [batch, seq_len, ...], when axis=1.</span>
<span class="sd">    length: an int to specify the length to pad x to.</span>
<span class="sd">    pad_val: an int or float used to pad x.</span>
<span class="sd">    shape: an int array specifying the shape of the padded tensor if specified.</span>
<span class="sd">    axis: The dimension that x will be padded, default to 1.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The padded tensor with shape [batch, seq_len, ...], where</span>
<span class="sd">    ret[:, :seq_len, ...] == x, when axis=1, and similarly for other axes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="k">assert</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="n">slen</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rank</span><span class="p">)[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rank</span><span class="p">)]</span>
    <span class="n">pad</span><span class="p">[</span><span class="n">axis</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pad_len</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]):</span>
      <span class="n">slen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">([[</span><span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">pad_len</span><span class="p">],</span> <span class="p">[</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">static_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">static_shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Shape must be a list or tuple.&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ensure_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="PadSequenceTo"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadSequenceTo">[docs]</a><span class="k">def</span> <span class="nf">PadSequenceTo</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads `xs` and `padding` to `length` using `pad_val` along the 2nd dim.</span>

<span class="sd">  Pads `xs` to `length` using `pad_val`, and `padding` using 1.</span>
<span class="sd">  Raise error if `x.shape[:2]` and `padding.shape` are not the same.</span>

<span class="sd">  Args:</span>
<span class="sd">    xs: A Tensor or a list of Tensors of shape [batch, seqlen] or [batch,</span>
<span class="sd">      seqlen, ...].</span>
<span class="sd">    padding: A 0/1 Tensor of shape [batch, seqlen]. 1 is for padded locations.</span>
<span class="sd">    length: A Python int, the length to pad to.</span>
<span class="sd">    pad_val: A Python numeric, used for padding x.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of padded xs and padding.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">new_xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">xs</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">new_xs</span> <span class="o">=</span> <span class="n">xs</span>

  <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">new_xs</span><span class="p">:</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">slen</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">padding</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">slen</span><span class="p">])</span>

    <span class="n">new_x</span> <span class="o">=</span> <span class="n">PadSequenceDimension</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">PadSequenceDimension</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">padding</span></div>


<div class="viewcode-block" id="ApplyPadding"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyPadding">[docs]</a><span class="k">def</span> <span class="nf">ApplyPadding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">padded</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_select</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies padding to a tensor.</span>

<span class="sd">  This is preferable to using arithmetic means for masking out padded values</span>
<span class="sd">  such as::</span>

<span class="sd">      # Equiv to ApplyPadding(padding, x))</span>
<span class="sd">      x *= 1.0 - padding</span>
<span class="sd">      # Equiv to ApplyPadding(padding, new, old)</span>
<span class="sd">      new = old * padding + new * (1 - padding)</span>

<span class="sd">  Aside from just being easier to read and reason about, using this function</span>
<span class="sd">  is friendly to quantized representations because it does not mix arithmetic</span>
<span class="sd">  on the padding values with the values in the tensor being padded (which can</span>
<span class="sd">  have a very different range than the 0..1 padding tensor).</span>

<span class="sd">  In addition, this works around issues in quantized schemes where we are</span>
<span class="sd">  guaranteed to have an exact 0 but not necessarily any other number (i.e. 1).</span>

<span class="sd">  Args:</span>
<span class="sd">    padding: Tensor of padding values where 0 == keep and 1 == pad.</span>
<span class="sd">    x: Tensor to apply padding to.</span>
<span class="sd">    padded: Optional. Values to include for padded elements. Defaults to zeros.</span>
<span class="sd">      Must be the same shape as &#39;x&#39; if specified.</span>
<span class="sd">    broadcast: Whether to broadcast the padding shape to the shape of &#39;x&#39;. You</span>
<span class="sd">      almost certainly want this to be true as it matches how padding would be</span>
<span class="sd">      expanded if applied arithmetically.</span>
<span class="sd">    use_select: Controls whether padding is applied with a select-mask</span>
<span class="sd">      (True/default) or arithmetically (False). Some platforms have a</span>
<span class="sd">      sensitivity to one or the other and this is used to work around such</span>
<span class="sd">      issues.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with the same shape as x with padded values masked.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">Assert</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))),</span> <span class="p">[</span><span class="n">padding</span><span class="p">])</span>
  <span class="p">],</span> <span class="n">padding</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">use_select</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">padded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">broadcast</span><span class="p">:</span>
      <span class="c1"># Broadcast padding to the full shape.</span>
      <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padding</span> <span class="o">&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">padded</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">+=</span> <span class="n">padded</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padded</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="LengthsFromPaddings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.LengthsFromPaddings">[docs]</a><span class="k">def</span> <span class="nf">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes lengths of each sequence in a batch, ignoring trailing padding.</span>

<span class="sd">  Note the following isn&#39;t guaranteed due to leading paddings.</span>
<span class="sd">  PaddingsFromLengths(LengthsFromPaddings(x)) == x</span>

<span class="sd">  Args:</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    lengths tensor shaped [batch] containing the unpadded length of each</span>
<span class="sd">    sequence in the batch.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="c1"># Find the last unpadded value.</span>
  <span class="c1"># Cannot just use tf.reduce_sum because there might be leading paddings.</span>
  <span class="c1"># Everything after the last unpadded value has 1.0 - paddings == 0.0, so in</span>
  <span class="c1"># the cumsum below they will have the same value.</span>
  <span class="n">cumsum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">same_as_last_element</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">cumsum</span><span class="p">,</span> <span class="n">cumsum</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
  <span class="c1"># Counting the number of elements with the same value gives us num_padded + 1</span>
  <span class="c1"># and so counting the number that differs gives us num_padded - 1.</span>
  <span class="n">length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
      <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">same_as_last_element</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="c1"># Special case for all 0 paddings.</span>
  <span class="n">all_zero_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">all_zero_paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">length</span><span class="p">),</span> <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="PaddingsFromLengths"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PaddingsFromLengths">[docs]</a><span class="k">def</span> <span class="nf">PaddingsFromLengths</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes paddings Tensor from lengths.</span>

<span class="sd">  Note the following isn&#39;t guaranteed due to leading paddings.</span>
<span class="sd">  PaddingsFromLengths(LengthsFromPaddings(x)) == x.</span>

<span class="sd">  This method does not generate leading paddings.</span>

<span class="sd">  Args:</span>
<span class="sd">    lengths: A int32 Tensor of shape [B].</span>
<span class="sd">    maxlen: None or a Python int or a scalar Tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A 0/1 valued Tensor of shape [B, maxlen or ?] where 1s are padded positions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">lengths</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">maxlen</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">lengths</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">)],</span>
        <span class="n">lengths</span><span class="p">)</span>

  <span class="k">return</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="TrimTrailingPaddings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TrimTrailingPaddings">[docs]</a><span class="k">def</span> <span class="nf">TrimTrailingPaddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Trims trailing paddings from inputs.</span>

<span class="sd">  Since the number of dimensions is not fixed, this will not work on TPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor with shape [batch, length, ...].</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    Trimmed inputs and paddings. For compatibility reasons, the trimmed tensors</span>
<span class="sd">    will always have length at least 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">max_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">],</span> <span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">output_shape</span><span class="p">),</span> <span class="n">output_shape</span><span class="p">)</span>
  <span class="n">out_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">out_paddings</span></div>


<div class="viewcode-block" id="ReversePaddedSequence"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReversePaddedSequence">[docs]</a><span class="k">def</span> <span class="nf">ReversePaddedSequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Reverse inputs based on paddings.</span>

<span class="sd">  Only reverse the unpadded portion of `inputs`. It assumes inputs are only</span>
<span class="sd">  padded in the end.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor of [seq_length, batch_size, num_input_nodes].</span>
<span class="sd">    paddings: a tensor of float32/float64 zero or one of shape [seq_length,</span>
<span class="sd">      batch_size, 1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    A reversed tensor of the same shape as `inputs`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inversed_paddings</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">inputs_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inversed_paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_length</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConcatenatePaddedSequences"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatenatePaddedSequences">[docs]</a><span class="k">def</span> <span class="nf">ConcatenatePaddedSequences</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">padding0</span><span class="p">,</span> <span class="n">padding1</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates input sequences with varying lengths as defined by paddings.</span>

<span class="sd">  This is a helper function for concatenating 2 batches of input sequences,</span>
<span class="sd">  where each example in the batch can have different lengths, as defined by</span>
<span class="sd">  the corresponding paddings. To concatenate correctly, it makes use of</span>
<span class="sd">  tf.reverse_sequence to partially reverse the sequences before</span>
<span class="sd">  concatenating them together.</span>

<span class="sd">  NOTE: We assume that the tensors have no leading paddings.</span>

<span class="sd">  Args:</span>
<span class="sd">    input0: A tensor of size [batch, max_length, ...] or [max_length, batch,</span>
<span class="sd">      ...] depending on the value set for axis.</span>
<span class="sd">    input1:  A tensor of size [batch, max_length, ...] or [max_length, batch,</span>
<span class="sd">      ...] depending on the value set for axis.</span>
<span class="sd">    padding0: A Tensor of size [batch, max_length] or [max_length, batch]</span>
<span class="sd">      corresponding to the padding for input0.</span>
<span class="sd">    padding1: A Tensor of size [batch, max_length] or [max_length, batch]</span>
<span class="sd">      corresponding to the padding for input1.</span>
<span class="sd">    seq_dim: int, the time axis along which the tensors will be concatenated.</span>
<span class="sd">      Should be 0 or 1. Assumes that batch_dim is 1 - seq_dim.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The concatenation of input0 and input1, and the corresponding padding.</span>

<span class="sd">  Raises:</span>
<span class="sd">    tf.errors.InvalidArgumentError when seq_dim is not 0 or 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;seq_dim must be 0 or 1.&#39;</span><span class="p">)</span>
  <span class="n">batch_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">seq_dim</span>
  <span class="c1"># inpu0 and input1 should have the same batch size and same rank.</span>
  <span class="n">input0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">input0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span>
                   <span class="n">GetShape</span><span class="p">(</span><span class="n">input1</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">]),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetRank</span><span class="p">(</span><span class="n">input0</span><span class="p">),</span> <span class="n">GetRank</span><span class="p">(</span><span class="n">input1</span><span class="p">))</span>
  <span class="p">],</span> <span class="n">input0</span><span class="p">)</span>

  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">padding0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">]</span>
  <span class="c1"># batch dimension of inputs and paddings should match.</span>
  <span class="n">input0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">input0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">padding1</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">input0</span><span class="p">)</span>
  <span class="n">input0_seq_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">padding0</span><span class="p">)[</span><span class="n">seq_dim</span><span class="p">]],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">input1_seq_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">padding1</span><span class="p">)[</span><span class="n">seq_dim</span><span class="p">]],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="c1"># LengthsFromPaddings assumes that paddings is of size [batch, max_length].</span>
  <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">padding0</span><span class="p">)</span>
    <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">padding1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">padding0</span><span class="p">))</span>
    <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">padding1</span><span class="p">))</span>
  <span class="c1"># We assume that the tensors have no leading paddings.</span>
  <span class="c1"># TODO(arunnt): Concatenate tensors with leading paddings correctly.</span>
  <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span>
          <span class="n">seq_length0</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding0</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
  <span class="p">],</span> <span class="n">seq_length0</span><span class="p">)</span>
  <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span>
          <span class="n">seq_length1</span><span class="p">,</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding1</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
  <span class="p">],</span> <span class="n">seq_length1</span><span class="p">)</span>
  <span class="c1"># Concatenate input sequences.</span>
  <span class="n">reversed_input0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">input0</span><span class="p">,</span> <span class="n">seq_length0</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_input1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">input1</span><span class="p">,</span> <span class="n">input1_seq_dim</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_concat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">reversed_input1</span><span class="p">,</span> <span class="n">reversed_input0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">)</span>
  <span class="n">concat_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">reversed_concat</span><span class="p">,</span>
      <span class="n">seq_length0</span> <span class="o">+</span> <span class="n">input1_seq_dim</span><span class="p">,</span>
      <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span>
      <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="c1"># Concatenate paddings. Note that paddings are always a Tensor of 0s and 1s,</span>
  <span class="c1"># so, unlike the inputs, we don&#39;t have to reverse padding1, we can simply</span>
  <span class="c1"># concatenate reversed padding0 and padding1.</span>
  <span class="n">reversed_padding0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">padding0</span><span class="p">,</span> <span class="n">input0_seq_dim</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_concat_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">reversed_padding0</span><span class="p">,</span> <span class="n">padding1</span><span class="p">],</span>
                                      <span class="n">axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">)</span>
  <span class="n">concat_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">reversed_concat_padding</span><span class="p">,</span>
      <span class="n">input0_seq_dim</span> <span class="o">+</span> <span class="n">seq_length1</span><span class="p">,</span>
      <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span>
      <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">concat_inputs</span><span class="p">,</span> <span class="n">concat_paddings</span></div>


<div class="viewcode-block" id="ShiftLeft"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ShiftLeft">[docs]</a><span class="k">def</span> <span class="nf">ShiftLeft</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shift_size</span><span class="p">,</span> <span class="n">pad_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Shifts the values in a tensor to the left along the axis dimension.</span>

<span class="sd">  The first shift_size values are dropped, and the tensor is padded on the</span>
<span class="sd">  right with pad_val.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: the input tensor with the axis dim being time.</span>
<span class="sd">    shift_size: the number of frames &gt;= 0 to shift.</span>
<span class="sd">    pad_val: the value to pad on the right of the tensor.</span>
<span class="sd">    axis: The dimension along which the tensor will be shifted, default to 1.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A left shifted tensor on dimension axis.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">rank</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span>
      <span class="p">[</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
       <span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">shift_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]):</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">([[</span><span class="n">axis</span><span class="p">]],</span> <span class="p">[</span><span class="n">shift_size</span><span class="p">],</span> <span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">PadSequenceDimension</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rank</span><span class="p">),</span> <span class="n">time</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="Retry"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Retry">[docs]</a><span class="k">def</span> <span class="nf">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">retry</span><span class="o">.</span><span class="n">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<span class="c1"># FailedPreconditionError: variables are not initialized.</span>
<span class="c1"># AbortedError: processes restarts.</span>
<span class="c1"># UnavailableError: Bad hardware status: 0x1</span>
<span class="n">transient_tf_errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">FailedPreconditionError</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">AbortedError</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">UnavailableError</span><span class="p">)</span>


<div class="viewcode-block" id="RetryOnTransientTfError"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RetryOnTransientTfError">[docs]</a><span class="k">def</span> <span class="nf">RetryOnTransientTfError</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">Retry</span><span class="p">(</span><span class="n">transient_tf_errors</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="PadOrTrimTo"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadOrTrimTo">[docs]</a><span class="k">def</span> <span class="nf">PadOrTrimTo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">pad_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_after_contents</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pad and slice x to the given shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A tensor.</span>
<span class="sd">    shape: The shape of the returned tensor.</span>
<span class="sd">    pad_val: An int or float used to pad x.</span>
<span class="sd">    pad_after_contents: Whether to pad and trim after the original contents of</span>
<span class="sd">      each dimension.</span>

<span class="sd">  Returns:</span>
<span class="sd">    &#39;x&#39; is padded with pad_val and sliced so that the result has the given</span>
<span class="sd">    shape.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if shape is a tf.TensorShape and not fully defined.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1"> padding </span><span class="si">%s</span><span class="s1"> must be fully defined.&#39;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>

  <span class="n">pad</span> <span class="o">=</span> <span class="n">shape</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
  <span class="n">zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">pad_after_contents</span><span class="p">:</span>
    <span class="c1"># If dim_i is less than shape[i], pads after contents.</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">zeros</span><span class="p">,</span> <span class="n">pad</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># If dim_i is larger than shape[i], we slice [0:shape[i]] for dim_i.</span>
    <span class="n">slice_begin</span> <span class="o">=</span> <span class="n">zeros</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># If dim_i is less than shape[i], pads before contents.</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">pad</span><span class="p">,</span> <span class="n">zeros</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># If dim-i is larger than shape[i], we slice [dim_i - shape[i]:dim_i]</span>
    <span class="c1"># for dim_i.</span>
    <span class="n">slice_begin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">shape</span>

  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">slice_begin</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="RepeatDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RepeatDim">[docs]</a><span class="k">def</span> <span class="nf">RepeatDim</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">multiple</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copies elements in tensor&#39;s axis &quot;multiple&quot; times, like np.repeat.&quot;&quot;&quot;</span>
  <span class="c1"># x = [[1, 2, 3], [4, 5, 6]]</span>
  <span class="c1"># RepeatDim(x, multiple=2, axis=1) gives:</span>
  <span class="c1"># [[1, 1, 2, 2, 3, 3]. [4, 4, 5, 5, 6, 6]]</span>
  <span class="c1"># As a comparison tf.tile(x, multiples=[1, 2]) gives:\</span>
  <span class="c1"># [[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]</span>

  <span class="k">if</span> <span class="n">multiple</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="n">t_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">tensor_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">t_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiple</span><span class="p">],</span> <span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">multiple_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">multiple</span><span class="p">],</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
  <span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">multiple_dims</span><span class="p">),</span> <span class="n">tensor_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="StackTensorsRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.StackTensorsRecursively">[docs]</a><span class="k">def</span> <span class="nf">StackTensorsRecursively</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively stacks Tensors in a list of `.NestedMap`.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a list of `.NestedMap` or Tensors to stacks.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` with stacked values or a stacked Tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">flatten</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
  <span class="n">stacked</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">stacked</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">flatten</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">))])]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">stacked</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="MixByWeight"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MixByWeight">[docs]</a><span class="k">def</span> <span class="nf">MixByWeight</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a weighted random choice and bprop type from the give inputs.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a list of callables, where each callable returns a tf.Tensor or a</span>
<span class="sd">      nested structure containing tf.Tensor. Function return types must be</span>
<span class="sd">      consistent across elements. The tf.Operation to compute the result tensor</span>
<span class="sd">      will only be invoked for one input at a time. For example, if each fn</span>
<span class="sd">      represents an input record stream, a record will be drawn only from a</span>
<span class="sd">      selected stream while the other streams will remain unchanged.</span>
<span class="sd">    weights: a 1D tensor of float &gt; 0 of the same length as inputs.</span>
<span class="sd">    seed: random seed.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A probablistic sample from the inputs proportional to the weights. The</span>
<span class="sd">    return type will be the same as return type of individual &#39;fn&#39; from the</span>
<span class="sd">    inputs.</span>
<span class="sd">    A one-hot vector of the source selected.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]),</span>
      <span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">weights</span><span class="p">)</span>

  <span class="n">lower</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">upper</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">upper</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">return_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">selected_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="k">lambda</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="p">)</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">bprop_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">selected_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">return_input</span><span class="p">,</span> <span class="n">bprop_index</span></div>


<div class="viewcode-block" id="CheckShapes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CheckShapes">[docs]</a><span class="k">def</span> <span class="nf">CheckShapes</span><span class="p">(</span><span class="n">shapes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Asserts that shapes is a tuple of NestedMap or tshape.Shape.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                 <span class="p">]),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> contains non-tensor value.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">s</span><span class="p">)</span></div>


<div class="viewcode-block" id="FPropDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FPropDtype">[docs]</a><span class="k">def</span> <span class="nf">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">params</span><span class="o">.</span><span class="n">dtype</span></div>


<div class="viewcode-block" id="UpdateFpropDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateFpropDtype">[docs]</a><span class="k">def</span> <span class="nf">UpdateFpropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the fprop_dtype of the Params.&quot;&quot;&quot;</span>
  <span class="c1"># Handle the case when the input &quot;params&quot; is not an instance of hyperparams</span>
  <span class="c1"># For example, when UpdateDtype is called recursively for all the items in</span>
  <span class="c1"># the &quot;sub&quot; list of SequentialLayer (see 1st elif below)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
    <span class="k">return</span>

  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateFpropDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">val</span><span class="p">:</span>
        <span class="n">UpdateFpropDtype</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;fprop_dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span></div>


<div class="viewcode-block" id="UpdateDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateDtype">[docs]</a><span class="k">def</span> <span class="nf">UpdateDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the dtype of the Params.&quot;&quot;&quot;</span>
  <span class="c1"># Handle the case when the input &quot;params&quot; is not an instance of hyperparams</span>
  <span class="c1"># For example, when UpdateDtype is called recursively for all the items in</span>
  <span class="c1"># the &quot;sub&quot; list of SequentialLayer (see 1st elif below)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
    <span class="k">return</span>

  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">val</span><span class="p">:</span>
        <span class="n">UpdateDtype</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span></div>


<div class="viewcode-block" id="NameScopeDecorator"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NameScopeDecorator">[docs]</a><span class="k">def</span> <span class="nf">NameScopeDecorator</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Decorates a python function to introduce a tf.name_scope.</span>

<span class="sd">  Example::</span>

<span class="sd">      @py_utils.NameScopeDecorator(&#39;foobar&#39;)</span>
<span class="sd">      def MyFoobarMethod(self):</span>
<span class="sd">        # ... Do TF things</span>

<span class="sd">  Args:</span>
<span class="sd">    name_scope: The name scope to introduce.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function decorator.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Decorator</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">Wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Wrapped</span>

  <span class="k">return</span> <span class="n">Decorator</span></div>


<div class="viewcode-block" id="SequencesToDebugStrings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SequencesToDebugStrings">[docs]</a><span class="k">def</span> <span class="nf">SequencesToDebugStrings</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns debug strings for the given sequences.</span>

<span class="sd">  Args:</span>
<span class="sd">    ids: int32 of [batch, len].</span>
<span class="sd">    lens: int32 of [batch].</span>
<span class="sd">    summarize: number of ids to summarize per sequence.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A string tensor of [batch].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_seqs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">lens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_Body</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">lens</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">result</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">i0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">result0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">strs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_seqs</span><span class="p">,</span>
      <span class="n">_Body</span><span class="p">,</span> <span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">result0</span><span class="p">),</span>
      <span class="n">shape_invariants</span><span class="o">=</span><span class="p">(</span><span class="n">i0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">])))</span>
  <span class="k">return</span> <span class="n">strs</span></div>


<span class="c1"># TODO(jamesqin): follow suggestions in</span>
<span class="c1"># b/167460492#comment16</span>
<div class="viewcode-block" id="RematerializeFn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RematerializeFn">[docs]</a><span class="k">def</span> <span class="nf">RematerializeFn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">xs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calls fn and rematerializes fn in the backward pass.</span>

<span class="sd">  `fn(*xs) -&gt; ys`, where xs and ys can be a single tensor or a tuple of tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: A python function to be rematerialized in the backprop pass.</span>
<span class="sd">    *xs: A single tensor or a list/tuple of tensors. `xs` are input args to the</span>
<span class="sd">      fn function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `fn(*xs)`</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">initial_step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="n">final_step_seed</span> <span class="o">=</span> <span class="n">MaybeGenerateSeedFromScope</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="n">fwd_xs</span><span class="p">,</span> <span class="n">fwd_ys</span><span class="p">,</span> <span class="n">d_fwd_ys</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The backward function that rematerializes forward outputs.&quot;&quot;&quot;</span>
    <span class="k">del</span> <span class="n">fwd_ys</span>
    <span class="n">always_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([])</span> <span class="o">&lt;</span> <span class="mf">2.0</span>
    <span class="c1"># Alternatively, can do this:</span>
    <span class="c1"># tf.where(tf.math.is_nan(x),</span>
    <span class="c1">#          tf.constant(float(&#39;nan&#39;), dtype=x.dtype) * tf.ones_like(x),</span>
    <span class="c1">#          x)</span>
    <span class="n">bak_xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">always_true</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fwd_xs</span><span class="o">.</span><span class="n">xs</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="n">MaybeResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
    <span class="n">dxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="n">d_fwd_ys</span><span class="p">)</span>
    <span class="n">dxs_final</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dxs</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">dx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dxs_final</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">NestedMap</span><span class="p">(</span>
        <span class="n">initial_step_seed</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">),</span> <span class="n">xs</span><span class="o">=</span><span class="n">dxs_final</span><span class="p">)</span>

  <span class="n">ys_shapes</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># TODO(huangyp, yonghui): Check Forward doesn&#39;t use any stateful random ops.</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="n">fwd_xs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward function plus sanity checks.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fwd_xs</span><span class="o">.</span><span class="n">xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">fwd_xs</span><span class="o">.</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">fwd_xs</span><span class="o">.</span><span class="n">xs</span><span class="p">)</span>
    <span class="c1"># Some sanity check.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">GetExtraInputs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">GetExtraArgs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">GetExtraVars</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
      <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>

  <span class="n">ys</span> <span class="o">=</span> <span class="n">CallDefun</span><span class="p">(</span>
      <span class="n">Forward</span><span class="p">,</span>
      <span class="n">NestedMap</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="o">=</span><span class="n">initial_step_seed</span><span class="p">,</span> <span class="n">xs</span><span class="o">=</span><span class="n">xs</span><span class="p">),</span>
      <span class="n">bak</span><span class="o">=</span><span class="n">Backward</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">ys_shapes</span><span class="p">):</span>
      <span class="n">y</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">ys_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="c1"># TODO(b/129159299): The ResetStepSeed below is needed to work around this</span>
  <span class="c1"># bug, which is a problem with global tensors being shared by different</span>
  <span class="c1"># inference graphs. It should be replaced with the new step seed value</span>
  <span class="c1"># returned from the Forward function when the bug is fixed.</span>
  <span class="n">MaybeResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ys</span></div>


<span class="c1"># A set of names of stateful random number generator ops.</span>
<span class="c1"># See tensorflow/core/ops/random_ops.cc</span>
<span class="n">_STATEFUL_RANDOM_OPS</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">({</span>
    <span class="c1"># pyformat: disable</span>
    <span class="s1">&#39;RandomUniform&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomUniformInt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomStandardNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ParameterizedTruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomShuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Multinomial&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomGamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoisson&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoissonV2&#39;</span><span class="p">,</span>
    <span class="c1"># pyformat: enable</span>
<span class="p">})</span>


<div class="viewcode-block" id="StatefulRandomOpsInDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.StatefulRandomOpsInDefun">[docs]</a><span class="k">def</span> <span class="nf">StatefulRandomOpsInDefun</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Checks whether the Defun depends on stateful random number ops.</span>

<span class="sd">  Stateful random number generator ops should be avoid in Recurrent() call.</span>
<span class="sd">  Otherwise, these ops produce inconsistent values between FProp and BProp.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: a _DefinedFunction or ConcreteFunction to check.</span>
<span class="sd">    graph: a Graph. Set None to use the default graph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of names of the stateful random ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if the input func/graph is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="n">func</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
  <span class="n">graph_def</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>

  <span class="c1"># A dict from function name to FunctionDef.</span>
  <span class="n">func_defs</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">}</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_DefinedFunction</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Defun </span><span class="si">{}</span><span class="s1"> is not in the graph .&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">function_def</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

  <span class="n">stateful_ops</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Recursively search for stateful random op.</span>
  <span class="k">while</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">),</span> <span class="n">node</span>

    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">_STATEFUL_RANDOM_OPS</span><span class="p">:</span>
      <span class="n">stateful_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">continue</span>

    <span class="k">def</span> <span class="nf">_AddDefunNodes</span><span class="p">(</span><span class="n">func_name</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;If the given func_name is a Defun, add its sub-nodes into nodes.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
        <span class="n">nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">func_defs</span><span class="p">[</span><span class="n">func_name</span><span class="p">]</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

    <span class="c1"># For functional.{While|For|If} ops, add their Defun attr into search.</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;While&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;cond&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;For&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;If&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;then_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;else_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;StatefulPartitionedCall&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="s1">&#39;PartitionedCall&#39;</span><span class="p">:</span>
      <span class="c1"># For other op, check whether itself is a Defun op.</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">stateful_ops</span></div>


<div class="viewcode-block" id="ToPlaceholders"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToPlaceholders">[docs]</a><span class="k">def</span> <span class="nf">ToPlaceholders</span><span class="p">(</span><span class="n">nmap</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts every Tensor in nmap to a placeholder.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_ToPlacerholder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">_ToPlacerholder</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Softmax">[docs]</a><span class="k">def</span> <span class="nf">Softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">extra_logit</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Softmax with extra_logits, might be useful for large xformer LM.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">extra_logit</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis</span>

  <span class="k">def</span> <span class="nf">ReduceLogSumExp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">base_logit</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_logit</span><span class="p">,</span> <span class="n">extra_logit</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">base_logit</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">sum_exp_x</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">extra_logit</span> <span class="o">-</span> <span class="n">base_logit</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sum_exp_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">base_logit</span>

  <span class="k">def</span> <span class="nf">LogSoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">ReduceLogSumExp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyFocalLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SoftmaxCrossEntropyFocalLoss">[docs]</a><span class="k">def</span> <span class="nf">SoftmaxCrossEntropyFocalLoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span>
                                 <span class="n">label_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">label_probs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">stop_gradient_on_focal_loss_coefficient</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sa">u</span><span class="sd">&quot;&quot;&quot;Focal loss for multinomial (softmax) logistic loss.</span>

<span class="sd">  [1] Focal loss https://arxiv.org/abs/1708.02002</span>

<span class="sd">  Args:</span>
<span class="sd">    logits: [..., C]. Logits for the multinomial logistic regression. C is the</span>
<span class="sd">      number of classes.</span>
<span class="sd">    label_ids: [...]. Each entry in labels must be an index in [0, C).</span>
<span class="sd">    label_probs: [..., C]. Each vector along last dimension must be a valid</span>
<span class="sd">      probability distribution.</span>
<span class="sd">    alpha: [C]. The weighting factor alpha. Eq (3) in [1].</span>
<span class="sd">    gamma: []. Tunable focusing parameter. Eq (4) in [1].</span>
<span class="sd">    stop_gradient_on_focal_loss_coefficient: If true, stops gradient on the</span>
<span class="sd">      focal loss coefficient (1-p)^gamma to stabilize the gradient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    loss[i..., j] = FL(pₜ) = - αₜ(1-pₜ)ˠlog(pₜ) Eq (5) in [1].</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_ApplyFocalLossCoefficient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
      <span class="n">coefficient</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">probs</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">stop_gradient_on_focal_loss_coefficient</span><span class="p">:</span>
        <span class="n">coefficient</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">coefficient</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">coefficient</span>
    <span class="k">return</span> <span class="n">loss</span>

  <span class="k">if</span> <span class="n">label_probs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">label_probs</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_ApplyFocalLossCoefficient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">alpha</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">label_ids</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_ApplyFocalLossCoefficient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="o">-</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyFocalLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SigmoidCrossEntropyFocalLoss">[docs]</a><span class="k">def</span> <span class="nf">SigmoidCrossEntropyFocalLoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">u</span><span class="sd">&quot;&quot;&quot;Focal loss for binary (sigmoid) logistic loss.</span>

<span class="sd">  [1] Focal loss https://arxiv.org/abs/1708.02002</span>

<span class="sd">  Args:</span>
<span class="sd">    logits: [..., C]. Logits for the sigmoid logistic regression.</span>
<span class="sd">    labels: [..., C]. 0/1 labels.</span>
<span class="sd">    alpha: The weighting factor alpha. Eq (3) in [1].</span>
<span class="sd">    gamma: Tunable focusing parameter. Eq (4) in [1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    loss[i..., j] = FL(pₜ) = - αₜ(1-pₜ)ˠlog(pₜ) Eq (5) in [1].</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># [1] Eq (4).</span>
  <span class="c1">#</span>
  <span class="c1"># The numerically-stable way to compute</span>
  <span class="c1">#  log(p) for positives;</span>
  <span class="c1">#  log(1 - p) for negatives.</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># The modulating factor. Note that</span>
    <span class="c1"># (1 - p)ˠ = [1 - σ(x)]ˠ = [σ(-x)]ˠ, for positives.</span>
    <span class="c1"># pˠ = [σ(x)]ˠ, for negatives.</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">labels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">gamma</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># [1] Eq (3)</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">labels</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">labels</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">loss</span></div>


<span class="n">_RECORD_FORMAT_RE</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;(^[A-Za-z]+):(.*)&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="RecordFormatFromFilePattern"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RecordFormatFromFilePattern">[docs]</a><span class="k">def</span> <span class="nf">RecordFormatFromFilePattern</span><span class="p">(</span><span class="n">file_pattern</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the record format string for a Lingvo file pattern.</span>

<span class="sd">  Lingvo file patterns take the form of:</span>
<span class="sd">    tfrecord:/path/to/bar -&gt; tfrecord is the record_format.</span>

<span class="sd">  This function takes a file pattern and returns a string indicating</span>
<span class="sd">  which format the filepattern implies.</span>

<span class="sd">  Args:</span>
<span class="sd">    file_pattern: String file pattern.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tuple (string, string):</span>

<span class="sd">      - record_format: String record format, e.g., &quot;tfrecord&quot;, etc.</span>
<span class="sd">      - file_pattern: The file pattern without any prefixes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">_RECORD_FORMAT_RE</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># TODO(vrv): Fix all callers so that file_pattern must contain</span>
    <span class="c1"># the record format prefix.</span>
    <span class="k">return</span> <span class="s1">&#39;sstable&#39;</span><span class="p">,</span> <span class="n">file_pattern</span>

  <span class="c1"># regexp ensures that a match implies there are two groups:</span>
  <span class="c1"># the record format and then the file pattern.</span>
  <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span></div>


<div class="viewcode-block" id="ReadFileLines"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadFileLines">[docs]</a><span class="k">def</span> <span class="nf">ReadFileLines</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Read a text file and return the lines.</span>

<span class="sd">  If the file cannot be found at the given path, attempt to load it from the</span>
<span class="sd">  Lingvo package (useful for data dependencies in par files).</span>

<span class="sd">  Args:</span>
<span class="sd">    file_path: path to file, either absolute or relative to the bazel workspace.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of lines from the file.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="n">pkgutil</span><span class="o">.</span><span class="n">get_data</span><span class="p">(</span>
          <span class="s1">&#39;lingvo&#39;</span><span class="p">,</span> <span class="n">file_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;lingvo/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                      <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">IOError</span><span class="p">:</span>
      <span class="c1"># If pkgutil can&#39;t find the file, continue and let GFile raise the error.</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">lines</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">lines</span></div>


<span class="c1"># Partially borrowed from</span>
<span class="c1"># https://github.com/tensorflow/tensor2tensor/blob/32929305e1a4ec926eff24123758b794df35492b/tensor2tensor/layers/common_layers.py#L349</span>
<div class="viewcode-block" id="CumSum"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CumSum">[docs]</a><span class="k">def</span> <span class="nf">CumSum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A TPU efficient implementation of tf.cumsum().</span>

<span class="sd">  This is equivalent to tf.cumsum and is faster on TPU as of 08/2019 unless</span>
<span class="sd">  the axis dimension is very large. The current Tensorflow implementation is</span>
<span class="sd">  based on scanning and reducing which is not efficient on TPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An input Tensor.</span>
<span class="sd">    axis: An int for the axis.</span>
<span class="sd">    exclusive: A bool for performing exclusive cumsum.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor of the same shape as x.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if the input axis is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="c1"># Fallback to tf.cumsum when inputs are not floats or not running on TPU.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">)</span>

  <span class="n">rank</span> <span class="o">=</span> <span class="n">GetRank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="c1"># Needs to know the rank for the final transpose if axis is not the last</span>
  <span class="c1"># dimension. Otherwise, falls back to tf.cumsum.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unexpected axis: </span><span class="si">%d</span><span class="s1"> (rank = </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
    <span class="n">axis</span> <span class="o">+=</span> <span class="n">rank</span>

  <span class="n">length</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="n">axis</span><span class="p">]</span>
  <span class="n">my_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
  <span class="n">comparator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span> <span class="k">if</span> <span class="n">exclusive</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">less_equal</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">comparator</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">my_range</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">my_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
      <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[[</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">result</span><span class="p">,</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="ProjectLastDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ProjectLastDim">[docs]</a><span class="k">def</span> <span class="nf">ProjectLastDim</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Linear projection on the last dim of the input tensor.</span>

<span class="sd">  This is a TPU efficient implementation to avoid reshaping inputs to Rank-2</span>
<span class="sd">  tensor by using Einsum for the compute.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: An input Tensor, the last dimension of which is input_dim.</span>
<span class="sd">    weight: A weight matrix with shape [input_dim, output_dim].</span>
<span class="sd">    input_dim: An integer or a symbolic dim, the last dimension of the inputs.</span>
<span class="sd">    output_dim: An integer or a symbolic dim, the last dimension of the outputs.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An output Tensor of the same rank as inputs, the last dimension is</span>
<span class="sd">    output_dim.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
      <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span> <span class="k">else</span> <span class="n">input_dim</span><span class="p">)</span>
  <span class="n">output_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
      <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">output_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">output_dim</span>
                                                      <span class="p">)</span> <span class="k">else</span> <span class="n">output_dim</span><span class="p">)</span>

  <span class="c1"># Assert input_dim and output_dim</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_dim</span><span class="p">)],</span>
                             <span class="n">inputs</span><span class="p">)</span>
  <span class="n">weight</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">weight</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_dim</span><span class="p">),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">weight</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_dim</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">weight</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">use_tpu</span><span class="p">()</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">):</span>
    <span class="c1"># Avoids reshape if feasible and uses Einsum.</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># This is equivalent to:</span>
      <span class="c1">#   outputs = tf.einsum(&#39;...y,yz-&gt;...z&#39;, inputs, weight)</span>
      <span class="c1"># Unfortunately ... in einsum() leads to extra HBM usage.</span>
      <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">chr</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">97</span><span class="p">,</span> <span class="mi">123</span><span class="p">)])</span>  <span class="c1"># abc...xyz</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{0}</span><span class="s1">y,yz-&gt;</span><span class="si">{0}</span><span class="s1">z&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">ToStaticShape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">])),</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="n">ToStaticShape</span><span class="p">([</span><span class="n">output_dim</span><span class="p">])</span>
        <span class="p">],</span>
                  <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="RemoveAssertContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RemoveAssertContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Hacks to replace certain unwanted tensorflow ops.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(zhifengc/huangyp): Consider implementing assert_equal</span>
  <span class="c1"># op replacement for lingvo. As assert_equal doesn&#39;t support String on GPUs.</span>
  <span class="c1"># Hack to replace tf.assert_equal</span>
  <span class="c1"># TODO(b/136040013): Remove this after migration to tf.function.</span>
  <span class="k">if</span> <span class="n">remove</span><span class="p">:</span>
    <span class="n">saved_assert_equal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span>

    <span class="k">def</span> <span class="nf">NoOP</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>

    <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span> <span class="o">=</span> <span class="n">NoOP</span>  <span class="c1"># Make assert_equal a no op.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span> <span class="o">=</span> <span class="n">saved_assert_equal</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">yield</span></div>


<div class="viewcode-block" id="_AssertInputsMatch"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._AssertInputsMatch">[docs]</a><span class="k">def</span> <span class="nf">_AssertInputsMatch</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">implicit_captures</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Assert that op&#39;s inputs match with args and implicit_captures.</span>

<span class="sd">  Args:</span>
<span class="sd">    op: The operation to check.</span>
<span class="sd">    args: A nested structure representing the explicit arguments of &#39;op&#39;.</span>
<span class="sd">    implicit_captures: A nested structure representing the implicitly captured</span>
<span class="sd">      inputs of &#39;op&#39;.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if the number of inputs mismatch.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">expected_inputs</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">([</span><span class="n">args</span><span class="p">,</span> <span class="n">implicit_captures</span><span class="p">])</span>
  <span class="n">expected_num_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">expected_num_inputs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s1">&#39;Too many inputs. The most likely cause is that fwd &#39;</span>
                      <span class="s1">&#39;captures additional tensors: extra inputs </span><span class="si">%r</span><span class="s1"> vs </span><span class="si">%r</span><span class="s1"> &#39;</span>
                      <span class="s1">&#39;captures=</span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">expected_inputs</span><span class="p">),</span>
                                        <span class="nb">list</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">implicit_captures</span><span class="p">))))</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">expected_num_inputs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s1">&#39;Mismatched inputs to fwd: Found </span><span class="si">%d</span><span class="s1"> vs expected </span><span class="si">%d</span><span class="s1">: </span><span class="si">%r</span><span class="s1">&#39;</span>
                      <span class="s1">&#39;. Implicit captures(</span><span class="si">%d</span><span class="s1">) = </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span>
                     <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span> <span class="n">expected_num_inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span>
                      <span class="nb">len</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">implicit_captures</span><span class="p">)),</span> <span class="n">implicit_captures</span><span class="p">))</span></div>


<div class="viewcode-block" id="TensorSpecs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TensorSpecs">[docs]</a><span class="k">def</span> <span class="nf">TensorSpecs</span><span class="p">(</span><span class="n">nmap</span><span class="p">,</span> <span class="n">keep_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transforms tensors in the input nested structure to TensorSpecs.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">nmap</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="n">fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">keep_shape</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Transform</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">nmap</span><span class="p">)</span></div>


<div class="viewcode-block" id="_DefineDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DefineDefun">[docs]</a><span class="k">def</span> <span class="nf">_DefineDefun</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">,</span> <span class="n">bak</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps fwd in a defun with custom gradient bak.</span>

<span class="sd">  Args:</span>
<span class="sd">    fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure.</span>
<span class="sd">    fwd_sig: A Nested Structure of tf.TensorSpec representing the input</span>
<span class="sd">      signature of `fwd`, or None (meaning that fwd takes no inputs).</span>
<span class="sd">    bak: A callable xs, ys, dys: Nested Structure -&gt; dxs[, dcapture]: Nested</span>
<span class="sd">      Structure. The custom backprop function for `fwd`. bak needs to return</span>
<span class="sd">      dcapture if fwd uses any implicitly captured tensors, whose gradients are</span>
<span class="sd">      dcapture.</span>
<span class="sd">    bak_as_function: Whether to create a TF graph function for `bak`.</span>
<span class="sd">    device: the device on which to run `fwd` and `bak`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A NestedMap containing:</span>

<span class="sd">    - call: A callable that will execute `fwd`. It has the same input and output</span>
<span class="sd">      signatures as `fwd`.</span>
<span class="sd">    - func: The underlying TF function that `call` calls. If not None, it will</span>
<span class="sd">      be a _DefinedFunction or ConcreteFunction that takes flat inputs and</span>
<span class="sd">      returns flat outputs, and can be used by routines that require a TF</span>
<span class="sd">      function object (e.g. tf.If, tf.While, etc).</span>
<span class="sd">      Always not None when `bak` is None.</span>
<span class="sd">    - output_dtypes: A nested structure compatible with the outputs of `fwd`</span>
<span class="sd">      containing the corresponding output dtypes.</span>
<span class="sd">    - stateful_ops: A list of (op_name, op_type) tuples representing the</span>
<span class="sd">      stateful ops used by `fwd`.</span>
<span class="sd">    - captured_inputs: Implicit inputs captured by `fwd`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">fwd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">noinline</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">use_xla</span><span class="p">()</span>

  <span class="k">if</span> <span class="n">fwd_sig</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fwd_sig</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">get_dtype</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
  <span class="n">arg_dtypes</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">Transform</span><span class="p">(</span><span class="n">get_dtype</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">))</span>
  <span class="n">get_shape</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">arg_shapes</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">Transform</span><span class="p">(</span><span class="n">get_shape</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">))</span>

  <span class="c1"># Used to hold the backward function used by Grad, which will be defined if</span>
  <span class="c1"># bak is set.</span>
  <span class="n">sigs</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
  <span class="c1"># Output of this method.</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>

  <span class="n">python_grad_func</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">bak</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">Grad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Gradient function for the forward function.</span>

<span class="sd">      Args:</span>
<span class="sd">        op: The forward operation.</span>
<span class="sd">        *args: Gradients wrt op.outputs.</span>

<span class="sd">      Returns:</span>
<span class="sd">        Tuple of derivatives.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">_AssertInputsMatch</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">)</span>
      <span class="c1"># Ensure dys contains no None.</span>
      <span class="n">args</span> <span class="o">=</span> <span class="n">ConvertNoneGradientToZeros</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
      <span class="n">xs</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">arg_dtypes</span><span class="p">)]</span>  <span class="c1"># The rest are captures.</span>
      <span class="k">return</span> <span class="n">sigs</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">([</span><span class="n">xs</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">args</span><span class="p">]))</span>

    <span class="n">python_grad_func</span> <span class="o">=</span> <span class="n">Grad</span>

  <span class="k">def</span> <span class="nf">_SetShape</span><span class="p">(</span><span class="n">dst_list</span><span class="p">,</span> <span class="n">shape_list</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dst_list</span><span class="p">,</span> <span class="n">shape_list</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="n">arg_dtypes</span><span class="p">,</span> <span class="n">python_grad_func</span><span class="o">=</span><span class="n">python_grad_func</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="n">noinline</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The forward function.&quot;&quot;&quot;</span>
    <span class="n">_SetShape</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">arg_shapes</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">):</span>
      <span class="n">call</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">fwd</span><span class="p">(</span><span class="n">Pack</span><span class="p">(</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span> <span class="k">if</span> <span class="n">args</span> <span class="k">else</span> <span class="n">fwd</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Defun will handle the device assignment.</span>
        <span class="n">rets</span> <span class="o">=</span> <span class="n">call</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
          <span class="n">rets</span> <span class="o">=</span> <span class="n">call</span><span class="p">()</span>
    <span class="n">res</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">rets</span>
    <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">rets</span><span class="p">)</span>

  <span class="n">forward</span> <span class="o">=</span> <span class="n">Forward</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">arg_dtypes</span><span class="p">:</span>
    <span class="c1"># In this case Forward is an _OverloadedFunction, we need to instantiate it.</span>
    <span class="n">forward</span> <span class="o">=</span> <span class="n">Forward</span><span class="o">.</span><span class="n">instantiate</span><span class="p">([])</span>

  <span class="c1"># Invokes fwd() to get res.outputs.</span>
  <span class="n">forward</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
  <span class="n">res</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">forward</span>
  <span class="n">res</span><span class="o">.</span><span class="n">stateful_ops</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">stateful_ops</span>
  <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span> <span class="o">=</span> <span class="n">forward</span><span class="o">.</span><span class="n">captured_inputs</span>
  <span class="n">output_dtypes</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">get_dtype</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
  <span class="n">output_shapes</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">get_shape</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Call</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper of fwd.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="n">forward</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">flat_rets</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="p">[</span><span class="n">flat_rets</span><span class="p">]</span>
    <span class="n">_SetShape</span><span class="p">(</span><span class="n">flat_rets</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">output_shapes</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Pack</span><span class="p">(</span><span class="n">output_dtypes</span><span class="p">,</span> <span class="n">flat_rets</span><span class="p">)</span>

  <span class="n">res</span><span class="o">.</span><span class="n">call</span> <span class="o">=</span> <span class="n">Call</span>

  <span class="k">if</span> <span class="n">bak</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;The backward function.&quot;&quot;&quot;</span>
      <span class="n">_SetShape</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">([</span><span class="n">arg_shapes</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">]))</span>
      <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">([</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">output_dtypes</span><span class="p">,</span> <span class="n">output_dtypes</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="c1"># Defun will handle the device assignment.</span>
          <span class="n">dxs</span> <span class="o">=</span> <span class="n">bak</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
            <span class="n">dxs</span> <span class="o">=</span> <span class="n">bak</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">dxs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bak_as_function</span><span class="p">:</span>
      <span class="n">sigs</span><span class="o">.</span><span class="n">backward</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span>
          <span class="o">*</span><span class="n">Flatten</span><span class="p">([</span><span class="n">arg_dtypes</span><span class="p">,</span> <span class="n">output_dtypes</span><span class="p">,</span> <span class="n">output_dtypes</span><span class="p">]),</span>
          <span class="n">noinline</span><span class="o">=</span><span class="n">noinline</span><span class="p">)(</span>
              <span class="n">Backward</span><span class="p">)</span>

      <span class="n">sigs</span><span class="o">.</span><span class="n">backward</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">sigs</span><span class="o">.</span><span class="n">backward</span> <span class="o">=</span> <span class="n">Backward</span>

  <span class="k">return</span> <span class="n">res</span></div>


<span class="c1"># Global variable to control rendezvous sharing in tf.function.</span>
<span class="c1"># If False (default) rendezvous sharing is disabled in tf.function, that is, the</span>
<span class="c1"># function body use a separate rendezvous and can&#39;t communicate with parent</span>
<span class="c1"># graph via send/recv.</span>
<span class="c1"># With _GetSharedRendezvous() == True, the function body share the same</span>
<span class="c1"># rendezvous with the parent graph and can talk to it using send/recv. This is</span>
<span class="c1"># useful for layers like StackedRecurrent.</span>
<span class="n">_SHARED_RENDEZVOUS</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>


<div class="viewcode-block" id="_SharedRendezvousScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._SharedRendezvousScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_SharedRendezvousScope</span><span class="p">(</span><span class="n">shared_rendezvous</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="n">_SHARED_RENDEZVOUS</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shared_rendezvous</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_SHARED_RENDEZVOUS</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="_GetSharedRendezvous"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetSharedRendezvous">[docs]</a><span class="k">def</span> <span class="nf">_GetSharedRendezvous</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the current rendezvous sharing setting.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_SHARED_RENDEZVOUS</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">_SHARED_RENDEZVOUS</span><span class="o">.</span><span class="n">stack</span> <span class="k">else</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="_ApplySharedRendezvous"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._ApplySharedRendezvous">[docs]</a><span class="k">def</span> <span class="nf">_ApplySharedRendezvous</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply the rendezvous sharing setting on the given tf.function func.&quot;&quot;&quot;</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">func</span><span class="o">.</span><span class="n">_shared_rendezvous</span> <span class="o">=</span> <span class="n">_GetSharedRendezvous</span><span class="p">()</span></div>
  <span class="c1"># pylint: enable=protected-access</span>


<div class="viewcode-block" id="_WrapFunction"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._WrapFunction">[docs]</a><span class="k">def</span> <span class="nf">_WrapFunction</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps func as a tf.function.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">input_signature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">input_signature</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">Decorated</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="n">input_signature</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">Fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
      <span class="c1"># TODO(b/163904067): mimic Defun&#39; behavior and reset the step seed to</span>
      <span class="c1"># avoid it being used as an implicit capture. This is not a desired</span>
      <span class="c1"># behavior, it should take the step seed from parent graph instead.</span>
      <span class="n">ResetStepSeed</span><span class="p">()</span>

      <span class="c1"># Mimic Defun and disable collection sharing.</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
      <span class="c1"># Don&#39;t share summaries collection with parent graph (b/168745134).</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">clear_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">SUMMARIES</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">_ApplySharedRendezvous</span><span class="p">(</span><span class="n">Fn</span><span class="p">)</span>

    <span class="c1"># Add the function to the graph so it&#39;ll be traced under the current</span>
    <span class="c1"># context. This is necessary if the function body captures any non-tensor</span>
    <span class="c1"># values from the environment, like symbolic maps.</span>
    <span class="n">cf</span> <span class="o">=</span> <span class="n">Fn</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>
    <span class="n">cf</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">cf</span>

  <span class="c1"># For the `foo = _WrapFunction(foo, ...)` use case.</span>
  <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">Decorated</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

  <span class="c1"># For the `@_WrapFunction(...)` use case.</span>
  <span class="k">return</span> <span class="n">Decorated</span></div>


<div class="viewcode-block" id="_DefineFunction"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DefineFunction">[docs]</a><span class="k">def</span> <span class="nf">_DefineFunction</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">,</span> <span class="n">bak</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps fwd in a defun with custom gradient bak.</span>

<span class="sd">  Args:</span>
<span class="sd">    fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure.</span>
<span class="sd">    fwd_sig: A Nested Structure of tf.TensorSpec representing the input</span>
<span class="sd">      signature of `fwd`, or None (meaning that fwd takes no inputs).</span>
<span class="sd">    bak: A callable xs, ys, dys: Nested Structure -&gt; dxs[, dcapture]: Nested</span>
<span class="sd">      Structure. The custom backprop function for `fwd`. bak needs to return</span>
<span class="sd">      dcapture if fwd uses any implicitly captured tensors, whose gradients are</span>
<span class="sd">      dcapture.</span>
<span class="sd">    bak_as_function: Whether to create a TF graph function for `bak`.</span>
<span class="sd">    device: the device on which to run `fwd` and `bak`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A NestedMap containing:</span>

<span class="sd">    - call: A callable that will execute `fwd`. It has the same input and output</span>
<span class="sd">      signatures as `fwd`.</span>
<span class="sd">    - func: The underlying TF function that `call` calls. If not None, it will</span>
<span class="sd">      be a _DefinedFunction or ConcreteFunction that takes flat inputs and</span>
<span class="sd">      returns flat outputs, and can be used by routines that require a TF</span>
<span class="sd">      function object (e.g. tf.If, tf.While, etc).</span>
<span class="sd">      Always not None when `bak` is None.</span>
<span class="sd">    - outputs: The outputs of `fwd`. Used for reflection only (e.g. to get the</span>
<span class="sd">      output dtypes, shapes, etc).</span>
<span class="sd">    - stateful_ops: A list of (op_name, op_type) tuples representing the</span>
<span class="sd">      stateful ops used by `fwd`.</span>
<span class="sd">    - captured_inputs: Implicit inputs captured by `fwd`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">fwd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">noinline</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">use_xla</span><span class="p">()</span>

  <span class="k">if</span> <span class="n">fwd_sig</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fwd_sig</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Get the current device to mimic Defun&#39;s behavior.</span>
    <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">device_funcs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_device_functions_outer_to_inner</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">device_funcs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">device_funcs</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="c1"># pylint: enable=protected-access</span>

  <span class="c1"># Output of this method.</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>

  <span class="nd">@_WrapFunction</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="n">Flatten</span><span class="p">(</span><span class="n">fwd_sig</span><span class="p">))</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The forward function.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">(</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="n">rets</span> <span class="o">=</span> <span class="n">fwd</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">rets</span> <span class="o">=</span> <span class="n">fwd</span><span class="p">()</span>
    <span class="n">res</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">rets</span>
    <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">rets</span><span class="p">)</span>

  <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span> <span class="o">=</span> <span class="n">Forward</span><span class="o">.</span><span class="n">captured_inputs</span>

  <span class="c1"># Get the stateful ops used in cell_fn. Logic borrowed from</span>
  <span class="c1"># _EagerDefinedFunction.__init__().</span>
  <span class="n">graph</span> <span class="o">=</span> <span class="n">Forward</span><span class="o">.</span><span class="n">graph</span>
  <span class="n">input_ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">operations</span> <span class="o">=</span> <span class="p">[</span><span class="n">op</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_operations</span><span class="p">()</span> <span class="k">if</span> <span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_ops</span><span class="p">]</span>
  <span class="n">res</span><span class="o">.</span><span class="n">stateful_ops</span> <span class="o">=</span> <span class="p">[(</span><span class="n">o</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">o</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">operations</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">Call</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper of fwd.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="n">func</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">flat_rets</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
      <span class="n">flat_rets</span> <span class="o">=</span> <span class="p">[</span><span class="n">flat_rets</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Pack</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">flat_rets</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">bak</span><span class="p">:</span>
    <span class="n">res</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">Forward</span>
    <span class="n">res</span><span class="o">.</span><span class="n">call</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="n">Call</span><span class="p">(</span><span class="n">Forward</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

  <span class="n">shared_rendezvous</span> <span class="o">=</span> <span class="n">_GetSharedRendezvous</span><span class="p">()</span>
  <span class="n">ret_specs</span> <span class="o">=</span> <span class="n">TensorSpecs</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">([</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">ret_specs</span><span class="p">,</span> <span class="n">ret_specs</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
      <span class="n">dxs</span> <span class="o">=</span> <span class="n">bak</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">dxs</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">bak_as_function</span><span class="p">:</span>
    <span class="n">backward_cf</span> <span class="o">=</span> <span class="n">_WrapFunction</span><span class="p">(</span>
        <span class="n">Backward</span><span class="p">,</span> <span class="n">input_signature</span><span class="o">=</span><span class="n">Flatten</span><span class="p">([</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">ret_specs</span><span class="p">,</span> <span class="n">ret_specs</span><span class="p">]))</span>
  <span class="k">else</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">BackwardWithSharedRendezvous</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">_SharedRendezvousScope</span><span class="p">(</span><span class="n">shared_rendezvous</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">backward_cf</span> <span class="o">=</span> <span class="n">BackwardWithSharedRendezvous</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">custom_gradient</span>
  <span class="k">def</span> <span class="nf">ForwardWithGrad</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward function and its custom gradient.&quot;&quot;&quot;</span>
    <span class="c1"># Note that `args` includes implicit captures. This is required by</span>
    <span class="c1"># tf.custom_gradient so that when the Grad() outputs include gradients to</span>
    <span class="c1"># implicit captures, they match the inputs to ForwardWithGrad().</span>
    <span class="c1">#</span>
    <span class="c1"># However, Forward doesn&#39;t take implicit captures as input, so we exclude</span>
    <span class="c1"># them here.</span>
    <span class="n">fwd_args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[:(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">)))]</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">Forward</span><span class="p">(</span><span class="o">*</span><span class="n">fwd_args</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">Grad</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Gradient function for the forward function.</span>

<span class="sd">      Args:</span>
<span class="sd">        *args: Gradients wrt op.outputs.</span>
<span class="sd">        **kwargs: Additional arguments from tf.custom_gradient.</span>

<span class="sd">      Returns:</span>
<span class="sd">        Tuple of derivatives.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s1">&#39;Ignoring additional arguments used by tf.custom_gradient: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">kwargs</span><span class="p">))</span>

      <span class="n">_AssertInputsMatch</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">fwd_sig</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">)</span>

      <span class="c1"># Ensure dys contains no None.</span>
      <span class="n">args</span> <span class="o">=</span> <span class="n">ConvertNoneGradientToZeros</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

      <span class="n">xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">([</span><span class="n">fwd_sig</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">],</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">backward_cf</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">([</span><span class="n">xs</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">args</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">Grad</span>

  <span class="n">res</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">forward</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">xs</span><span class="p">:</span> <span class="n">ForwardWithGrad</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">([</span><span class="n">xs</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">]))</span>
  <span class="n">res</span><span class="o">.</span><span class="n">call</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="n">Call</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">res</span></div>


<span class="c1"># Global variable to control whether to use tf.function.</span>
<span class="c1"># If not set, the result is determined by tf2 status. See _UseTfFunction for</span>
<span class="c1"># details.</span>
<span class="c1"># TODO(laigd): remove after b/169869929 is fixed.</span>
<span class="n">_USE_TF_FUNCTION</span> <span class="o">=</span> <span class="n">ThreadLocalStack</span><span class="p">()</span>

<span class="c1"># Constants for propagating framework tensors through Function.</span>
<span class="n">_FRAMEWORK_TENSOR_GLOBAL_STEP</span> <span class="o">=</span> <span class="s1">&#39;_global_step&#39;</span>


<div class="viewcode-block" id="TfFunctionScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TfFunctionScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">TfFunctionScope</span><span class="p">(</span><span class="n">use_tf_function</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="n">_USE_TF_FUNCTION</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">use_tf_function</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_USE_TF_FUNCTION</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="_UseTfFunction"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._UseTfFunction">[docs]</a><span class="k">def</span> <span class="nf">_UseTfFunction</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Whether to use tf.function instead of tf.Defun.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_USE_TF_FUNCTION</span><span class="o">.</span><span class="n">stack</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_USE_TF_FUNCTION</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">tf2_enabled</span><span class="p">()</span></div>


<div class="viewcode-block" id="Function"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Function">[docs]</a><span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Function builds a TensorFlow graph function from a callable.</span>

<span class="sd">  In the high level this is similar to tf.Defun and tf.function. In fact this</span>
<span class="sd">  relies on those as underlying implementations, but with specific configuration</span>
<span class="sd">  so it&#39;s easier to use and can work well in some extreme cases in Lingvo.</span>

<span class="sd">  Example usage:</span>

<span class="sd">  - No inputs:</span>

<span class="sd">    &gt;&gt;&gt; @Function()</span>
<span class="sd">    ... def foo():</span>
<span class="sd">    ...   return tf.constant(1.0)</span>
<span class="sd">    &gt;&gt;&gt; y = foo()</span>

<span class="sd">  - Scalar input:</span>

<span class="sd">    &gt;&gt;&gt; @Function(fwd_sig=tf.TensorSpec(None, tf.float32))</span>
<span class="sd">    ... def foo(x):</span>
<span class="sd">    ...   return x * 2</span>
<span class="sd">    &gt;&gt;&gt; y = foo(1.0)</span>

<span class="sd">  - List input:</span>

<span class="sd">    &gt;&gt;&gt; @Function(fwd_sig=[tf.TensorSpec(None, tf.float32) for _ in range(2)])</span>
<span class="sd">    ... def foo(xs):</span>
<span class="sd">    ...   return xs[0] + xs[1]</span>
<span class="sd">    &gt;&gt;&gt; y = foo([1.0, 2.0])</span>

<span class="sd">  - Nested input:</span>

<span class="sd">    &gt;&gt;&gt; @Function(fwd_sig=NestedMap(x=tf.TensorSpec(None, tf.float32)))</span>
<span class="sd">    ... def foo(nmap):</span>
<span class="sd">    ...   return nmap.x * 2</span>
<span class="sd">    &gt;&gt;&gt; y = foo(NestedMap(x=1.0))</span>

<span class="sd">  - With custom gradient function (other input types mentioned above are also</span>
<span class="sd">    supported):</span>

<span class="sd">    &gt;&gt;&gt; def bar(x, y, dy):</span>
<span class="sd">    ...   del y, dy</span>
<span class="sd">    ...   return 4.0 * x * dy</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @Function(fwd_sig=tf.TensorSpec(None, tf.float32), bak=bar)</span>
<span class="sd">    ... def foo(x):</span>
<span class="sd">    ...   return 2.0 * x * x</span>

<span class="sd">  - Used in control flow ops:</span>

<span class="sd">    &gt;&gt;&gt; then_branch = Function(tf.TensorSpec([], tf.int32))(lambda x: x / 2)</span>
<span class="sd">    &gt;&gt;&gt; else_branch = Function(tf.TensorSpec([], tf.int32))(lambda x: 3 * x + 1)</span>
<span class="sd">    &gt;&gt;&gt; y = tf.If(cond, inputs, then_branch.func, else_branch.func)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># TODO(laigd): the use_tf_function option is added for backward compatibility</span>
  <span class="c1"># reasons. Remove it after the migration.</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">fwd_sig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bak</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_tf_function</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Below we assume `fwd` is the input to `__call__` that is used to build the</span>
<span class="sd">    TensorFlow graph function encapsulated by this object.</span>

<span class="sd">    Args:</span>
<span class="sd">      fwd_sig: A Nested Structure of tf.TensorSpec representing the input</span>
<span class="sd">        signature of `fwd`, or None (meaning that `fwd` takes no inputs). The</span>
<span class="sd">        actual inputs should be compatible with this (have same shapes and</span>
<span class="sd">        dtypes).</span>
<span class="sd">      bak: A callable xs, ys, dys: Nested Structure -&gt; dxs[, dcapture]: Nested</span>
<span class="sd">        Structure. The custom backprop function for `fwd`. bak needs to return</span>
<span class="sd">        dcapture if `fwd` uses any implicitly captured tensors, whose gradients</span>
<span class="sd">        are dcapture.</span>
<span class="sd">      bak_as_function: Whether to create a TF graph function for `bak`.</span>
<span class="sd">      device: The device on which to run `fwd` and `bak`. Defaults to the</span>
<span class="sd">        current device.</span>
<span class="sd">      use_tf_function: Whether use tf.function. Defaults to _UseTfFunction().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_fwd_sig</span> <span class="o">=</span> <span class="n">fwd_sig</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bak</span> <span class="o">=</span> <span class="n">bak</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bak_as_function</span> <span class="o">=</span> <span class="n">bak_as_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_tf_function</span> <span class="o">=</span> <span class="n">use_tf_function</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fwd</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a graph function.</span>

<span class="sd">    Args:</span>
<span class="sd">      fwd: a callable xs: Nested Structure -&gt; ys: Nested Structure.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A DefinedFunction object encapsulating `fwd` as a graph function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="n">fwd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">DefinedFunction</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fwd_sig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bak</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bak_as_function</span><span class="p">,</span>
                           <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_tf_function</span><span class="p">)</span></div>


<div class="viewcode-block" id="DefinedFunction"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefinedFunction">[docs]</a><span class="k">class</span> <span class="nc">DefinedFunction</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Encapsulates a TensorFlow graph function and its properties.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">fwd</span><span class="p">,</span>
               <span class="n">fwd_sig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bak</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_tf_function</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure. Used to</span>
<span class="sd">        build the TensorFlow graph function that this object encapsulates.</span>
<span class="sd">      fwd_sig: A Nested Structure of tf.TensorSpec representing the input</span>
<span class="sd">        signature of `fwd`, or None (meaning that `fwd` takes no inputs). The</span>
<span class="sd">        actual inputs should be compatible with this (have same shapes and</span>
<span class="sd">        dtypes).</span>
<span class="sd">      bak: A callable xs, ys, dys: Nested Structure -&gt; dxs[, dcapture]: Nested</span>
<span class="sd">        Structure. The custom backprop function for `fwd`. bak needs to return</span>
<span class="sd">        dcapture if `fwd` uses any implicitly captured tensors, whose gradients</span>
<span class="sd">        are dcapture.</span>
<span class="sd">      bak_as_function: Whether to create a TF graph function for `bak`.</span>
<span class="sd">      device: The device on which to run `fwd` and `bak`. Defaults to the</span>
<span class="sd">        current device.</span>
<span class="sd">      use_tf_function: Whether use tf.function. Defaults to _UseTfFunction().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_fwd_sig</span> <span class="o">=</span> <span class="n">fwd_sig</span>

    <span class="n">wrapped_fwd_sig</span> <span class="o">=</span> <span class="n">fwd_sig</span>
    <span class="n">fwd_fn</span> <span class="o">=</span> <span class="n">fwd</span>
    <span class="n">bak_fn</span> <span class="o">=</span> <span class="n">bak</span>

    <span class="n">graph_random_seed</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">graph_random_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span>

    <span class="c1"># Wrap the forward function to propagate framework tensors like step_seed</span>
    <span class="c1"># and global_step.</span>
    <span class="n">wrapped_fwd_sig</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_added_global_step</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">GetGlobalStep</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">wrapped_fwd_sig</span><span class="p">[</span><span class="n">_FRAMEWORK_TENSOR_GLOBAL_STEP</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_added_global_step</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">fwd_sig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">wrapped_fwd_sig</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">fwd_sig</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">wrapped_fwd_sig</span><span class="p">:</span>
      <span class="n">wrapped_fwd_sig</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">ForwardWrapped</span><span class="p">(</span><span class="n">wrapped_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">graph_random_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">graph_random_seed</span><span class="p">)</span>
      <span class="n">global_step</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">if</span> <span class="n">wrapped_inputs</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrapped_inputs</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span>
        <span class="n">global_step</span> <span class="o">=</span> <span class="n">wrapped_inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_FRAMEWORK_TENSOR_GLOBAL_STEP</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">GlobalStepContext</span><span class="p">(</span><span class="n">global_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">wrapped_inputs</span> <span class="ow">and</span> <span class="s1">&#39;inputs&#39;</span> <span class="ow">in</span> <span class="n">wrapped_inputs</span><span class="p">:</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">fwd</span><span class="p">(</span><span class="n">wrapped_inputs</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">fwd</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">result</span>

    <span class="n">fwd_fn</span> <span class="o">=</span> <span class="n">ForwardWrapped</span>

    <span class="k">if</span> <span class="n">bak</span><span class="p">:</span>

      <span class="c1"># Wrap the backward function to return zero gradients for framework</span>
      <span class="c1"># tensors like step_seed and global_step.</span>
      <span class="k">def</span> <span class="nf">BackwardWrapped</span><span class="p">(</span><span class="n">wrapped_xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">graph_random_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">graph_random_seed</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">GlobalStepContext</span><span class="p">(</span>
            <span class="n">wrapped_xs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_FRAMEWORK_TENSOR_GLOBAL_STEP</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">bak</span><span class="p">(</span><span class="n">wrapped_xs</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">)</span>
        <span class="n">dxs</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">,</span> <span class="n">wrapped_xs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
          <span class="n">dxs</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dcapture</span> <span class="o">=</span> <span class="n">result</span>
          <span class="k">return</span> <span class="n">dxs</span><span class="p">,</span> <span class="n">dcapture</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">dxs</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">result</span>
          <span class="k">return</span> <span class="n">dxs</span>

      <span class="n">bak_fn</span> <span class="o">=</span> <span class="n">BackwardWrapped</span>

    <span class="k">if</span> <span class="n">use_tf_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">use_tf_function</span> <span class="o">=</span> <span class="n">_UseTfFunction</span><span class="p">()</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">_DefineFunction</span> <span class="k">if</span> <span class="n">use_tf_function</span> <span class="k">else</span> <span class="n">_DefineDefun</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span>
        <span class="n">fwd</span><span class="o">=</span><span class="n">fwd_fn</span><span class="p">,</span>
        <span class="n">fwd_sig</span><span class="o">=</span><span class="n">wrapped_fwd_sig</span><span class="p">,</span>
        <span class="n">bak</span><span class="o">=</span><span class="n">bak_fn</span><span class="p">,</span>
        <span class="n">bak_as_function</span><span class="o">=</span><span class="n">bak_as_function</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Invokes the graph function.</span>

<span class="sd">    Args:</span>
<span class="sd">      args: the inputs to the graph function, must be compatible with `fwd_sig`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The output tensors with the same structure as the output of `fwd`,</span>
<span class="sd">      returned by a call to the graph function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">IsCompatible</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_fwd_sig</span><span class="p">),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> vs </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fwd_sig</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AddFrameworkInputs</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The underlying TensorFlow graph function that this object encapsulates.</span>

<span class="sd">    The returned graph function is created by tracing `fwd` during construction.</span>
<span class="sd">    If not None, it will be a _DefinedFunction or ConcreteFunction that takes</span>
<span class="sd">    flat inputs and returns flat outputs, and can be used by routines that</span>
<span class="sd">    require a TensorFlow function object (e.g. tf.If, tf.While, etc).</span>

<span class="sd">    If no backprop function is provided during construction, the result is</span>
<span class="sd">    always not None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">func</span>

<div class="viewcode-block" id="DefinedFunction.AddFrameworkInputs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefinedFunction.AddFrameworkInputs">[docs]</a>  <span class="k">def</span> <span class="nf">AddFrameworkInputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add framework tensors like step_seed and global_step to inputs.</span>

<span class="sd">    This is only necessary when using `func`, as wrapping is handled</span>
<span class="sd">    automatically in __call__.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: inputs to the function.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Inputs wrapped with framework tensors suitable for use with `func`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_added_global_step</span><span class="p">:</span>
      <span class="n">global_step</span> <span class="o">=</span> <span class="n">GetGlobalStep</span><span class="p">()</span>
      <span class="k">assert</span> <span class="n">global_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">result</span><span class="p">[</span><span class="n">_FRAMEWORK_TENSOR_GLOBAL_STEP</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">result</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">return</span> <span class="n">result</span> <span class="k">if</span> <span class="n">result</span> <span class="k">else</span> <span class="kc">None</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_dtypes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Output dtypes of the graph function.</span>

<span class="sd">    The result will have the same structure as the outputs of `fwd` but contain</span>
<span class="sd">    the corresponding output dtypes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stateful_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stateful ops used by `fwd`, as a list of (op_name, op_type) tuples.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">stateful_ops</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">captured_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implicit input tensors captured by `fwd`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">captured_inputs</span></div>


<div class="viewcode-block" id="CallDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CallDefun">[docs]</a><span class="k">def</span> <span class="nf">CallDefun</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bak</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bak_as_function</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps fwd in a defun with custom gradient bak and calls it with args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure.</span>
<span class="sd">    args: A Nested Structure of tf.Tensor or None.</span>
<span class="sd">    bak: A callable xs, ys, dys: Nested Structure -&gt; dxs[, dcapture]: Nested</span>
<span class="sd">      Structure. The custom backprop function for fwd. bak needs to return</span>
<span class="sd">      dcapture if fwd uses any implicitly captured tensors, whose gradients are</span>
<span class="sd">      dcapture.</span>
<span class="sd">    bak_as_function: Whether to create a TF graph function for bak.</span>
<span class="sd">    device: the device on which to run fwd and bak.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Nested Structure equivalent to what fwd(args) computes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
  <span class="n">sigs</span> <span class="o">=</span> <span class="n">Function</span><span class="p">(</span>
      <span class="n">fwd_sig</span><span class="o">=</span><span class="n">TensorSpecs</span><span class="p">(</span><span class="n">args</span><span class="p">),</span>
      <span class="n">bak</span><span class="o">=</span><span class="n">bak</span><span class="p">,</span>
      <span class="n">bak_as_function</span><span class="o">=</span><span class="n">bak_as_function</span><span class="p">,</span>
      <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)(</span>
          <span class="n">fwd</span><span class="o">=</span><span class="n">fwd</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">sigs</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">sigs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span></div>


<div class="viewcode-block" id="If"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.If">[docs]</a><span class="k">def</span> <span class="nf">If</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">then_branch</span><span class="p">,</span> <span class="n">else_branch</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to construct an if/else statement.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond: A scalar `Tensor` that can be converted to boolean.</span>
<span class="sd">    inputs: A flattenable representing the input tensors of the if/else</span>
<span class="sd">      statement. Can be None to represent no inputs.</span>
<span class="sd">    then_branch: A callable &#39;inputs&#39; -&gt; flattenable. The returned value should</span>
<span class="sd">      be compatible with what &#39;else_branch&#39; returns.</span>
<span class="sd">    else_branch: A callable &#39;inputs&#39; -&gt; flattenable. The returned value should</span>
<span class="sd">      be compatible with what &#39;then_branch&#39; returns.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Output returned by the call to either &#39;then_branch&#39; or &#39;else_branch&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">fwd_sig</span> <span class="o">=</span> <span class="n">TensorSpecs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">then_sigs</span> <span class="o">=</span> <span class="n">Function</span><span class="p">(</span><span class="n">fwd_sig</span><span class="o">=</span><span class="n">fwd_sig</span><span class="p">)(</span><span class="n">fwd</span><span class="o">=</span><span class="n">then_branch</span><span class="p">)</span>
  <span class="n">else_sigs</span> <span class="o">=</span> <span class="n">Function</span><span class="p">(</span><span class="n">fwd_sig</span><span class="o">=</span><span class="n">fwd_sig</span><span class="p">)(</span><span class="n">fwd</span><span class="o">=</span><span class="n">else_branch</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">IsCompatible</span><span class="p">(</span><span class="n">then_sigs</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">,</span> <span class="n">else_sigs</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">),</span> <span class="p">(</span>
      <span class="s1">&#39;Outputs of then_branch and else_branch are not compatible: </span><span class="si">{}</span><span class="s1"> vs </span><span class="si">{}</span><span class="s1">&#39;</span>
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">then_sigs</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">,</span> <span class="n">else_sigs</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">then_sigs</span><span class="o">.</span><span class="n">captured_inputs</span> <span class="o">!=</span> <span class="n">else_sigs</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Differing captured inputs in then and else. &#39;</span>
                     <span class="s1">&#39;Ensure the same tensors are captured in the same order.&#39;</span><span class="p">)</span>

  <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">If</span><span class="p">(</span>
      <span class="n">cond</span><span class="o">=</span><span class="n">cond</span><span class="p">,</span>
      <span class="n">inputs</span><span class="o">=</span><span class="n">Flatten</span><span class="p">(</span><span class="n">then_sigs</span><span class="o">.</span><span class="n">AddFrameworkInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span> <span class="o">+</span>
      <span class="n">then_sigs</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">,</span>
      <span class="n">then_branch</span><span class="o">=</span><span class="n">then_sigs</span><span class="o">.</span><span class="n">func</span><span class="p">,</span>
      <span class="n">else_branch</span><span class="o">=</span><span class="n">else_sigs</span><span class="o">.</span><span class="n">func</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Pack</span><span class="p">(</span><span class="n">then_sigs</span><span class="o">.</span><span class="n">output_dtypes</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span></div>


<div class="viewcode-block" id="_Itype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Itype">[docs]</a><span class="k">def</span> <span class="nf">_Itype</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Loop iterator data type.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">use_xla</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span></div>


<div class="viewcode-block" id="WhileLoop"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WhileLoop">[docs]</a><span class="k">def</span> <span class="nf">WhileLoop</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">loop_state</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to construct a while loop.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond: A callable NestedMap -&gt; tf.bool.</span>
<span class="sd">    body: A callable NestedMap -&gt; NestedMap.</span>
<span class="sd">    loop_state: A flattenable (NestedMap, list, tuple, etc.) representing the</span>
<span class="sd">      loop state.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The final loop state in the same structure as loop_state.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">fwd_sig</span> <span class="o">=</span> <span class="n">TensorSpecs</span><span class="p">(</span><span class="n">loop_state</span><span class="p">)</span>
  <span class="n">cond_sigs</span> <span class="o">=</span> <span class="n">Function</span><span class="p">(</span><span class="n">fwd_sig</span><span class="o">=</span><span class="n">fwd_sig</span><span class="p">)(</span><span class="n">fwd</span><span class="o">=</span><span class="n">cond</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">BodyWrapped</span><span class="p">(</span><span class="n">loop_state</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="n">loop_state</span><span class="p">)</span>
    <span class="c1"># loop_state is augmented with global tensors inside of DefinedFunction.</span>
    <span class="c1"># WhileLoop needs to return the same structure as the inputs, so we augment</span>
    <span class="c1"># the return value here to match.</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">cond_sigs</span><span class="o">.</span><span class="n">AddFrameworkInputs</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="n">body_sigs</span> <span class="o">=</span> <span class="n">Function</span><span class="p">(</span><span class="n">fwd_sig</span><span class="o">=</span><span class="n">fwd_sig</span><span class="p">)(</span><span class="n">fwd</span><span class="o">=</span><span class="n">BodyWrapped</span><span class="p">)</span>
  <span class="n">wrapped_inputs</span> <span class="o">=</span> <span class="n">body_sigs</span><span class="o">.</span><span class="n">AddFrameworkInputs</span><span class="p">(</span><span class="n">loop_state</span><span class="p">)</span>
  <span class="n">new_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">While</span><span class="p">(</span>
      <span class="n">Flatten</span><span class="p">(</span><span class="n">wrapped_inputs</span><span class="p">),</span> <span class="n">cond</span><span class="o">=</span><span class="n">cond_sigs</span><span class="o">.</span><span class="n">func</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body_sigs</span><span class="o">.</span><span class="n">func</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Pack</span><span class="p">(</span><span class="n">wrapped_inputs</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span><span class="o">.</span><span class="n">inputs</span></div>


<div class="viewcode-block" id="ForLoop"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ForLoop">[docs]</a><span class="k">def</span> <span class="nf">ForLoop</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">loop_state</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to construct a for loop.</span>

<span class="sd">  Args:</span>
<span class="sd">    body: A callable (tf.int, NestedMap) -&gt; NestedMap.</span>
<span class="sd">    start: Loop variable&#39;s initial value.</span>
<span class="sd">    limit: Loop variable&#39;s limit value.</span>
<span class="sd">    delta: Loop variable&#39;s change per iteration.</span>
<span class="sd">    loop_state: A flattenable (NestedMap, list, tuple, etc.) representing the</span>
<span class="sd">      loop state.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The final loop state in the same structure as loop_state.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span>
      <span class="nb">iter</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">limit</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">delta</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">loop_state</span><span class="o">=</span><span class="n">loop_state</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LoopCond</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">limit</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LoopBody</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">state</span><span class="o">.</span><span class="n">loop_state</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">loop_state</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>

  <span class="k">return</span> <span class="n">WhileLoop</span><span class="p">(</span><span class="n">LoopCond</span><span class="p">,</span> <span class="n">LoopBody</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">loop_state</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TopK">[docs]</a><span class="k">def</span> <span class="nf">TopK</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Equivalent to tf.math.top_k(x_in, k) but more efficient on tpu.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">k</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;This implementation is only efficient for small k.&#39;</span>
  <span class="c1"># TODO(yonghui): Try out an alternative idea where we first reshape x_in as a</span>
  <span class="c1"># 2d tensor, then call tf.math.top_k, and then reshape back.</span>
  <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">x_rank</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">assert</span> <span class="n">x_rank</span> <span class="ow">and</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
  <span class="n">last_dim_size</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
  <span class="n">min_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>

  <span class="n">out_indices</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">out_values</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">unused_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="n">index_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">mask_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">index_i</span><span class="p">,</span> <span class="n">last_dim_size</span><span class="p">)</span>
    <span class="c1"># TODO(yonghui): Would tf.gather be more efficient and numerically stable</span>
    <span class="c1"># here?</span>
    <span class="n">value_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask_i</span> <span class="o">*</span> <span class="n">x_in</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x_in</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mask_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_in</span> <span class="o">+</span> <span class="n">mask_i</span> <span class="o">*</span> <span class="n">min_value</span>
    <span class="n">out_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">index_i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">out_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value_i</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out_values</span><span class="p">,</span> <span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out_indices</span><span class="p">,</span> <span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReadVariable"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadVariable">[docs]</a><span class="k">def</span> <span class="nf">ReadVariable</span><span class="p">(</span><span class="n">var_op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the value of the given variable operation.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_op: the `Operation` object for a VarHandleOp.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if var_op is not a VarHandleOp.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` containing the value of the variable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">var_op</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s1">&#39;VarHandleOp&#39;</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;var_op should be a VarHandleOp, got </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">var_op</span><span class="o">.</span><span class="n">type</span><span class="p">))</span>
  <span class="c1"># Filter out the ReadVariableOps that have control dependencies to avoid</span>
  <span class="c1"># side-effects when the user runs it.</span>
  <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;ReadVariableOp&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span>
  <span class="n">var_readers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">filter_fn</span><span class="p">,</span> <span class="n">var_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">consumers</span><span class="p">()))</span>
  <span class="k">assert</span> <span class="n">var_readers</span>
  <span class="k">return</span> <span class="n">var_readers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="n">_TPU_SUMMARY_TENSORS_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_tpu_summary_tensors&#39;</span><span class="p">)</span>

<span class="n">_get_tpu_summary_tensors</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_TPU_SUMMARY_TENSORS_KEY</span><span class="p">,</span>
                                             <span class="k">lambda</span><span class="p">:</span> <span class="p">[])</span>


<div class="viewcode-block" id="AddTpuSummaryTensor"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddTpuSummaryTensor">[docs]</a><span class="k">def</span> <span class="nf">AddTpuSummaryTensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds tensor to global collection of summaries.</span>

<span class="sd">  This needs to be used in situations where tf.summary() could be used but</span>
<span class="sd">  currently tf.summary is not supported. Use py_utils.AddTpuSummaryTensor() in</span>
<span class="sd">  low level code to add summary tensors to global collection of summaries.</span>
<span class="sd">  Then recover all summary tensors from global collection by calling</span>
<span class="sd">  py_utils.GetTpuSummaryTensors() from top level code (for example from</span>
<span class="sd">  ComputeLoss method of BaseTask).</span>

<span class="sd">  In addition to &#39;name&#39; argument, current tensorflow name scope is also</span>
<span class="sd">  captured and added to the metric name. This way for example summaries from</span>
<span class="sd">  a repeated layer will appear as separate graphs in the tensorboard.</span>

<span class="sd">  Weight argument is optional and defaults to 1.0. See BaseTask.ComputeLoss for</span>
<span class="sd">  the exact definition of weight for eval metrics.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: metric name</span>
<span class="sd">    value: metric value tensor</span>
<span class="sd">    weight: weight tensor for weighted metrics</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tpu_summary_tensors</span> <span class="o">=</span> <span class="n">_get_tpu_summary_tensors</span><span class="p">()</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
  <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
  <span class="n">x</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
  <span class="n">x</span><span class="o">.</span><span class="n">name_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">()</span>
  <span class="n">tpu_summary_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetTpuSummaryTensors"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetTpuSummaryTensors">[docs]</a><span class="k">def</span> <span class="nf">GetTpuSummaryTensors</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns summary tensors from global collection.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dict containing str keys and (metric, weight) pairs as values</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tpu_summary_tensors</span> <span class="o">=</span> <span class="n">_get_tpu_summary_tensors</span><span class="p">()</span>
  <span class="k">return</span> <span class="p">{</span>
      <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">SanitizeScopeKey</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name_scope</span><span class="p">)):</span> <span class="n">x</span><span class="o">.</span><span class="n">value</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tpu_summary_tensors</span>
  <span class="p">}</span></div>


<div class="viewcode-block" id="ClearTpuSummaryTensors"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ClearTpuSummaryTensors">[docs]</a><span class="k">def</span> <span class="nf">ClearTpuSummaryTensors</span><span class="p">():</span>
  <span class="n">tpu_summary_tensors</span> <span class="o">=</span> <span class="n">_get_tpu_summary_tensors</span><span class="p">()</span>
  <span class="k">del</span> <span class="n">tpu_summary_tensors</span><span class="p">[:]</span></div>


<div class="viewcode-block" id="ComputationShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputationShape">[docs]</a><span class="k">def</span> <span class="nf">ComputationShape</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">topology</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Decides the computation shape based on the split_size.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">topology</span><span class="p">:</span>
    <span class="n">topology_info</span> <span class="o">=</span> <span class="n">tf_topology</span><span class="o">.</span><span class="n">Topology</span><span class="p">(</span><span class="n">serialized</span><span class="o">=</span><span class="n">topology</span><span class="p">)</span>
  <span class="n">computation_shape</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">topology</span> <span class="ow">and</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">,</span>
                                   <span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">split_size</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">topology</span> <span class="ow">and</span> <span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">[</span>
      <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">split_size</span> <span class="ow">in</span> <span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">:</span>
    <span class="c1"># For Megacore, if we find exact match on mesh shape, map split_size to it</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">computation_shape</span><span class="p">[</span><span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">(</span>
        <span class="n">split_size</span><span class="p">)]</span> <span class="o">=</span> <span class="n">split_size</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">32</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">64</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">topology</span> <span class="ow">and</span> <span class="n">topology_info</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">32</span><span class="p">:</span>
      <span class="c1"># Fwd within-replica all-reduces is performed along column;</span>
      <span class="c1"># Bwd gradient cross-replica all-reduces is performed along row.</span>
      <span class="c1"># This currently has better performance than the strided patten.</span>
      <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">128</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">256</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">512</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">1024</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">2048</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">4096</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">elif</span> <span class="n">split_size</span> <span class="o">==</span> <span class="mi">8192</span><span class="p">:</span>
    <span class="n">computation_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Model parallelism with </span><span class="si">%d</span><span class="s1"> devices is currently not&#39;</span>
                   <span class="s1">&#39; supported.&#39;</span> <span class="o">%</span> <span class="n">split_size</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">computation_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="k">return</span> <span class="n">computation_shape</span></div>


<div class="viewcode-block" id="GetExtraVars"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetExtraVars">[docs]</a><span class="k">def</span> <span class="nf">GetExtraVars</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the captured variables by the function.&quot;&quot;&quot;</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="o">.</span><span class="n">variable_captures</span>
  <span class="k">return</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_vars</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetExtraInputs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetExtraInputs">[docs]</a><span class="k">def</span> <span class="nf">GetExtraInputs</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the captured input tensors by the function.&quot;&quot;&quot;</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="o">.</span><span class="n">external_captures</span>
  <span class="k">return</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_inputs</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetExtraArgs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetExtraArgs">[docs]</a><span class="k">def</span> <span class="nf">GetExtraArgs</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the corresponding function arguments for the captured inputs.&quot;&quot;&quot;</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="o">.</span><span class="n">internal_captures</span>
  <span class="k">return</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_args</span><span class="p">()</span></div>


<div class="viewcode-block" id="ShardedFilePatternToGlob"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ShardedFilePatternToGlob">[docs]</a><span class="k">def</span> <span class="nf">ShardedFilePatternToGlob</span><span class="p">(</span><span class="n">file_pattern</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts a file pattern path@shards to path-?????-of-shards.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="s1">&#39;,&#39;</span> <span class="ow">in</span> <span class="n">file_pattern</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s1">&#39;ShardedFilePatternToGlob does not support multiple file patterns.&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="s1">&#39;@&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">file_pattern</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">file_pattern</span>
  <span class="n">path</span><span class="p">,</span> <span class="n">shards</span> <span class="o">=</span> <span class="n">file_pattern</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;@&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">shards</span> <span class="o">==</span> <span class="s1">&#39;*&#39;</span><span class="p">:</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s1">-?????-of-*&#39;</span>
  <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s1">-?????-of-</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">:</span><span class="s1">05</span><span class="si">}</span><span class="s1">&#39;</span></div>


<div class="viewcode-block" id="ComputeNceAndAuc"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeNceAndAuc">[docs]</a><span class="k">def</span> <span class="nf">ComputeNceAndAuc</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute normalized cross entropy and AUC of the PR curve for a batch.</span>

<span class="sd">  Args:</span>
<span class="sd">    probs: a tensor of shape [batch, time].</span>
<span class="sd">    targets: a tensor of shape [batch, time], where each element is either 0 or</span>
<span class="sd">      1 indicating wrong or correct.</span>
<span class="sd">    mask: a tensor of shape [batch, time], a mask for hyp sequence.</span>

<span class="sd">  Returns:</span>
<span class="sd">    nce: a tensor of shape [1], the normalized cross entropy value.</span>
<span class="sd">    auc: a tensor of shape [1], the AUC value.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">LogWithClip</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clip all elements of a tensor to a minimum before taking log.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

  <span class="n">bce</span> <span class="o">=</span> <span class="o">-</span><span class="n">targets</span> <span class="o">*</span> <span class="n">LogWithClip</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">*</span> <span class="n">LogWithClip</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probs</span><span class="p">)</span>
  <span class="n">num_cor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span>
  <span class="n">num_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
  <span class="n">wcr</span> <span class="o">=</span> <span class="n">num_cor</span> <span class="o">/</span> <span class="n">num_tokens</span>
  <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">wcr</span> <span class="o">*</span> <span class="n">LogWithClip</span><span class="p">(</span><span class="n">wcr</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">wcr</span><span class="p">)</span> <span class="o">*</span> <span class="n">LogWithClip</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">wcr</span><span class="p">)</span>
  <span class="n">avg_conditional_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">bce</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span>
  <span class="n">nce</span> <span class="o">=</span> <span class="p">(</span><span class="n">entropy</span> <span class="o">-</span> <span class="n">avg_conditional_entropy</span><span class="p">)</span> <span class="o">/</span> <span class="n">entropy</span>
  <span class="n">auc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">curve</span><span class="o">=</span><span class="s1">&#39;PR&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">nce</span><span class="p">,</span> <span class="n">auc</span></div>


<div class="viewcode-block" id="GatherTensorValuesBySeqIndices"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GatherTensorValuesBySeqIndices">[docs]</a><span class="k">def</span> <span class="nf">GatherTensorValuesBySeqIndices</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">class_indices</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gather values from a 3d tensor according to sequences of indices.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: a 3d tensor of [dim0, dim1, num_class], e.g. output from softmax.</span>
<span class="sd">    class_indices: a 2d tensor of [dim0, dim1], where the second dim is a</span>
<span class="sd">      sequence of class indices between 0 to num_class - 1, inclusive.</span>
<span class="sd">    keepdims: bool, expand the last dimension of the returned tensor if True.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor ret of [dim0, dim1], where</span>
<span class="sd">      ret[b, t] = tensor[b, t, indices[b, t]].</span>
<span class="sd">      If keepdims is True, then ret has shape [dim0, dim1, 1].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
  <span class="n">class_indices</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">class_indices</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">class_indices</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">dim0</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">class_indices</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">dim1</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">class_indices</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">dim0_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">dim0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="p">])</span>
  <span class="n">dim1_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">dim1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">dim0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">gather_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dim0_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">class_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dim1_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">class_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">class_indices</span>
  <span class="p">],</span>
                            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_indices</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">keepdims</span><span class="p">:</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="GetSoftmaxProbsBySeqIndices"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetSoftmaxProbsBySeqIndices">[docs]</a><span class="k">def</span> <span class="nf">GetSoftmaxProbsBySeqIndices</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get softmax probabilities from index sequences given logits sequences.</span>

<span class="sd">  Args:</span>
<span class="sd">    logits: a tensor of [batch, time, num_class] or [time, batch, num_class].</span>
<span class="sd">    indices: a tensor of [batch, time] or [time, batch].</span>
<span class="sd">    keepdims: bool, expand the last dimension of the returned tensor if True.</span>

<span class="sd">  Returns:</span>
<span class="sd">    a tensor of [batch, time] or [time, batch] for the corresponding softmax</span>
<span class="sd">      probabilities. If keepdims is True, returned tensor has a third dimension</span>
<span class="sd">      of size 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">GatherTensorValuesBySeqIndices</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="DivideNoNan"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DivideNoNan">[docs]</a><span class="k">def</span> <span class="nf">DivideNoNan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Equivalent to tf.math.divide_no_nan but supports bfloat16.&quot;&quot;&quot;</span>
  <span class="n">safe_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">0.</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span> <span class="o">/</span> <span class="n">safe_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="SequencePaddings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SequencePaddings">[docs]</a><span class="k">def</span> <span class="nf">SequencePaddings</span><span class="p">(</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span></div>


<div class="viewcode-block" id="AppendDims"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AppendDims">[docs]</a><span class="k">def</span> <span class="nf">AppendDims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndims</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndims</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>