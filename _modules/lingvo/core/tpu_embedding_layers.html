

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.tpu_embedding_layers &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.tpu_embedding_layers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.tpu_embedding_layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;TPU embedding layers.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">schedule</span>

<span class="c1"># pylint:disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="kn">import</span> <span class="n">tpu_embedding</span> <span class="k">as</span> <span class="n">tpu_embedding_lib</span>
<span class="c1"># pylint:enable=g-direct-tensorflow-import</span>


<div class="viewcode-block" id="_AddTpuEmbeddingSummaryTensor"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._AddTpuEmbeddingSummaryTensor">[docs]</a><span class="k">def</span> <span class="nf">_AddTpuEmbeddingSummaryTensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING_SUMMARY_TENSORS</span><span class="p">,</span>
                       <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">)))</span></div>


<span class="c1"># TODO(jeffreyzhao): Add the rest of the TPU Embedding optimizers.</span>
<div class="viewcode-block" id="_TPUEmbeddingOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer">[docs]</a><span class="k">class</span> <span class="nc">_TPUEmbeddingOptimizer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for TPUEmbeddingLayer, TPUEmbeddingTable optimizers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_weight_min&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The minimum value to clip by; None means -infinity.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_weight_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The maximum value to clip by; None means +infinity.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weight_decay_factor&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Amount of weight decay to apply; None means that the weights are not &#39;</span>
        <span class="s1">&#39;decayed.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;multiply_weight_decay_factor_by_learning_rate&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If true, weight_decay_factor is multiplied by the current learning &#39;</span>
        <span class="s1">&#39;rate.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create TPUEmbedding API optimzier parameters.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create slot variables and infeed/retrieval ops.</span>

<span class="sd">    Args:</span>
<span class="sd">      table_vars: A list of all embedding table shard variables.</span>
<span class="sd">      tpu_embedding_table: Parent TPUEmbeddingTable layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of load ops</span>
<span class="sd">      List of retrieve ops</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingSGDOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;SGD optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">StochasticGradientDescentParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">load_tpu_embedding_stochastic_gradient_descent_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_stochastic_gradient_descent_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingAdagradOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adagrad optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;initial_accumulator&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span>
             <span class="s1">&#39;Initial value of Adagrad accumulator.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_gradient_accumulation&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;Setting this to False makes embedding gradients calculation less &#39;</span>
        <span class="s1">&#39;accurate but faster. See tpu_embedding_lib for more details.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">AdagradParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">initial_accumulator</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>
    <span class="n">slot_var_collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">tpu_embedding_table</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="n">w_ada</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/Adagrad&#39;</span> <span class="o">%</span> <span class="n">var_name</span><span class="p">,</span> <span class="n">w_ada</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">accumulator_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/Adagrad&#39;</span> <span class="o">%</span> <span class="n">var_name</span><span class="p">]</span>

        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span><span class="o">.</span><span class="n">load_tpu_embedding_adagrad_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">accumulators</span><span class="o">=</span><span class="n">accumulator_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span><span class="p">,</span> <span class="n">retrieved_accumulator</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_adagrad_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">accumulator_var</span><span class="p">,</span> <span class="n">retrieved_accumulator</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingTable"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingTable</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An embedding table controlled by TPUEmbeddingLayer.</span>

<span class="sd">  Note that all input_keys needs to be declared upfront.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingTable.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_keys&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Name of inputs in InputBatch.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;combiner&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Must be &quot;sum&quot;, &quot;sqrtn&quot;, &quot;mean&quot; or None in the case of a &#39;</span>
        <span class="s1">&#39;&quot;sequence embedding &quot;&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;max_sequence_length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If not None or 0, embedding lookup will return a &#39;</span>
        <span class="s1">&#39;&quot;sequence embedding&quot; of shape &#39;</span>
        <span class="s1">&#39;`[batch, max_sequence_length, embedding_dim]` without applying a &#39;</span>
        <span class="s1">&#39;sequence  reducing combiner&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_tpu_hosts&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of TPU hosts.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Table optimizer parameters. Will override the optimizer parameters &#39;</span>
        <span class="s1">&#39;defined in this table</span><span class="se">\&#39;</span><span class="s1">s TPUEmbeddingLayer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Overrides TPUEmbeddingLayer</span><span class="se">\&#39;</span><span class="s1">s learning_rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;lr_schedule&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Overrides TPUEmbeddingLayer</span><span class="se">\&#39;</span><span class="s1">s lr_schedule.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_keys</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">combiner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">combiner</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_padded_vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_keys</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_keys</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;schedule&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">LearningRateFn</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GlobalStepContext</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span><span class="o">.</span><span class="n">Value</span><span class="p">()</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
      <span class="n">_AddTpuEmbeddingSummaryTensor</span><span class="p">(</span><span class="s1">&#39;tpu_embedding_lr/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">),</span> <span class="n">lr</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">lr</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_table_name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_table&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_table_config</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TableConfig</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_padded_vocab_size</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
        <span class="n">combiner</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">combiner</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_fn</span><span class="o">=</span><span class="n">LearningRateFn</span><span class="p">,</span>
        <span class="c1"># All TableConfigs passed to API will have a learning rate function,</span>
        <span class="c1"># so the learning_rate in the optimization_parameters is not used.</span>
        <span class="n">optimization_parameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">CreateOptimizerParameters</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="TPUEmbeddingTable._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">],</span>
        <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

    <span class="n">embedding_table_vars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">):</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
        <span class="n">embedding_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
        <span class="n">embedding_table_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding_var</span><span class="p">)</span>
        <span class="c1"># Remove from _private_vars / _private_thetas to be added later as wm.</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="c1"># We don&#39;t want to add this for TrainerTpu, otherwise the identity</span>
      <span class="c1"># reference leads to copying the embedding to the TPU for no reason.</span>
      <span class="c1"># However, this is needed for CPU (eval/decode/controller).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_table_vars</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">embedding_table_vars</span><span class="p">]</span>

    <span class="c1"># Only trainer and controller need slot variables and load/retrieve ops.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_load_op_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_op_list</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="n">embedding_table_vars</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span></div>

  <span class="c1"># Return device to place sharded variables on.</span>
<div class="viewcode-block" id="TPUEmbeddingTable.GetDeviceName"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.GetDeviceName">[docs]</a>  <span class="k">def</span> <span class="nf">GetDeviceName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/replica:0/task:</span><span class="si">{}</span><span class="s1">/device:CPU:0&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">host_id</span><span class="p">)</span></div>

  <span class="c1"># Return variable name for embedding table shards.</span>
<div class="viewcode-block" id="TPUEmbeddingTable.GetVariableName"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.GetVariableName">[docs]</a>  <span class="k">def</span> <span class="nf">GetVariableName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_id</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;var_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">host_id</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">table_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_config</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">table_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">retrieve_op_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_op_list</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">load_op_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_op_list</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_keys</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">max_sequence_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span>

<div class="viewcode-block" id="TPUEmbeddingTable.CpuEmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.CpuEmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">CpuEmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">,</span> <span class="n">partition_strategy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A dict of `input_key` string -&gt; [batch, sequence] int32 Tensor.</span>
<span class="sd">        -1 is used as a padding id.</span>
<span class="sd">      partition_strategy: See TPUEmbeddingLayer partition_strategy param.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An activations dict of string -&gt; float32 Tensor.</span>
<span class="sd">      For non-sequence embeddings: [batch, 1, embedding_dim]</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">rets</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># &quot;Sequence embedding&quot;, no combiner case</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">partition_strategy</span><span class="o">=</span><span class="n">partition_strategy</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">rets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Non-&quot;Sequence embedding&quot;, combiner case</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Dense to sparse.</span>
        <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">embedding_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">sample_indices</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">sparse_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">sample_indices</span><span class="p">,</span>
            <span class="n">values</span><span class="o">=</span><span class="n">embedding_indices</span><span class="p">,</span>
            <span class="n">dense_shape</span><span class="o">=</span><span class="n">dense_shape</span><span class="p">)</span>
        <span class="c1"># [?, embedding_dim]</span>
        <span class="c1"># For tf.nn.embedding_lookup_sparse, output.dim0 might be different from</span>
        <span class="c1"># sparse_ids.dense_shape.dim0.</span>
        <span class="c1"># In fact, the &#39;?&#39; is the smallest span starting from the index=0 that</span>
        <span class="c1"># covers all the results.</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup_sparse</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span>
            <span class="n">sparse_ids</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>  <span class="c1"># sp_weights</span>
            <span class="n">combiner</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">combiner</span><span class="p">,</span>
            <span class="n">partition_strategy</span><span class="o">=</span><span class="n">partition_strategy</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">dense_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Explicitly pad results to maintain dim0=batch.</span>
        <span class="n">dim0_padlen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">embs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim0_padlen</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
        <span class="c1"># [batch, 1, embedding_dim]</span>
        <span class="n">embs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rets</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Monolithic interface to TPU embedding.</span>

<span class="sd">  This layer has some important caveats, due to the interface of the</span>
<span class="sd">  TPU embedding hardware. Its behavior most closely mimics that of</span>
<span class="sd">  tf.nn.embedding_lookup_sparse.</span>

<span class="sd">  Supports multiple tables and multiple input_keys per table.</span>
<span class="sd">  Requires its own optimizer parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;tables&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;TPUEmbeddingTables&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pipeline_execution_with_tensor_core&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Set to True to be faster. See tpu_embedding.py for details.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Per-core batch size.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="n">TPUEmbeddingAdagradOptimizer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Layer optimizer parameters. Will be used for any TPUEmbeddingTables &#39;</span>
        <span class="s1">&#39;with None optimizer parameters.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Learning rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;lr_schedule&#39;</span><span class="p">,</span> <span class="n">schedule</span><span class="o">.</span><span class="n">ContinuousSchedule</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Lingvo learning rate schedule. Will be multiplied to learning rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;partition_strategy&#39;</span><span class="p">,</span> <span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="s1">&#39;A string, either &quot;mod&quot; or &quot;div&quot;, &#39;</span>
        <span class="s1">&#39;specifying how to map the lookup id to the embedding tensor. For &#39;</span>
        <span class="s1">&#39;more information see `tf.nn.embedding_lookup_sparse`.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;mod&#39;</span><span class="p">,</span> <span class="s1">&#39;div&#39;</span><span class="p">]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">==</span> <span class="n">num_tpu_hosts</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">])</span>

    <span class="c1"># Stop if a table has no optimizer parameters and the layer also has no</span>
    <span class="c1"># optimizer parameters</span>
    <span class="n">table_optimizer_missing</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">table_params</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">table_params</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">and</span> <span class="n">table_optimizer_missing</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;A table is missing optimizer parameters, and no layer-level &#39;</span>
          <span class="s1">&#39;optimizer parameters were given.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">table_optimizer_missing</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">table_params</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">table_params</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">table_params</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">table_params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">table_params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
        <span class="k">if</span> <span class="n">table_params</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">table_params</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;tables&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">)</span>

<div class="viewcode-block" id="TPUEmbeddingLayer._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Backwards compatibility: manually call child.InstantiateVariables()</span>
    <span class="c1"># outside of tf.variable_scope(p.name).</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
      <span class="n">table</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># At the feature level, track which are associated</span>
    <span class="c1"># with &quot;sequence embeddings&quot;.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">num_cores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">tpus_per_replica</span>
      <span class="n">global_batch_size</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">num_splits_per_client</span><span class="p">)</span>
      <span class="n">table_to_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="n">feature_to_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_to_config_dict</span><span class="p">[</span><span class="n">table</span><span class="o">.</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">table_config</span>
        <span class="n">load_op_list</span> <span class="o">+=</span> <span class="n">table</span><span class="o">.</span><span class="n">load_op_list</span>
        <span class="n">retrieve_op_list</span> <span class="o">+=</span> <span class="n">table</span><span class="o">.</span><span class="n">retrieve_op_list</span>
        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">table</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="n">feature_to_config_dict</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">FeatureConfig</span><span class="p">(</span>
              <span class="n">table</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">table</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;adding load and retrieve ops to collection.&#39;</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING_LOAD_OPS</span><span class="p">,</span> <span class="n">load_op_list</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING_RETRIEVE_OPS</span><span class="p">,</span>
                           <span class="n">retrieve_op_list</span><span class="p">)</span>

      <span class="n">tpu_embedding_collection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tpu_embedding_collection</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tpu_embedding_collection</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;TPUEmbedding API singleton already exists, reusing&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="n">tpu_embedding_collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TRAINING</span>
        <span class="n">device_config</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">DeviceConfig</span><span class="p">(</span>
            <span class="n">num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="n">num_hosts</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
            <span class="n">job_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TPUEmbedding</span><span class="p">(</span>
            <span class="n">table_to_config_dict</span><span class="p">,</span>
            <span class="n">feature_to_config_dict</span><span class="p">,</span>
            <span class="n">global_batch_size</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">,</span>
            <span class="n">master</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">pipeline_execution_with_tensor_core</span><span class="o">=</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">pipeline_execution_with_tensor_core</span><span class="p">),</span>
            <span class="n">partition_strategy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span><span class="p">,</span>
            <span class="n">device_config</span><span class="o">=</span><span class="n">device_config</span><span class="p">)</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for each entry in ids_map.</span>

<span class="sd">    Since the TPUEmbedding is monolothic, and consulted once per</span>
<span class="sd">    FProp/BPRop, we must centralize the lookup. Thus, for multiple</span>
<span class="sd">    features, we contain them into a single-lookup rather than allowing</span>
<span class="sd">    the caller to call Lookup multiple times.</span>

<span class="sd">    Currently, there&#39;s also an implied combination step which combines</span>
<span class="sd">    the sequence into a single set of activations by sum, mean or</span>
<span class="sd">    sqrtn.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A dict of `input_key` string -&gt; [batch, sequence] int32 Tensor.</span>
<span class="sd">        -1 is used as a padding id.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Activations dict of string -&gt;</span>
<span class="sd">      For non-sequence embeddings:  [batch, 1, embedding_dim],</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>
<span class="sd">      float32 Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">TpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;TPU Embedding lookup.&quot;&quot;&quot;</span>
      <span class="k">del</span> <span class="n">ids_map</span>
      <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span><span class="o">.</span><span class="n">get_activations</span><span class="p">()</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">TPU_EMBEDDING_ACTIVATIONS</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span>
      <span class="n">ret</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span><span class="p">:</span>
          <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Non-sequence embeddings, we fill the &quot;time&quot; dimension with 1.</span>
          <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">CpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup.&quot;&quot;&quot;</span>
      <span class="n">rets</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_id_map</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="n">table_id_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ids_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="n">table_rets</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">CpuEmbLookup</span><span class="p">(</span><span class="n">table_id_map</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span><span class="p">)</span>
        <span class="c1"># Merge table_rets with rets</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">table_rets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
          <span class="n">rets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
      <span class="k">return</span> <span class="n">rets</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">CpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">TpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>