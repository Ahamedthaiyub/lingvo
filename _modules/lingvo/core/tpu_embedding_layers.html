<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lingvo.core.tpu_embedding_layers &mdash; Lingvo  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>lingvo.core.tpu_embedding_layers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for lingvo.core.tpu_embedding_layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python3</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;TPU embedding layers.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">py_utils</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">schedule</span>

<span class="c1"># pylint:disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="kn">import</span> <span class="n">tpu_embedding</span> <span class="k">as</span> <span class="n">tpu_embedding_lib</span>
<span class="c1"># pylint:enable=g-direct-tensorflow-import</span>


<div class="viewcode-block" id="_IsTpuTraining"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._IsTpuTraining">[docs]</a><span class="k">def</span> <span class="nf">_IsTpuTraining</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Whether we should create embedding tables and run lookup on tpu.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">()</span></div>


<div class="viewcode-block" id="_RemovePrivateVar"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._RemovePrivateVar">[docs]</a><span class="k">def</span> <span class="nf">_RemovePrivateVar</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">var_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Remove a variable by name from `layer`.</span>

<span class="sd">  This is usually used to avoid copying the variable to TPU, for example, by the</span>
<span class="sd">  tf.cast when accessing layer.theta.</span>

<span class="sd">  Args:</span>
<span class="sd">    layer: The layer to remove the variable from.</span>
<span class="sd">    var_name: The name of the variable to remove.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">del</span> <span class="n">layer</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
  <span class="k">del</span> <span class="n">layer</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span></div>
  <span class="c1"># pylint: enable=protected-access</span>


<div class="viewcode-block" id="TpuEmbeddingCollection"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection">[docs]</a><span class="k">class</span> <span class="nc">TpuEmbeddingCollection</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Manage various TPU embedding related ops and tensors.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TpuEmbeddingCollection.Get"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.Get">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Get</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the TpuEmbeddingCollection associated with the current graph.&quot;&quot;&quot;</span>
    <span class="n">emb_collection</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetTpuEmbeddingGraphCollection</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">emb_collection</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">emb_collection</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
          <span class="s1">&#39;TpuEmbeddingCollection singleton already exists, reusing&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">emb_collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">singleton</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
      <span class="n">emb_collection</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">singleton</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">singleton</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Maps table name to the list of variables for the corresponding table.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_table_vars</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="c1"># The TPUEmbedding configuration.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Maps table name to the list of ops that loads/retrieves embedding tables</span>
    <span class="c1"># to/from TPU.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_load_ops_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_ops_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>

    <span class="c1"># Maps task name to the (feature_name -&gt; activation_tensor) dict for the</span>
    <span class="c1"># corresponding task.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># List of (name, value, weight) tuples for summary.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary_tensors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Set of embedding feature names.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Schedule for the value that is used as TPU embedding gradient multiplier.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_multiplier_schedule</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Maps task name to the mode used by that task.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mode_by_task</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Maps task name to the send gradient op for that task. Mainly used to</span>
    <span class="c1"># ensure that send gradient op is created only once for each task.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_send_gradient_op_by_task</span> <span class="o">=</span> <span class="p">{}</span>

<div class="viewcode-block" id="TpuEmbeddingCollection.AddTableVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.AddTableVariables">[docs]</a>  <span class="k">def</span> <span class="nf">AddTableVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">var_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add TPU embedding table variable list to the collection.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_vars</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variables for table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s1"> already exist.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_table_vars</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">var_list</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">table_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a NestedMap mapping table names to variables.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_vars</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">tpu_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span>

  <span class="nd">@tpu_embedding</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">tpu_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tpu_embedding</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;TPUEmbedding already set before.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="n">tpu_embedding</span>

<div class="viewcode-block" id="TpuEmbeddingCollection.AddLoadRetrieveOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.AddLoadRetrieveOps">[docs]</a>  <span class="k">def</span> <span class="nf">AddLoadRetrieveOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">load_ops</span><span class="p">,</span> <span class="n">retrieve_ops</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_ops_map</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Load ops for table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s1"> already exist.&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">table_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_ops_map</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_load_ops_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_ops</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_ops_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">retrieve_ops</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">load_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_ops_map</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">retrieve_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_ops_map</span>

<div class="viewcode-block" id="TpuEmbeddingCollection._ValidateTaskScope"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection._ValidateTaskScope">[docs]</a>  <span class="k">def</span> <span class="nf">_ValidateTaskScope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">task_call_scope</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;It expects a non-empty task call scope name, but get &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1">. This usually means the current code is not run &#39;</span>
          <span class="s1">&#39;under a py_utils.TaskCallScope() context.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.AddActivations"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.AddActivations">[docs]</a>  <span class="k">def</span> <span class="nf">AddActivations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ValidateTaskScope</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;Adding TPU embedding activations for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task_call_scope</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span><span class="p">:</span>
      <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span><span class="o">.</span><span class="n">get_activations</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span> <span class="o">=</span> <span class="n">activations</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.GetActivations"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.GetActivations">[docs]</a>  <span class="k">def</span> <span class="nf">GetActivations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;Getting TPU embedding activations for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task_call_scope</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ValidateTaskScope</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activations_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span>
    <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.AddSummaryTensor"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.AddSummaryTensor">[docs]</a>  <span class="k">def</span> <span class="nf">AddSummaryTensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_summary_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">)))</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">summary_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary_tensors</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">feature_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span>

  <span class="nd">@feature_names</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">feature_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span> <span class="o">!=</span> <span class="n">feature_names</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;feature_names already exists. &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;Existing feature names: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span><span class="si">}</span><span class="s1">, &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;feature names being added: </span><span class="si">{</span><span class="n">feature_names</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_feature_names</span> <span class="o">=</span> <span class="n">feature_names</span>

<div class="viewcode-block" id="TpuEmbeddingCollection.SetGradientMultiplierSchedule"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.SetGradientMultiplierSchedule">[docs]</a>  <span class="k">def</span> <span class="nf">SetGradientMultiplierSchedule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">multiplier_schedule</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_multiplier_schedule</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;gradient_multiplier_schedule was set before.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_multiplier_schedule</span> <span class="o">=</span> <span class="n">multiplier_schedule</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.SetTaskMode"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.SetTaskMode">[docs]</a>  <span class="k">def</span> <span class="nf">SetTaskMode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ValidateTaskScope</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;Setting TPU embedding mode for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1"> as </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mode_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.ShouldStopGradient"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.ShouldStopGradient">[docs]</a>  <span class="k">def</span> <span class="nf">ShouldStopGradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ValidateTaskScope</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task_call_scope</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mode_by_task</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s1">&#39;TPU embedding mode for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1"> not found.&#39;</span><span class="p">)</span>
    <span class="n">should_stop_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mode_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">((</span><span class="s1">&#39;Disabled&#39;</span> <span class="k">if</span> <span class="n">should_stop_gradient</span> <span class="k">else</span> <span class="s1">&#39;Enabled&#39;</span><span class="p">)</span> <span class="o">+</span>
                    <span class="sa">f</span><span class="s1">&#39; TPU embedding gradient for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">should_stop_gradient</span></div>

<div class="viewcode-block" id="TpuEmbeddingCollection.ApplyGradients"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TpuEmbeddingCollection.ApplyGradients">[docs]</a>  <span class="k">def</span> <span class="nf">ApplyGradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_call_scope</span><span class="p">,</span> <span class="n">feature_to_gradient_dict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply tpu embedding gradient updates.</span>

<span class="sd">    Args:</span>
<span class="sd">      task_call_scope: The current task call scope name.</span>
<span class="sd">      feature_to_gradient_dict: A `py_utils.NestedMap` of: tpu embedding feature</span>
<span class="sd">        name -&gt; gradient tensor for the embedding feature.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The gradient update op and a dict of eval metrics.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if gradients have been applied before for the current task.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ValidateTaskScope</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task_call_scope</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_gradient_op_by_task</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s1">&#39;Send gradient op for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1"> already exist.&#39;</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;Applying TPU embedding gradients for task </span><span class="si">{</span><span class="n">task_call_scope</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="c1"># Apply gradient multiplier schedule.</span>
    <span class="n">grad_multiplier</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_multiplier_schedule</span><span class="o">.</span><span class="n">Value</span><span class="p">()</span>
    <span class="n">feature_to_gradient_dict</span> <span class="o">=</span> <span class="n">feature_to_gradient_dict</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="n">g</span> <span class="o">*</span> <span class="n">grad_multiplier</span><span class="p">)</span>

    <span class="n">send_gradient_op</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span><span class="o">.</span><span class="n">generate_send_gradients_op</span><span class="p">(</span>
            <span class="n">feature_to_gradient_dict</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">GetGlobalStep</span><span class="p">()))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_send_gradient_op_by_task</span><span class="p">[</span><span class="n">task_call_scope</span><span class="p">]</span> <span class="o">=</span> <span class="n">send_gradient_op</span>

    <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetActivations</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="n">eval_metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;tpu_embedding_activation_norm&#39;</span><span class="p">:</span>
            <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">SumSquared</span><span class="p">(</span><span class="n">activations</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)),</span>
        <span class="s1">&#39;tpu_embedding_grad_norm&#39;</span><span class="p">:</span>
            <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">py_utils</span><span class="o">.</span><span class="n">SumSquared</span><span class="p">(</span><span class="n">feature_to_gradient_dict</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())),</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)),</span>
        <span class="s1">&#39;tpu_embedding_gradient_multiplier&#39;</span><span class="p">:</span>
            <span class="p">(</span><span class="n">grad_multiplier</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">send_gradient_op</span><span class="p">,</span> <span class="n">eval_metrics</span></div></div>


<div class="viewcode-block" id="_TPUEmbeddingOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer">[docs]</a><span class="k">class</span> <span class="nc">_TPUEmbeddingOptimizer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base class for TPUEmbeddingLayer, TPUEmbeddingTable optimizers.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_weight_min&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The minimum value to clip by; None means -infinity.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_weight_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;The maximum value to clip by; None means +infinity.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;weight_decay_factor&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Amount of weight decay to apply; None means that the weights are not &#39;</span>
        <span class="s1">&#39;decayed.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;multiply_weight_decay_factor_by_learning_rate&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If true, weight_decay_factor is multiplied by the current learning &#39;</span>
        <span class="s1">&#39;rate.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create TPUEmbedding API optimzier parameters.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="_TPUEmbeddingOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers._TPUEmbeddingOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create slot variables and load/retrieve ops.</span>

<span class="sd">    Args:</span>
<span class="sd">      table_vars: A list of all embedding table shard variables.</span>
<span class="sd">      tpu_embedding_table: Parent TPUEmbeddingTable layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of load ops</span>
<span class="sd">      List of retrieve ops</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingSGDOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;SGD optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">StochasticGradientDescentParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingSGDOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingSGDOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">load_tpu_embedding_stochastic_gradient_descent_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_stochastic_gradient_descent_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingAdagradOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adagrad optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;initial_accumulator&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span>
             <span class="s1">&#39;Initial value of Adagrad accumulator.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_gradient_accumulation&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;Setting this to False makes embedding gradients calculation less &#39;</span>
        <span class="s1">&#39;accurate but faster. See tpu_embedding_lib for more details.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">AdagradParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">initial_accumulator</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdagradOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdagradOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>
    <span class="n">slot_var_collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">tpu_embedding_table</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="n">w_ada</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/Adagrad&#39;</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">w_ada</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">accumulator_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>

        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># Remove the slot vars from the variable list to avoid them being</span>
          <span class="c1"># copied to TPU.</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="n">tpu_embedding_table</span><span class="p">,</span> <span class="n">var_name</span><span class="p">)</span>

          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span><span class="o">.</span><span class="n">load_tpu_embedding_adagrad_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">accumulators</span><span class="o">=</span><span class="n">accumulator_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span><span class="p">,</span> <span class="n">retrieved_accumulator</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_adagrad_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">accumulator_var</span><span class="p">,</span> <span class="n">retrieved_accumulator</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingAdamOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdamOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingAdamOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adam optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingAdamOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdamOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;clip_gradient_min&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Controls clipping of the gradient. The minimum value to clip by.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_gradient_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;The max value to clip by.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;sum_inside_sqrt&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;When this is true, the Adam update&#39;</span>
        <span class="s1">&#39;formula is changed from m / (sqrt(v) + epsilon) to m / &#39;</span>
        <span class="s1">&#39;sqrt(v + epsilon**2). This option improves the performance of&#39;</span>
        <span class="s1">&#39;TPU training and is not expected to harm model quality.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;lazy_adam&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Use lazy Adam instead of Adam. Lazy Adam&#39;</span>
             <span class="s1">&#39;trains faster.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;The exponential decay rate for the 1st moment&#39;</span>
             <span class="s1">&#39;estimates&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="s1">&#39;The exponential decay rate for the 2nd moment&#39;</span>
             <span class="s1">&#39;estimates&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="mf">1e-08</span><span class="p">,</span> <span class="s1">&#39;A small constant for numerical stability&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;use_gradient_accumulation&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Setting this to False makes&#39;</span>
        <span class="s1">&#39;embedding gradients calculation less accurate but faster&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdamOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdamOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">AdamParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">beta1</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span>
        <span class="n">beta2</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
        <span class="n">lazy_adam</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">lazy_adam</span><span class="p">,</span>
        <span class="n">sum_inside_sqrt</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">sum_inside_sqrt</span><span class="p">,</span>
        <span class="n">use_gradient_accumulation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_gradient_accumulation</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">,</span>
        <span class="n">clip_gradient_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_gradient_min</span><span class="p">,</span>
        <span class="n">clip_gradient_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_gradient_max</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingAdamOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingAdamOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>
    <span class="n">slot_var_collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">tpu_embedding_table</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="n">m_adam</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">var_name_m</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/Adam/m&#39;</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name_m</span><span class="p">,</span> <span class="n">m_adam</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">m_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name_m</span><span class="p">]</span>

        <span class="n">v_adam</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">var_name_v</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/Adam/v&#39;</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name_v</span><span class="p">,</span> <span class="n">v_adam</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">v_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name_v</span><span class="p">]</span>

        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># Remove the slot vars from the variable list to avoid them being</span>
          <span class="c1"># copied to TPU.</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="n">tpu_embedding_table</span><span class="p">,</span> <span class="n">var_name_m</span><span class="p">)</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="n">tpu_embedding_table</span><span class="p">,</span> <span class="n">var_name_v</span><span class="p">)</span>

          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span><span class="o">.</span><span class="n">load_tpu_embedding_adam_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">momenta</span><span class="o">=</span><span class="n">m_var</span><span class="p">,</span>
                    <span class="n">velocities</span><span class="o">=</span><span class="n">v_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span><span class="p">,</span> <span class="n">retrieved_m</span><span class="p">,</span> <span class="n">retrieved_v</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_adam_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">m_var</span><span class="p">,</span> <span class="n">retrieved_m</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v_var</span><span class="p">,</span> <span class="n">retrieved_v</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingFTRLOptimizer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingFTRLOptimizer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingFTRLOptimizer</span><span class="p">(</span><span class="n">_TPUEmbeddingOptimizer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;FTRL optimizer for TPUEmbeddingLayer, TPUEmbeddingTable.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingFTRLOptimizer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingFTRLOptimizer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;learning_rate_power&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="s1">&#39;A float value, must be less or equal to zero. Controls how the&#39;</span>
        <span class="s1">&#39;learning rate decreases during training. Use zero for a fixed learning&#39;</span>
        <span class="s1">&#39;rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;initial_accumulator_value&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;The starting value for&#39;</span>
        <span class="s1">&#39;accumulators. Only zero or positive values are allowed.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;l1_regularization_strength&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;A float value, must be greater&#39;</span>
        <span class="s1">&#39;than or equal to zero. Defaults to 0.0.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;l2_regularization_strength&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;A float value, must be greater&#39;</span>
        <span class="s1">&#39;than or equal to zero. Defaults to 0.0.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;multiply_linear_by_learning_rate&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether multiply&#39;</span>
             <span class="s1">&#39;linear by learning rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;A float value, representing the beta value from the&#39;</span>
        <span class="s1">&#39;FTLR paper. Defaults to 0.0.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;allow_zero_accumulator&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Whether allowing zero&#39;</span>
             <span class="s1">&#39;accumulator.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_gradient_min&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Clip gradient min value.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;clip_gradient_max&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Clip gradient max value.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;use_gradient_accumulation&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Use gradient accumulation.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;initial_linear_value&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Initial linear value.&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="TPUEmbeddingFTRLOptimizer.CreateOptimizerParameters"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingFTRLOptimizer.CreateOptimizerParameters">[docs]</a>  <span class="k">def</span> <span class="nf">CreateOptimizerParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">FtrlParameters</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">learning_rate_power</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">learning_rate_power</span><span class="p">,</span>
        <span class="n">initial_accumulator_value</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator_value</span><span class="p">,</span>
        <span class="n">l1_regularization_strength</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">l1_regularization_strength</span><span class="p">,</span>
        <span class="n">l2_regularization_strength</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">l2_regularization_strength</span><span class="p">,</span>
        <span class="n">use_gradient_accumulation</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">use_gradient_accumulation</span><span class="p">,</span>
        <span class="n">clip_weight_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_min</span><span class="p">,</span>
        <span class="n">clip_weight_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_weight_max</span><span class="p">,</span>
        <span class="n">weight_decay_factor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">weight_decay_factor</span><span class="p">,</span>
        <span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="o">=</span><span class="n">p</span>
        <span class="o">.</span><span class="n">multiply_weight_decay_factor_by_learning_rate</span><span class="p">,</span>
        <span class="n">multiply_linear_by_learning_rate</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">multiply_linear_by_learning_rate</span><span class="p">,</span>
        <span class="n">beta</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
        <span class="n">allow_zero_accumulator</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">allow_zero_accumulator</span><span class="p">,</span>
        <span class="n">clip_gradient_min</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_gradient_min</span><span class="p">,</span>
        <span class="n">clip_gradient_max</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">clip_gradient_max</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingFTRLOptimizer.CreateSlotVariablesAndOps"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingFTRLOptimizer.CreateSlotVariablesAndOps">[docs]</a>  <span class="k">def</span> <span class="nf">CreateSlotVariablesAndOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_vars</span><span class="p">,</span> <span class="n">tpu_embedding_table</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="n">load_op_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">retrieve_op_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="n">table_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">table_name</span>
    <span class="n">slot_var_collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">tpu_embedding_table</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">table_var</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_tpu_hosts</span><span class="p">),</span> <span class="n">table_vars</span><span class="p">):</span>
      <span class="c1"># The slot vars should be on the same device as the table var.</span>
      <span class="n">device_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
        <span class="n">accumulator</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">initial_accumulator_value</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">accumulator_name</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/Ftrl&#39;</span><span class="p">)</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span>
            <span class="n">accumulator_name</span><span class="p">,</span> <span class="n">accumulator</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">accumulator_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">accumulator_name</span><span class="p">]</span>

        <span class="n">linear</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">table_var</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span>
            <span class="n">init</span><span class="o">=</span><span class="n">py_utils</span><span class="o">.</span><span class="n">WeightInit</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">initial_linear_value</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">slot_var_collections</span><span class="p">)</span>
        <span class="n">linear_name</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">host_id</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/Ftrl_1&#39;</span>
        <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">linear_name</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">linear_var</span> <span class="o">=</span> <span class="n">tpu_embedding_table</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">linear_name</span><span class="p">]</span>

        <span class="c1"># Only the Trainer needs these ops.</span>
        <span class="k">if</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">():</span>
          <span class="c1"># Remove the slot vars from the variable list to avoid them being</span>
          <span class="c1"># copied to TPU.</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="n">tpu_embedding_table</span><span class="p">,</span> <span class="n">accumulator_name</span><span class="p">)</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="n">tpu_embedding_table</span><span class="p">,</span> <span class="n">linear_name</span><span class="p">)</span>

          <span class="c1"># TPU Embedding load/retrieve ops need to be in the outer graph scope.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;creating load and retrieve ops.&#39;</span><span class="p">)</span>
            <span class="n">load_parameters_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span><span class="o">.</span><span class="n">load_tpu_embedding_ftrl_parameters</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">table_var</span><span class="p">,</span>
                    <span class="n">accumulators</span><span class="o">=</span><span class="n">accumulator_var</span><span class="p">,</span>
                    <span class="n">linears</span><span class="o">=</span><span class="n">linear_var</span><span class="p">,</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">load_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_parameters_op</span><span class="p">)</span>

            <span class="n">retrieved_table</span><span class="p">,</span> <span class="n">retrieved_accumulator</span><span class="p">,</span> <span class="n">retrieved_linear</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">tpu_ops</span>
                <span class="o">.</span><span class="n">retrieve_tpu_embedding_ftrl_parameters</span><span class="p">(</span>
                    <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
                    <span class="n">num_shards</span><span class="o">=</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
                    <span class="n">shard_id</span><span class="o">=</span><span class="n">host_id</span><span class="p">))</span>
            <span class="n">retrieve_parameters_op</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">table_var</span><span class="p">,</span> <span class="n">retrieved_table</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">accumulator_var</span><span class="p">,</span> <span class="n">retrieved_accumulator</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">linear_var</span><span class="p">,</span> <span class="n">retrieved_linear</span><span class="p">))</span>
            <span class="n">retrieve_op_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">retrieve_parameters_op</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">load_op_list</span><span class="p">,</span> <span class="n">retrieve_op_list</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingTable"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingTable</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An embedding table controlled by TPUEmbeddingLayer.</span>

<span class="sd">  Note that all input_keys needs to be declared upfront.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingTable.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the input.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;embedding_dim&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Depth of the output.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;input_keys&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Name of inputs in InputBatch.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;combiner&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Must be &quot;sum&quot;, &quot;sqrtn&quot;, &quot;mean&quot; or None in the case of a &#39;</span>
        <span class="s1">&#39;&quot;sequence embedding &quot;&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;max_sequence_length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;If not None or 0, embedding lookup will return a &#39;</span>
        <span class="s1">&#39;&quot;sequence embedding&quot; of shape &#39;</span>
        <span class="s1">&#39;`[batch, max_sequence_length, embedding_dim]` without applying a &#39;</span>
        <span class="s1">&#39;sequence  reducing combiner&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_tpu_hosts&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of TPU hosts.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Table optimizer parameters. Will override the optimizer parameters &#39;</span>
        <span class="s1">&#39;defined in this table</span><span class="se">\&#39;</span><span class="s1">s TPUEmbeddingLayer.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
             <span class="s1">&#39;Overrides TPUEmbeddingLayer</span><span class="se">\&#39;</span><span class="s1">s learning_rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;lr_schedule&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Overrides TPUEmbeddingLayer</span><span class="se">\&#39;</span><span class="s1">s lr_schedule.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;inference_use_merged_variable&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;Whether to use merged embedding table variable during inference. &#39;</span>
        <span class="s1">&#39;If set to True, only one table variable will be created, and &#39;</span>
        <span class="s1">&#39;the user will need to manually merge the sharded table variables &#39;</span>
        <span class="s1">&#39;in the trained checkpoint before generating the inference graph.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">input_keys</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">combiner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">combiner</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_padded_vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_keys</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">input_keys</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">max_sequence_length</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;schedule&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">lr_schedule</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span> <span class="o">=</span> <span class="n">TpuEmbeddingCollection</span><span class="o">.</span><span class="n">Get</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">LearningRateFn</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GlobalStepContext</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span><span class="o">.</span><span class="n">Value</span><span class="p">()</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">AddSummaryTensor</span><span class="p">(</span>
          <span class="s1">&#39;tpu_embedding_lr/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">),</span> <span class="n">lr</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">lr</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_table_name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_table&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_table_config</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TableConfig</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_padded_vocab_size</span><span class="p">,</span>
        <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
        <span class="n">combiner</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">combiner</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_fn</span><span class="o">=</span><span class="n">LearningRateFn</span><span class="p">,</span>
        <span class="c1"># All TableConfigs passed to API will have a learning rate function,</span>
        <span class="c1"># so the learning_rate in the optimization_parameters is not used.</span>
        <span class="n">optimization_parameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">CreateOptimizerParameters</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">))</span>

<div class="viewcode-block" id="TPUEmbeddingTable._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Reuse the singleton table variables if they were created before.</span>
    <span class="n">all_table_vars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">table_variables</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span> <span class="ow">in</span> <span class="n">all_table_vars</span><span class="p">:</span>
      <span class="n">embedding_table_vars</span> <span class="o">=</span> <span class="n">all_table_vars</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">w_pc</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">WeightParams</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ids_per_shard</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">],</span>
          <span class="n">init</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">params_init</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_vars&#39;</span><span class="p">])</span>

      <span class="n">embedding_table_vars</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">inference_use_merged_variable</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
          <span class="n">var_name</span> <span class="o">=</span> <span class="s1">&#39;merged_var&#39;</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
          <span class="n">embedding_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
          <span class="n">embedding_table_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding_var</span><span class="p">)</span>
          <span class="c1"># Remove from _private_vars / _private_thetas to be added later as wm.</span>
          <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">):</span>
          <span class="n">device_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetDeviceName</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name</span><span class="p">),</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">outside_all_rewrites</span><span class="p">():</span>
            <span class="n">var_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetVariableName</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">CreateVariable</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">w_pc</span><span class="p">)</span>
            <span class="n">embedding_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span>
            <span class="n">embedding_table_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding_var</span><span class="p">)</span>
            <span class="c1"># Remove from _private_vars / _private_thetas to be added later as</span>
            <span class="c1"># wm.</span>
            <span class="n">_RemovePrivateVar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_name</span><span class="p">)</span>

      <span class="c1"># Track the table variables so they can be excluded from EMA.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">AddTableVariables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span>
                                                       <span class="n">embedding_table_vars</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_IsTpuTraining</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="c1"># We don&#39;t need this for TrainerTpu, as the vars are not directly</span>
      <span class="c1"># accessed besides in the TPU embeddding load/retrieve ops.</span>
      <span class="c1"># However, this is needed for CPU (eval/decode/controller).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_private_vars</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_table_vars</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_private_theta</span><span class="p">[</span><span class="s1">&#39;wm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_table_vars</span>

    <span class="c1"># If slot variables and load/retrieve ops were created before, maybe by a</span>
    <span class="c1"># different program or task, don&#39;t create it again.</span>
    <span class="c1"># Note that there should be only one copy of slot variables and</span>
    <span class="c1"># load/retrieve ops in the graph and they&#39;re shared by different</span>
    <span class="c1"># tasks/programs.</span>
    <span class="n">all_load_ops</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">load_ops</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_load_ops</span><span class="p">:</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">retrieve_ops</span>
      <span class="c1"># Only trainer and controller (for checkpointing) need slot variables.</span>
      <span class="c1"># Only trainer needs load/retrieve ops.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
        <span class="n">load_ops</span><span class="p">,</span> <span class="n">retrieve_ops</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">CreateSlotVariablesAndOps</span><span class="p">(</span>
            <span class="n">embedding_table_vars</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">AddLoadRetrieveOps</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span> <span class="n">load_ops</span><span class="p">,</span> <span class="n">retrieve_ops</span><span class="p">)</span></div>

  <span class="c1"># Return device to place sharded variables on.</span>
<div class="viewcode-block" id="TPUEmbeddingTable.GetDeviceName"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.GetDeviceName">[docs]</a>  <span class="k">def</span> <span class="nf">GetDeviceName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">is_inference</span><span class="p">:</span>
      <span class="c1"># This is to place variables on the same device as other variables.</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span><span class="p">:</span>
      <span class="k">return</span> <span class="s1">&#39;/cpu:0&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">/replica:0/task:</span><span class="si">{}</span><span class="s1">/device:CPU:0&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">host_id</span><span class="p">)</span></div>

  <span class="c1"># Return variable name for embedding table shards.</span>
<div class="viewcode-block" id="TPUEmbeddingTable.GetVariableName"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.GetVariableName">[docs]</a>  <span class="k">def</span> <span class="nf">GetVariableName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_id</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;var_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">host_id</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">table_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_config</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">table_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_table_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_keys</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">max_sequence_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_sequence_length</span>

<div class="viewcode-block" id="TPUEmbeddingTable._SequenceEmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable._SequenceEmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">_SequenceEmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dense_ids</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">partition_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sequence embedding lookup.</span>

<span class="sd">    Note that we do not support padding ids in sequence embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">      dense_ids: An int Tensor of shape [batch, sequence].</span>
<span class="sd">      partition_strategy: See TPUEmbeddingLayer partition_strategy param.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A float32 activations Tensor of shape</span>
<span class="sd">      [batch, max_sequence_length, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dense_ids</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">partition_strategy</span><span class="o">=</span><span class="n">partition_strategy</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">dense_ids</span><span class="p">),</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingTable._CombinerEmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable._CombinerEmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">_CombinerEmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_ids</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">,</span>
                         <span class="n">partition_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Combiner embedding lookup.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_ids: An int SparseTensor of shape [batch, ...].</span>
<span class="sd">      partition_strategy: See TPUEmbeddingLayer partition_strategy param.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A float32 activations Tensor of shape [batch, 1, embedding_dim].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup_sparse</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">wm</span><span class="p">,</span>
        <span class="n">sparse_ids</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>  <span class="c1"># sp_weights</span>
        <span class="n">combiner</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">combiner</span><span class="p">,</span>
        <span class="n">partition_strategy</span><span class="o">=</span><span class="n">partition_strategy</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">sparse_ids</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># For tf.nn.embedding_lookup_sparse, output.dim0 might be different from</span>
    <span class="c1"># sparse_ids.dense_shape.dim0.</span>
    <span class="c1"># Explicitly pad results to maintain dim0=batch.</span>
    <span class="n">dim0_padlen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">embs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim0_padlen</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="c1"># [batch, 1, embedding_dim]</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">HasShape</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingTable.CpuEmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.CpuEmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">CpuEmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">,</span>
                   <span class="n">partition_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup for dense tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A NestedMap of nested `input_key` string -&gt; [batch, sequence]</span>
<span class="sd">        int Tensor. For sequence embeddings, -1 is used as a padding id.</span>
<span class="sd">        Non-sequence embeddings do not support padded ids.</span>
<span class="sd">      partition_strategy: See TPUEmbeddingLayer partition_strategy param.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An activations NestedMap of nested string -&gt; float32 Tensor.</span>
<span class="sd">      For non-sequence embeddings: [batch, 1, embedding_dim]</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># &quot;Sequence embedding&quot;, no combiner case</span>
      <span class="k">return</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">ids</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SequenceEmbLookup</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">partition_strategy</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Non-&quot;Sequence embedding&quot;, combiner case</span>
      <span class="k">def</span> <span class="nf">_Lookup</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
        <span class="c1"># Dense to sparse.</span>
        <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">embedding_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">sample_indices</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="c1"># [?, embedding_dim]</span>
        <span class="n">sparse_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">sample_indices</span><span class="p">,</span>
            <span class="n">values</span><span class="o">=</span><span class="n">embedding_indices</span><span class="p">,</span>
            <span class="n">dense_shape</span><span class="o">=</span><span class="n">dense_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CombinerEmbLookup</span><span class="p">(</span><span class="n">sparse_ids</span><span class="p">,</span> <span class="n">partition_strategy</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">_Lookup</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingTable.CpuEmbLookupSparse"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingTable.CpuEmbLookupSparse">[docs]</a>  <span class="k">def</span> <span class="nf">CpuEmbLookupSparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">,</span>
                         <span class="n">partition_strategy</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup for SparseTensors.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A NestedMap of nested `input_key` string -&gt; [batch, ...] int</span>
<span class="sd">        SparseTensor.</span>
<span class="sd">      partition_strategy: See TPUEmbeddingLayer partition_strategy param.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An activations NestedMap of nested string -&gt; float32 Tensor.</span>
<span class="sd">      For non-sequence embeddings: [batch, 1, embedding_dim]</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># &quot;Sequence embedding&quot;, no combiner case</span>
      <span class="k">def</span> <span class="nf">_Lookup</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
        <span class="c1"># Sparse to dense.</span>
        <span class="n">dense_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SequenceEmbLookup</span><span class="p">(</span><span class="n">dense_ids</span><span class="p">,</span> <span class="n">partition_strategy</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">_Lookup</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Non-&quot;Sequence embedding&quot;, combiner case</span>
      <span class="k">return</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">ids</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CombinerEmbLookup</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">partition_strategy</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="TPUEmbeddingLayer"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer">[docs]</a><span class="k">class</span> <span class="nc">TPUEmbeddingLayer</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">BaseLayer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Monolithic interface to TPU embedding.</span>

<span class="sd">  This layer has some important caveats, due to the interface of the</span>
<span class="sd">  TPU embedding hardware. Its behavior most closely mimics that of</span>
<span class="sd">  tf.nn.embedding_lookup_sparse.</span>

<span class="sd">  Supports multiple tables and multiple input_keys per table.</span>
<span class="sd">  Requires its own optimizer parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TPUEmbeddingLayer.Params"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer.Params">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">Params</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;num_tpu_hosts&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Total number of TPU hosts.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;tables&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;TPUEmbeddingTables&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;pipeline_execution_with_tensor_core&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
             <span class="s1">&#39;Set to True to be faster. See tpu_embedding.py for details.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Per-core batch size.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="n">TPUEmbeddingAdagradOptimizer</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Layer optimizer parameters. Will be used for any TPUEmbeddingTables &#39;</span>
        <span class="s1">&#39;with None optimizer parameters.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Learning rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;lr_schedule&#39;</span><span class="p">,</span> <span class="n">schedule</span><span class="o">.</span><span class="n">ContinuousSchedule</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Lingvo learning rate schedule. Will be multiplied to learning rate.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;partition_strategy&#39;</span><span class="p">,</span> <span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="s1">&#39;A string, either &quot;mod&quot; or &quot;div&quot;, &#39;</span>
        <span class="s1">&#39;specifying how to map the lookup id to the embedding tensor. For &#39;</span>
        <span class="s1">&#39;more information see `tf.nn.embedding_lookup_sparse`.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
        <span class="s1">&#39;gradient_multiplier_schedule&#39;</span><span class="p">,</span> <span class="n">schedule</span><span class="o">.</span><span class="n">ConstantOne</span><span class="o">.</span><span class="n">Params</span><span class="p">(),</span>
        <span class="s1">&#39;Values from this schedule will be multiplied to the embedding &#39;</span>
        <span class="s1">&#39;gradients. Gradients from Tensorcore will not be affected.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span></div>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">gradient_multiplier_schedule</span>
    <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;mod&#39;</span><span class="p">,</span> <span class="s1">&#39;div&#39;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">table_params</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">table_params</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
        <span class="k">if</span> <span class="n">num_tpu_hosts</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">num_tpu_hosts</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="sa">f</span><span class="s1">&#39;num_tpu_hosts mismatch: </span><span class="si">{</span><span class="n">num_tpu_hosts</span><span class="si">}</span><span class="s1"> vs </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">table_params</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">num_tpu_hosts</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_tpu_hosts</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">num_tpu_hosts</span> <span class="o">==</span> <span class="n">num_tpu_hosts</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">])</span>

    <span class="c1"># Stop if a table has no optimizer related parameters and the layer also</span>
    <span class="c1"># has no optimizer parameters</span>
    <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_schedule&#39;</span><span class="p">]:</span>
      <span class="n">table_param_missing</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
          <span class="n">table_params</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">table_params</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">table_param_missing</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;A table is missing </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1"> parameters, and no layer-level &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1"> parameters were given.&#39;</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">table_param_missing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">table_params</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">table_params</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
              <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>  <span class="c1"># Avoid mutating the original copy.</span>
            <span class="n">table_params</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChildren</span><span class="p">(</span><span class="s1">&#39;tables&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">tables</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">CreateChild</span><span class="p">(</span><span class="s1">&#39;gradient_multiplier_schedule&#39;</span><span class="p">,</span>
                     <span class="n">p</span><span class="o">.</span><span class="n">gradient_multiplier_schedule</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span> <span class="o">=</span> <span class="n">TpuEmbeddingCollection</span><span class="o">.</span><span class="n">Get</span><span class="p">()</span>

    <span class="c1"># Save embedding feature names in the collection.</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">feature_names</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Input key </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s1"> was used by multiple tables.&#39;</span><span class="p">)</span>
        <span class="n">feature_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_names</span>

<div class="viewcode-block" id="TPUEmbeddingLayer._CreateChildrenVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._CreateChildrenVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateChildrenVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Backwards compatibility: manually call child.InstantiateVariables()</span>
    <span class="c1"># outside of tf.variable_scope(p.name).</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
      <span class="n">table</span><span class="o">.</span><span class="n">InstantiateVariables</span><span class="p">()</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateChildrenVariables</span><span class="p">()</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer._CheckTPUEmbeddingConfig"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._CheckTPUEmbeddingConfig">[docs]</a>  <span class="k">def</span> <span class="nf">_CheckTPUEmbeddingConfig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tpu_embedding</span><span class="p">,</span> <span class="n">table_to_config_dict</span><span class="p">,</span>
                               <span class="n">feature_to_config_dict</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check that the existing tpu_embedding config matches the given ones.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_Match</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">namedtuple_attrs_to_check</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">d1</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">d2</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v1</span> <span class="ow">in</span> <span class="n">d1</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d2</span><span class="p">:</span>
          <span class="k">return</span> <span class="kc">False</span>
        <span class="n">v2</span> <span class="o">=</span> <span class="n">d2</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">namedtuple_attrs_to_check</span><span class="p">:</span>
          <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
      <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># We just check numeric/string settings for simplicity, this excludes things</span>
    <span class="c1"># like learning_rate_fn, optimization_parameters, etc since it&#39;s hard to</span>
    <span class="c1"># compare them.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_Match</span><span class="p">(</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">table_to_config_dict</span><span class="p">,</span> <span class="n">table_to_config_dict</span><span class="p">,</span>
                  <span class="p">[</span><span class="s1">&#39;vocabulary_size&#39;</span><span class="p">,</span> <span class="s1">&#39;dimension&#39;</span><span class="p">,</span> <span class="s1">&#39;combiner&#39;</span><span class="p">]):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;table_to_config_dict mismatch. &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;Expecting </span><span class="si">{</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">table_to_config_dict</span><span class="si">}</span><span class="s1">, &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;got </span><span class="si">{</span><span class="n">table_to_config_dict</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_Match</span><span class="p">(</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">feature_to_config_dict</span><span class="p">,</span> <span class="n">feature_to_config_dict</span><span class="p">,</span>
                  <span class="p">[</span><span class="s1">&#39;table_id&#39;</span><span class="p">,</span> <span class="s1">&#39;max_sequence_length&#39;</span><span class="p">]):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;feature_to_config_dict mismatch. &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;Expecting </span><span class="si">{</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">feature_to_config_dict</span><span class="si">}</span><span class="s1">, &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;got </span><span class="si">{</span><span class="n">feature_to_config_dict</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">batch_size_per_core</span> <span class="o">*</span> <span class="n">tpu_embedding</span><span class="o">.</span><span class="n">num_cores</span> <span class="o">!=</span>
        <span class="n">global_batch_size</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;global_batch_size mismatch. &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;batch_size_per_core: </span><span class="si">{</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">batch_size_per_core</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;num_cores: </span><span class="si">{</span><span class="n">tpu_embedding</span><span class="o">.</span><span class="n">num_cores</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;global_batch_size: </span><span class="si">{</span><span class="n">global_batch_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer._CreateLayerVariables"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._CreateLayerVariables">[docs]</a>  <span class="k">def</span> <span class="nf">_CreateLayerVariables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_CreateLayerVariables</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># At the feature level, track which are associated</span>
    <span class="c1"># with &quot;sequence embeddings&quot;.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">_IsTpuTraining</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="n">num_cores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">tpus_per_replica</span>
      <span class="n">global_batch_size</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">num_splits_per_client</span><span class="p">)</span>
      <span class="n">table_to_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="n">feature_to_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_to_config_dict</span><span class="p">[</span><span class="n">table</span><span class="o">.</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">table_config</span>
        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">table</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="n">feature_to_config_dict</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">FeatureConfig</span><span class="p">(</span>
              <span class="n">table</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="o">=</span><span class="n">table</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">)</span>

      <span class="n">tpu_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">tpu_embedding</span>
      <span class="k">if</span> <span class="n">tpu_embedding</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_CheckTPUEmbeddingConfig</span><span class="p">(</span><span class="n">tpu_embedding</span><span class="p">,</span> <span class="n">table_to_config_dict</span><span class="p">,</span>
                                      <span class="n">feature_to_config_dict</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="p">)</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;TPUEmbedding API singleton already exists, reusing&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="n">tpu_embedding</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TRAINING</span>
        <span class="n">device_config</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">DeviceConfig</span><span class="p">(</span>
            <span class="n">num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
            <span class="n">num_hosts</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_tpu_hosts</span><span class="p">,</span>
            <span class="n">job_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span> <span class="o">=</span> <span class="n">tpu_embedding_lib</span><span class="o">.</span><span class="n">TPUEmbedding</span><span class="p">(</span>
            <span class="n">table_to_config_dict</span><span class="p">,</span>
            <span class="n">feature_to_config_dict</span><span class="p">,</span>
            <span class="n">global_batch_size</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">,</span>
            <span class="n">master</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">pipeline_execution_with_tensor_core</span><span class="o">=</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">pipeline_execution_with_tensor_core</span><span class="p">),</span>
            <span class="n">partition_strategy</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span><span class="p">,</span>
            <span class="n">device_config</span><span class="o">=</span><span class="n">device_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">tpu_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">SetGradientMultiplierSchedule</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_multiplier_schedule</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer._TpuEmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer._TpuEmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">_TpuEmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;TPU Embedding lookup.&quot;&quot;&quot;</span>
    <span class="n">task_call_scope</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">GetTaskCallScope</span><span class="p">()</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tpu_embedding_collection</span><span class="o">.</span><span class="n">AddActivations</span><span class="p">(</span><span class="n">task_call_scope</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sequence_features</span><span class="p">:</span>
          <span class="n">ret</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Non-sequence embeddings, we fill the &quot;time&quot; dimension with 1.</span>
          <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">ret</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer.EmbLookup"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer.EmbLookup">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for each entry in dense Tensor ids_map.</span>

<span class="sd">    Since the TPUEmbedding is monolothic, and consulted once per</span>
<span class="sd">    FProp/BProp, we must centralize the lookup. Thus, for multiple</span>
<span class="sd">    features, we contain them into a single-lookup rather than allowing</span>
<span class="sd">    the caller to call Lookup multiple times.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A NestedMap of nested `input_key` string -&gt; [batch, sequence] int</span>
<span class="sd">        Tensor.</span>
<span class="sd">        For sequence embeddings, -1 is used as a padding id. Non-sequence</span>
<span class="sd">        embeddings do not support padded ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Activations NestedMap of nested string -&gt;</span>
<span class="sd">      For non-sequence embeddings:  [batch, 1, embedding_dim],</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>
<span class="sd">      float32 Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids_map</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">CpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup.&quot;&quot;&quot;</span>
      <span class="n">rets</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_id_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">table_id_map</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="n">table_rets</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">CpuEmbLookup</span><span class="p">(</span><span class="n">table_id_map</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span><span class="p">)</span>
        <span class="c1"># Merge table_rets with rets</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rets</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">table_rets</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">rets</span>

    <span class="k">if</span> <span class="n">_IsTpuTraining</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_TpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">CpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span></div>

<div class="viewcode-block" id="TPUEmbeddingLayer.EmbLookupSparse"><a class="viewcode-back" href="../../../lingvo.core.tpu_embedding_layers.html#lingvo.core.tpu_embedding_layers.TPUEmbeddingLayer.EmbLookupSparse">[docs]</a>  <span class="k">def</span> <span class="nf">EmbLookupSparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids_map</span><span class="p">:</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Looks up embedding vectors for each entry in SparseTensor ids_map.</span>

<span class="sd">    Since the TPUEmbedding is monolothic, and consulted once per</span>
<span class="sd">    FProp/BProp, we must centralize the lookup. Thus, for multiple</span>
<span class="sd">    features, we contain them into a single-lookup rather than allowing</span>
<span class="sd">    the caller to call Lookup multiple times.</span>

<span class="sd">    Args:</span>
<span class="sd">      ids_map: A NestedMap of nested `input_key` string -&gt; [batch, ...] int</span>
<span class="sd">      SparseTensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Activations NestedMap of nested string -&gt;</span>
<span class="sd">      For non-sequence embeddings:  [batch, 1, embedding_dim],</span>
<span class="sd">      For sequence embeddings: [batch, max_sequence_length, embedding_dim]</span>
<span class="sd">      float32 Tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids_map</span><span class="p">,</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span> <span class="nf">CpuEmbLookupSparse</span><span class="p">(</span><span class="n">ids_map</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;CPU evaluation embedding lookup.&quot;&quot;&quot;</span>
      <span class="n">rets</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_id_map</span> <span class="o">=</span> <span class="n">py_utils</span><span class="o">.</span><span class="n">NestedMap</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">table_id_map</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="n">table_rets</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">CpuEmbLookupSparse</span><span class="p">(</span><span class="n">table_id_map</span><span class="p">,</span>
                                              <span class="n">p</span><span class="o">.</span><span class="n">partition_strategy</span><span class="p">)</span>
        <span class="c1"># Merge table_rets with rets</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">input_keys</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">ids_map</span><span class="o">.</span><span class="n">Get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rets</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">table_rets</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">rets</span>

    <span class="k">if</span> <span class="n">_IsTpuTraining</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_TpuEmbLookup</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">CpuEmbLookupSparse</span><span class="p">(</span><span class="n">ids_map</span><span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>