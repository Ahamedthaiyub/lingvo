<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lingvo.core.attention_util module &mdash; Lingvo  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.core.base_decoder module" href="lingvo.core.base_decoder.html" />
    <link rel="prev" title="lingvo.core.attention module" href="lingvo.core.attention.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Lingvo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="lingvo.core.html#subpackages">Subpackages</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.core.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Lingvo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
          <li><a href="lingvo.core.html">lingvo.core package</a> &raquo;</li>
      <li>lingvo.core.attention_util module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lingvo.core.attention_util.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-lingvo.core.attention_util">
<span id="lingvo-core-attention-util-module"></span><h1>lingvo.core.attention_util module<a class="headerlink" href="#module-lingvo.core.attention_util" title="Permalink to this headline"></a></h1>
<p>Attention related utils, e.g. relative positional embeddings.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.ConvertToBlocks">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">ConvertToBlocks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#ConvertToBlocks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.ConvertToBlocks" title="Permalink to this definition"></a></dt>
<dd><p>Turns a sequence to non overlapping blocks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – a tensor of [batch, time, …].</p></li>
<li><p><strong>block_size</strong> – int. Number of time frames in a block.</p></li>
<li><p><strong>padding_val</strong> – float. value on the padded frames.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of [batch, num_blocks, block_size, …], with necessary paddings,
where output[:, i, …] are x[:, i*block_size:(i+1)*block_size, …].</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.ExtractBlockContext">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">ExtractBlockContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left_context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">right_context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#ExtractBlockContext"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.ExtractBlockContext" title="Permalink to this definition"></a></dt>
<dd><p>Extracts temporal context for every block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – a tensor of [batch, time, …].</p></li>
<li><p><strong>block_size</strong> – int. Number of time frames in a block.</p></li>
<li><p><strong>left_context</strong> – int. Left context size.</p></li>
<li><p><strong>right_context</strong> – int. Right context size.</p></li>
<li><p><strong>padding_val</strong> – float. value on the padded frames.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of [batch, num_blocks, context_size, …], with necessary paddings,
where context_size = block_size + (left_context - 1) + right_context,
and output[:, i, …] are x[:, start-left_context+1:end+right_context, …],
start = i * block_size, end = (i + 1) * block_size.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.MakeLocalMask">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">MakeLocalMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left_context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">right_context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">tf.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#MakeLocalMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.MakeLocalMask" title="Permalink to this definition"></a></dt>
<dd><p>Makes the mask tensor for a full sequence.</p>
<p>The returned mask reflects the given context sizes, where position i
attends to tokens in the range [i - (left_context-1), i + right_context].</p>
<p>For example, given seq_len=4, block_size=2, left_context=3, right_context=0,
the result mask is
[[[0., 0., 1., 0.], 1st query in 1st block attends 1st key.
[0., 0., 1., 1.]],  2nd query in 1st block attends 2nd and left keys
[[1., 1., 1., 0.],  1st query in 2nd block attends 1st and left keys
[0., 1., 1., 1.]]]  2st query in 2nd block attends 2nd and left keys</p>
<p>The position i can move by stride, which means queries are pooled by stride.
For example, given same params and stride=2, the result mask is
[[[0., 0., 1., 1.]], The pooled query in 1st block attends 1st and 2nd keys
[[1., 1., 1., 1.]]]  The pooled query in 2st block attends 1st, 2nd and left</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq_len</strong> – int or scalar int tensor. Sequence length.</p></li>
<li><p><strong>block_size</strong> – int. Number of time frames in a block.</p></li>
<li><p><strong>left_context</strong> – int. Left context size.</p></li>
<li><p><strong>right_context</strong> – int. Right context size.</p></li>
<li><p><strong>query_stride</strong> – int. Query stride for funnel pool.</p></li>
<li><p><strong>dtype</strong> – tf.dtype, default is tf.float32.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of [num_blocks, block_size//stride, context_size] taking values in
{0, 1}, where context_size = block_size + (left_context - 1) + right_context
Element b, i, j is 1 if in the b-th block, the i-th frame can access
the j-th frame in the context.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.RelShift">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">RelShift</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#RelShift"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.RelShift" title="Permalink to this definition"></a></dt>
<dd><p>Performs relative shift on 4D tensor (first 2 axis are batching dims).</p>
<p>Given input of shape [?, ?, W, W], this does “relative shifting” for the
last two dims, s.t. output[b, n, i, j] = 0 if i &gt; j else input[b, n, i, j-i]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – A Tensor of shape [?, ?, W, W]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of the same shape as input with its content shifted (as described
above).</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.AttenLogits">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">AttenLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#AttenLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.AttenLogits" title="Permalink to this definition"></a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.AttenContext">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">AttenContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#AttenContext"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.AttenContext" title="Permalink to this definition"></a></dt>
<dd><p>Computes the attention context vector based on per-head probs and value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> – [B, N, T, S].</p></li>
<li><p><strong>value</strong> – [B, S, N, H].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">PositionalAttenLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.quant_utils.html#lingvo.core.quant_utils.QuantizableLayer" title="lingvo.core.quant_utils.QuantizableLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.quant_utils.QuantizableLayer</span></code></a></p>
<p>Implementation of the positional attention logit computation from …</p>
<ul class="simple">
<li><p>‘Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context’
<a class="reference external" href="https://arxiv.org/pdf/1901.02860.pdf">https://arxiv.org/pdf/1901.02860.pdf</a> section 3.3</p></li>
<li><p>‘Self-Attention with Relative Position Representations’
<a class="reference external" href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a> section 3</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits.RelPositionBias">
<span class="sig-name descname"><span class="pre">RelPositionBias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">content</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_term_b</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits.RelPositionBias"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits.RelPositionBias" title="Permalink to this definition"></a></dt>
<dd><p>Compute relative position bias.</p>
<p>This is a subroutine used by variants of self-attentions with relative
positional embedding.</p>
<p>output[b][n][i][j] = content[b][i][n] x abs_pos_emb[i-j+T-1][n]</p>
<p>Padding should be masked by the caller of this function.</p>
<p>B: batch size
T: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shapes</strong> (<em>tensors of the following</em>) – </p></li>
<li><p><strong>content</strong> – [N, H] if skip_term_b else [B, T, N, H]</p></li>
<li><p><strong>abs_pos_emb</strong> – [2T - 1, N, H], the absolute positional embedding.
abs_pos_emb[i] is the emb of relative distance i - (T-1).</p></li>
<li><p><strong>skip_term_b</strong> – If to skip term_b in section 3.3 equation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The attention logits tensor. [N, T, T] if skip_term_b else [B, N, T, T].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits._ValidateBiases">
<span class="sig-name descname"><span class="pre">_ValidateBiases</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">content_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits._ValidateBiases"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits._ValidateBiases" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits._AttenLogits">
<span class="sig-name descname"><span class="pre">_AttenLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">content_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_term_b</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits._AttenLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits._AttenLogits" title="Permalink to this definition"></a></dt>
<dd><p>Attention logits from TransformerXL and Self Attention RPE.</p>
<p>Padding should be masked by the caller of this function.</p>
<p>B: batch size
T: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shapes</strong> (<em>tensors of the following</em>) – </p></li>
<li><p><strong>query</strong> – [B, T, N, H]</p></li>
<li><p><strong>key</strong> – [B, T, N, H]</p></li>
<li><p><strong>abs_pos_emb</strong> – [2T - 1, N, H]. The sinusoid positional embedding from
‘Attention Is All You Need’ (<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>).
abs_pos_emb[i] is the emb of relative distance i - (T-1).</p></li>
<li><p><strong>content_bias</strong> – [N, H] or None</p></li>
<li><p><strong>positional_bias</strong> – [N, H] or None</p></li>
<li><p><strong>skip_term_b</strong> – If to skip term_b in section 3.3 equation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The attention logits tensor. [B, N, T, T]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsXL">
<span class="sig-name descname"><span class="pre">AttenLogitsXL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">content_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_term_b</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits.AttenLogitsXL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsXL" title="Permalink to this definition"></a></dt>
<dd><p>Attention logits from Transformer-XL.</p>
<p>Transformer-XL(<a class="reference external" href="https://arxiv.org/pdf/1901.02860.pdf">https://arxiv.org/pdf/1901.02860.pdf</a>, section 3.3) version of
self attention with relative position embedding.</p>
<p>Padding should be masked by the caller of this function.</p>
<p>B: batch size
T: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shapes</strong> (<em>tensors of the following</em>) – </p></li>
<li><p><strong>query</strong> – [B, T, N, H]</p></li>
<li><p><strong>key</strong> – [B, T, N, H]</p></li>
<li><p><strong>abs_pos_emb</strong> – [2T - 1, N, H]. The sinusoid positional embedding from
‘Attention Is All You Need’ (<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>).
abs_pos_emb[i] is the emb of relative distance i - (T-1).</p></li>
<li><p><strong>content_bias</strong> – [N, H] or None</p></li>
<li><p><strong>positional_bias</strong> – [N, H] or None</p></li>
<li><p><strong>skip_term_b</strong> – If to skip term_b in section 3.3 equation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The attention logits tensor. [B, N, T, T]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsRPE">
<span class="sig-name descname"><span class="pre">AttenLogitsRPE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits.AttenLogitsRPE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsRPE" title="Permalink to this definition"></a></dt>
<dd><p>Attention logits for Relative Position Representations.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a> with trainable rel position emb.</p>
<p>Padding should be masked by the caller of this function.</p>
<p>B: batch size
T: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shapes</strong> (<em>tensors of the following</em>) – </p></li>
<li><p><strong>query</strong> – [B, T, N, H]</p></li>
<li><p><strong>key</strong> – [B, T, N, H]</p></li>
<li><p><strong>abs_pos_emb</strong> – [2T - 1, N, H]. The trainable embdding. abs_pos_emb[i] is
the emb of relative distance i - (T-1).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The attention logits tensor. [B, N, T, T]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsXLOneStep">
<span class="sig-name descname"><span class="pre">AttenLogitsXLOneStep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">content_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_term_b</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits.AttenLogitsXLOneStep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsXLOneStep" title="Permalink to this definition"></a></dt>
<dd><p>Transformer-XL attention logits for one single target (query) step.</p>
<p>B: batch size
S: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>abs_pos_emb</strong> – [B, S, N, H] or [S, N, H]</p></li>
<li><p><strong>content_bias</strong> – [N, H] or None</p></li>
<li><p><strong>positional_bias</strong> – [N, H] or None</p></li>
<li><p><strong>skip_term_b</strong> – If to skip term_b in section 3.3 equation of the
TransformerXL paper.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsRPEOneStep">
<span class="sig-name descname"><span class="pre">AttenLogitsRPEOneStep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abs_pos_emb</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#PositionalAttenLogits.AttenLogitsRPEOneStep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.PositionalAttenLogits.AttenLogitsRPEOneStep" title="Permalink to this definition"></a></dt>
<dd><p>RPE attention logits for one single target (query) step.</p>
<p>B: batch size
S: sequence length
N: num of attention heads.
H: per-head attention dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>abs_pos_emb</strong> – [S, 1, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lingvo.core.attention_util.KMeansClusteringForAtten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">KMeansClusteringForAtten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#KMeansClusteringForAtten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.KMeansClusteringForAtten" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Implements k-means clustering with mini-batch updates.</p>
<p>This is used in the implementation of <a class="reference external" href="https://arxiv.org/pdf/2003.05997">https://arxiv.org/pdf/2003.05997</a>.</p>
<dl class="simple">
<dt>We use the following capital letters to denote shape parameters:</dt><dd><p>B = batch size
L = length of the input sequence (referred to as S or T elsewhere)
N = number of attention heads
H = dimensions of each attention head
K = number of clusters</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.KMeansClusteringForAtten.Params">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#KMeansClusteringForAtten.Params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.KMeansClusteringForAtten.Params" title="Permalink to this definition"></a></dt>
<dd><p>Params.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.KMeansClusteringForAtten._CreateLayerVariables">
<span class="sig-name descname"><span class="pre">_CreateLayerVariables</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#KMeansClusteringForAtten._CreateLayerVariables"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.KMeansClusteringForAtten._CreateLayerVariables" title="Permalink to this definition"></a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.KMeansClusteringForAtten.LayerNorm">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#KMeansClusteringForAtten.LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.KMeansClusteringForAtten.LayerNorm" title="Permalink to this definition"></a></dt>
<dd><p>Performs layer normalization on the last dimension of ‘x’.</p>
<p>This differs from layers.LayerNorm in that it fixes both scale and bias at
0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – An input tensor to be normalized.</p></li>
<li><p><strong>epsilon</strong> – Tiny value used to guard against rsqrt of 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>‘x’ with its last dimension normalized.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.attention_util.KMeansClusteringForAtten.FProp">
<span class="sig-name descname"><span class="pre">FProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">paddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#KMeansClusteringForAtten.FProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.KMeansClusteringForAtten.FProp" title="Permalink to this definition"></a></dt>
<dd><p>Computes distances of the given input ‘x’ to all centroids.</p>
<p>This implementation applies layer normalization on ‘x’ internally first,
and the returned ‘dists’ is computed using the normalized ‘x’.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of weights’ values of this layer.</p></li>
<li><p><strong>x</strong> – A tensor of shape [B, L, N, H].</p></li>
<li><p><strong>paddings</strong> – If not None, a tensor of shape [B, L].</p></li>
<li><p><strong>update</strong> – bool, whether to update centroids using x.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>“distances” of the given input ‘x’ to all centroids.</dt><dd><p>Shape [B, L, N, K].</p>
</dd>
<dt>k_means_loss: the average squared Euclidean distances to the closest</dt><dd><p>centroid, a scalar.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dists</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lingvo.core.attention_util.ComputeSparseAttention">
<span class="sig-prename descclassname"><span class="pre">lingvo.core.attention_util.</span></span><span class="sig-name descname"><span class="pre">ComputeSparseAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">paddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/attention_util.html#ComputeSparseAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.attention_util.ComputeSparseAttention" title="Permalink to this definition"></a></dt>
<dd><p>Computes attention according to a sparsity pattern.</p>
<dl class="simple">
<dt>We use the following capital letters to denote shape parameters:</dt><dd><p>B = batch size
S = length of the source sequence
T = length of the target sequence
N = number of attention heads
H = dimensions of each attention head
K = number of clusters
W = attention window (K &lt;= S)</p>
</dd>
</dl>
<p>The ‘sparsity_indices’ is a tensor of integral type where the last dimension
contains W indices (W is the attention window) for each corresponding position
along S in ‘k’ that the query is allowed to attend to.</p>
<p>For example, if sparsity_indices[batch_idx, target time step, head_idx] =
[1, 7, 8], it means that token in the query attends to values with indices
1, 7, and 8, and the attention window here is 3.</p>
<p>The valid values in ‘sparsity_indices’ are [-1, S-1]. Note that the value -1
is reserved to mean paddings, distinct from the value (S-1).</p>
<p>For example, if W=S and ‘sparsity_indices’ contains range(S) on the last
dimension, this degenerates to the original full attention.</p>
<p>We require that ‘sparsity_indices’ does not contain duplicates (except for -1
to indicate paddings), but we do not require ‘sparsity_indices’ to be sorted.</p>
<p>Note that this implementation is flexible and generic but is not optimized for
time or space complexity. Please consider grouping queries that attend to the
same subset of values first for efficiency.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>q</strong> – (projected) queries, [B, T, N, H];</p></li>
<li><p><strong>k</strong> – (projected) keys, [B, S, N, H];</p></li>
<li><p><strong>v</strong> – (projected) values, [B, S, N, H];</p></li>
<li><p><strong>sparsity_indices</strong> – [B, T, N, W], where W is the attention window;</p></li>
<li><p><strong>paddings</strong> – paddings for keys, [B, S] if not None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the encoded output, [B, T, N, H].
atten_probs: the attention weights, [B, T, N, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lingvo.core.attention.html" class="btn btn-neutral float-left" title="lingvo.core.attention module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lingvo.core.base_decoder.html" class="btn btn-neutral float-right" title="lingvo.core.base_decoder module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>