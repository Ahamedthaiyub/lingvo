

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.batch_major_attention module &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.core.batch_utils module" href="lingvo.core.batch_utils.html" />
    <link rel="prev" title="lingvo.core.base_model_params module" href="lingvo.core.base_model_params.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="lingvo.core.html#subpackages">Subpackages</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.core.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
        
          <li><a href="lingvo.core.html">lingvo.core package</a> &raquo;</li>
        
      <li>lingvo.core.batch_major_attention module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/lingvo.core.batch_major_attention.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-lingvo.core.batch_major_attention">
<span id="lingvo-core-batch-major-attention-module"></span><h1>lingvo.core.batch_major_attention module<a class="headerlink" href="#module-lingvo.core.batch_major_attention" title="Permalink to this headline">¶</a></h1>
<p>Multi-headed attention layers for Transformer machine translation.</p>
<dl class="simple">
<dt>[1] Attention is all you need.</dt><dd><p><a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a> Section 3.</p>
</dd>
</dl>
<dl class="py function">
<dt id="lingvo.core.batch_major_attention.CausalSegmentMask">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">CausalSegmentMask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">segment_ids</span></em>, <em class="sig-param"><span class="n">dtype</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#CausalSegmentMask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.CausalSegmentMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the masks which combines causal masking and segment masks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>segment_ids</strong> – a tensor of shape [b, slen], the segment that each token
belongs to.</p></li>
<li><p><strong>dtype</strong> – tf dtype.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape [b, 1, slen, slen].</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.CausalPadding">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">CausalPadding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">slen</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#CausalPadding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.CausalPadding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.GetDtypeMin">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">GetDtypeMin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GetDtypeMin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GetDtypeMin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.SegmentMask">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">SegmentMask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">segment_id</span></em>, <em class="sig-param"><span class="n">source_segment_id</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">apply_dtype_min</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#SegmentMask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.SegmentMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates a segment mask for attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>segment_id</strong> – [B, T]</p></li>
<li><p><strong>source_segment_id</strong> – [B, S]</p></li>
<li><p><strong>dtype</strong> – data type of generated mask.</p></li>
<li><p><strong>apply_dtype_min</strong> – Outputs a 0/1 padding mask if set to False. This is needed
for GPipe layers to avoid nan issues.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, T, S]: A mask that is ready to
be added to [B, N, T, S] attention logits. if apply_dtype_min is False,
outputs a 0/1 padding mask instead.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>segment_mask</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.PerDimScaleLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">PerDimScaleLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PerDimScaleLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PerDimScaleLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer to scale individual dims of the input.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PerDimScaleLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PerDimScaleLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PerDimScaleLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for <a class="reference internal" href="#lingvo.core.batch_major_attention.PerDimScaleLayer" title="lingvo.core.batch_major_attention.PerDimScaleLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerDimScaleLayer</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PerDimScaleLayer._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PerDimScaleLayer._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PerDimScaleLayer._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PerDimScaleLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PerDimScaleLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PerDimScaleLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Return theta.scale * inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>inputs</strong> – 4D tensor with shape […, p.dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>4D tensor with shape […, p.dim]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>outpus</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PerDimScaleLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PerDimScaleLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PerDimScaleLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.PerDimScaleLayer.FProp" title="lingvo.core.batch_major_attention.PerDimScaleLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.PerDimScaleLayer.FProp" title="lingvo.core.batch_major_attention.PerDimScaleLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiHeadedProjectionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiHeadedProjectionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedProjectionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Layer that computes multi heads projection.</p>
<p>This layer is expected to be used within MultiHeadedAttention below.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedProjectionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for MultiHeadedProjectionLayer.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedProjectionLayer._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedProjectionLayer._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedProjectionLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedProjectionLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the multi headed projection for inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>inputs</strong> – A tensor of shape [batch_size, time_steps, num_heads,
dim_per_head] or [batch_size, time_steps, hidden_size].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The projected tensor with shape [[batch_size, time_steps, hidden_size] or
[batch_size, time_steps, num_heads, dim_per_head].</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Dot-product attention with multiple attention heads.</p>
<p>This implementation heavily uses einsum (wrapped in py_utils.Einsum) to be
efficient on TPUs.  We use the following capital letters to denote certain
tensor parameters.</p>
<blockquote>
<div><p>B = batch size
S = length of the key/value (source)
T = length of the query (target)
D = model dimension
N = number of attention heads
H = dimensions of each attention head.</p>
</div></blockquote>
<p>The algorithm is sketched as follows. Each intermediate tensor or weight
tensor is annotated with its shape. E.g., Wq, the weight tensor for query’s
projection, its shape is [D, N, H].</p>
<dl class="simple">
<dt>Trainable weights:</dt><dd><p>Wq, Wk, Wv: [D, N, H]
Wout: [D, N, H]</p>
</dd>
</dl>
<p>Input q:[B, T, D]; k:[B, S, D]; v:[B, S, D]
q_proj:[B, T, N, H] = einsum(‘BTD,DNH-&gt;BTNH’, x, Wq)
k_proj:[B, S, N, H] = einsum(‘BSD,DNH-&gt;BSNH’, x, Wk)
v_proj:[B, S, N, H] = einsum(‘BSD,DNH-&gt;BSNH’, x, Wv)
logits:[B, N, T, S] = einsum(‘BTNH,BSNH-&gt;BNTS’, q_proj, k_proj) / sqrt(H)
probs:[B, N, T, S] = softmax(logits)
context:[B, T, N, H] = einsum(‘BNTS,BSNH-&gt;BTNH’, probs, v_proj)
Output y:[B, T, D] = einsum(‘BTNH,DNH&gt;BTD’, context, Wout)</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for _MultiHeadedAttention.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogits">
<code class="sig-name descname">_AttenLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._AttenLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogitsOneStep">
<code class="sig-name descname">_AttenLogitsOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">time_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._AttenLogitsOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenLogitsOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Attention logits for one single target (query) step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>time_step</strong> – Current time step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.AttenProbs">
<code class="sig-name descname">AttenProbs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.AttenProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.AttenProbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute attention probability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, T, N, H].</p></li>
<li><p><strong>key</strong> – [B, S, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, S].</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S]: A mask that is applied to prevent attention
between different segments. This is already been converted into large
negative logits. Only applied if packed_input = True.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, T, S] if
not None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, N, T, S].
probs_sum: [B, N, T, 1].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>probs</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContext">
<code class="sig-name descname">_AttenContext</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._AttenContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContext" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContextOneStep">
<code class="sig-name descname">_AttenContextOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">h</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._AttenContextOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._AttenContextOneStep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._DotAtten">
<code class="sig-name descname">_DotAtten</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._DotAtten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAtten" title="Permalink to this definition">¶</a></dt>
<dd><p>Main attention function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, T, N, H].</p></li>
<li><p><strong>key</strong> – [B, S, N, H].</p></li>
<li><p><strong>value</strong> – [B, S, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, S].</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S]: A mask that is applied to prevent attention
between different segments. This is already been converted into large
negative logits. Only applied if packed_input = True.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, T, S] if
not None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, N, H].
atten_probs: [B, N, T, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention._DotAttenOneStep">
<code class="sig-name descname">_DotAttenOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_step</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention._DotAttenOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention._DotAttenOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot attention function for queries with 1 time step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, 1, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>value</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>paddings</strong> – [B, S].</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S]: A mask that is applied to prevent attention
between different segments. This is already been converted into large
negative logits. Only applied if packed_input = True.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, S] if
not None.</p></li>
<li><p><strong>time_step</strong> – Current time step.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">key_vec</span></em>, <em class="sig-param"><span class="n">value_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the current query output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, T, D].</p></li>
<li><p><strong>key_vec</strong> – [B, S, D].</p></li>
<li><p><strong>value_vec</strong> – [B, S, D].</p></li>
<li><p><strong>paddings</strong> – [B, S].</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S]. A mask only applied if packed_input=True.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, T, T] if
not None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, D].
atten_probs: [B, N, T, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.InitStates">
<code class="sig-name descname">InitStates</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">target_batch_size</span></em>, <em class="sig-param"><span class="n">target_max_length</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.InitStates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.InitStates" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] or None.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, T] if
not None.</p></li>
<li><p><strong>time_step</strong> – A scalar or tensor with [B], current decode step, 0-based. if
it’s a scalar, all the time step are the same decode step. if it’s a
tensor, it represents current decode step for each sample.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_key_vec:   [T, B, N, H].
updated_value_vec: [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttention.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttention.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.FProp" title="lingvo.core.batch_major_attention.MultiHeadedAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention.FProp" title="lingvo.core.batch_major_attention.MultiHeadedAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiHeadedAttentionXL</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention" title="lingvo.core.batch_major_attention.MultiHeadedAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.MultiHeadedAttention</span></code></a></p>
<p>Transformer-XL multiheaded attention with relative positional embedding.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1901.02860.pdf">https://arxiv.org/pdf/1901.02860.pdf</a> section 3.3.</p>
<p>Notice this is only intended for self attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for _MultiHeadedAttention.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogits">
<code class="sig-name descname">_AttenLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL._AttenLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogitsOneStep">
<code class="sig-name descname">_AttenLogitsOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">time_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL._AttenLogitsOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL._AttenLogitsOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Attention logits for one single target (query) step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>time_step</strong> – Current time step. if it’s a scalar, all the time step are the
same decode step. if it’s a tensor, it represents current decode step
for each sample.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionXL.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionXL.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionXL.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] or None.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, T] if
not None.</p></li>
<li><p><strong>time_step</strong> – A scalar or tensor with [B], current decode step, 0-based. if
it’s a scalar, all the time step are the same decode step. if it’s a
tensor, it represents current decode step for each sample.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_key_vec:   [T, B, N, H].
updated_value_vec: [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiHeadedAttentionRPE</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention" title="lingvo.core.batch_major_attention.MultiHeadedAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.MultiHeadedAttention</span></code></a></p>
<p>Multiheaded attention with relative positional embedding …</p>
<p>See <a class="reference external" href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a>.</p>
<p>Notice this is only intended for self attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for _MultiHeadedAttention.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._CreateChildrenVariables">
<code class="sig-name descname">_CreateChildrenVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._CreateChildrenVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._CreateChildrenVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Create variables for child layers.</p>
<p>Should be rarely overridden, only in cases when control over the context of
children InstantiateVariables calls are needed. eg, if children variables
need to be created inside of a specific context manager.</p>
<p>There are a few cases of this in the codebase marked as for backwards
compability. This is only to ensure that variable scopes remain compatible
through the code migration. New layers should not copy that pattern, and
instead follow the standard pattern of self.CreateChild() in __init__() and
self.CreateVariable() in _CreateLayerVariables(). If you are okay with
breaking old checkpoints, you can go ahead and delete those functions.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._RelativePositionValueEmb">
<code class="sig-name descname">_RelativePositionValueEmb</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._RelativePositionValueEmb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._RelativePositionValueEmb" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets relative positional value embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>key</strong> – The attention key, a tensor of shape [batch, seqlen, dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Relative positional embedding, a Tensor of shape
[tgt_time=seqlen, src_time=seqlen, num_heads, attenion_dim]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogits">
<code class="sig-name descname">_AttenLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._AttenLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogitsOneStep">
<code class="sig-name descname">_AttenLogitsOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">time_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._AttenLogitsOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenLogitsOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Attention logits for one single target (query) step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>time_step</strong> – Current time step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContext">
<code class="sig-name descname">_AttenContext</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._AttenContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContext" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContextOneStep">
<code class="sig-name descname">_AttenContextOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">h</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE._AttenContextOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE._AttenContextOneStep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] or None.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, T] if
not None.</p></li>
<li><p><strong>time_step</strong> – A scalar or tensor with [B], current decode step, 0-based. if
it’s a scalar, all the time step are the same decode step. if it’s a
tensor, it represents current decode step for each sample.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_key_vec:   [T, B, N, H].
updated_value_vec: [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiHeadedAttentionRPE.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiHeadedAttentionRPE.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">LocalSelfAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention" title="lingvo.core.batch_major_attention.MultiHeadedAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.MultiHeadedAttention</span></code></a></p>
<p>Dot-product causal self attention using a sliding window.</p>
<p>We use the following capital letters to denote certain
tensor parameters.</p>
<blockquote>
<div><p>B = batch size
S=T= length of the key/value (source) and query (target)
D = model dimension
N = number of attention heads
H = dimensions of each attention head
W = block size
L = left context size, including left L-1 positions and self
R = right context size
F = L + R = context size of one position.
C = L + R + W - 1 = context size of a block of W positions.
U = ceiling(T/W).</p>
</div></blockquote>
<dl class="simple">
<dt>The key difference to base class is on calculating logits:</dt><dd><dl class="simple">
<dt>Base class:</dt><dd><ol class="arabic simple">
<li><p>Compute the full S x T attention.</p></li>
<li><p>Apply a S x T mask to enforce local attention window.</p></li>
</ol>
</dd>
<dt>This implementation:</dt><dd><p>1)  Compute a W x C attention for each of the U blocks. Where the i-th
block has query[W*i:W*(i+1)] and key[W*(i-1)-L-1:W*(i+1)+R].
2)  Apply a W x C mask for each block.</p>
</dd>
</dl>
</dd>
</dl>
<p>Effectively, we reduce both time and space complexities for computing the
sliding window attention from O(S * T) to O(S * C). In practice we observe
reduced HBM usage on TPU but no speed gains.</p>
<p>Note: Cross attention is not supported. As a result in speech models this
class can only be used for encoder.</p>
<p>TODO(weihan): add masking based local attention to the base class.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for LocalSelfAttention.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._AttenLogits">
<code class="sig-name descname">_AttenLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._AttenLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._AttenLogits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.AttenProbs">
<code class="sig-name descname">AttenProbs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.AttenProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.AttenProbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute attention probability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, T, N, H].</p></li>
<li><p><strong>key</strong> – [B, S=T, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T].</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] not used right now.</p></li>
<li><p><strong>per_step_padding</strong> – Not used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, N, U, W, C]
probs_sum: [B, N, U, W, 1].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>probs</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._AttenContext">
<code class="sig-name descname">_AttenContext</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._AttenContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._AttenContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the local attention context vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – Layer theta: NestedMap.</p></li>
<li><p><strong>probs</strong> – Local-self-MultiHeaded Attention probablities: [B, N, U, W, C].</p></li>
<li><p><strong>value</strong> – Input value vector: [B, S=T, N, H].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Attention context vector: [B, T, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">key_vec</span></em>, <em class="sig-param"><span class="n">value_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the current query output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, T, D].</p></li>
<li><p><strong>key_vec</strong> – [B, S, D] with S == T (self-attention).</p></li>
<li><p><strong>value_vec</strong> – [B, S, D] with S == T (self-attention).</p></li>
<li><p><strong>paddings</strong> – [B, S] with S == T (self-attention).</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S]. A mask only applied if packed_input=True.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, T, T] if
not None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, D].
probs: [B, N, U, W, C].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_step</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding, as opposed to
StreamStep which is for single step self attention.</p>
<p>Note: When the context window size is much smaller than target sequence
length, to make it run more efficent, T below can be just the window size.
Then, time_step should be the relative decode step and not bigger than T.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] or None. Not used right now.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, T] if
not None. Not used right now.</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization. Not
supported right now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_key_vec:   [T, B, N, H].
updated_value_vec: [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If right_context is non-zero.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#NotImplementedError" title="(in Python v3.7)"><strong>NotImplementedError</strong></a> – If use_short_seq_opt is true.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.zero_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial state given the batch size.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_static_length">
<code class="sig-name descname">_zero_state_static_length</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._zero_state_static_length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_static_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial state given the batch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – the batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing</p>
<ul class="simple">
<li><p>key: [B, p.inference_step_max_length + p.left_context - 1, N, H] or
[…, N * H] if p.use_3d_recurrent_state.</p></li>
<li><p>value: [B, p.inference_step_max_length + p.left_context - 1, N, H] or
[…, N * H] if p.use_3d_recurrent_state.</p></li>
<li><p>masks: [B, p.inference_step_max_length + p.left_context-1]. A 0/1
Tensor where 0s are masked out positions.</p></li>
<li><p>tail: [B, 1], currently only effective if use_3d_recurrent_state is
True, the tail pointer to key, value and paddings circular buffers.
Value range is [0, p.inference_step_max_length + p.left_context - 1).</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_dynamic_length">
<code class="sig-name descname">_zero_state_dynamic_length</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._zero_state_dynamic_length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._zero_state_dynamic_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial state given the batch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – the batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing</p>
<ul class="simple">
<li><p>key: [B, p.left_context - 1, N, H].</p></li>
<li><p>value: [B, p.left_context - 1, N, H].</p></li>
<li><p>masks: [B, p.left_context-1]. Tensor where 0s are masked out positions.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.StreamStep">
<code class="sig-name descname">StreamStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">state0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.StreamStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.StreamStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This differs from ExtendStep() which requires key/value seq lengths being
known in advance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A NestedMap of layer params.</p></li>
<li><p><strong>query_vec</strong> – A query vector of shape [B, Q, D].</p></li>
<li><p><strong>paddings</strong> – A 0/1 valued tensor of shape [B, Q].</p></li>
<li><p><strong>state0</strong> – A NestedMap of the same structure as returned by zero_state().</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of the given query vector with shape [B, Q, D].
padding: the same as input paddings.
state1: Updated state of the same structure as state0.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepStaticLength">
<code class="sig-name descname">_StreamStepStaticLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">state0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._StreamStepStaticLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepStaticLength" title="Permalink to this definition">¶</a></dt>
<dd><p>query_vec length is staticly known.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepDynamicLength">
<code class="sig-name descname">_StreamStepDynamicLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">state0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention._StreamStepDynamicLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention._StreamStepDynamicLength" title="Permalink to this definition">¶</a></dt>
<dd><p>query_vec length is dynamic.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttention.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttention.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttention.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.LocalSelfAttention.FProp" title="lingvo.core.batch_major_attention.LocalSelfAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.LocalSelfAttention.FProp" title="lingvo.core.batch_major_attention.LocalSelfAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">LocalSelfAttentionXL</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.LocalSelfAttention" title="lingvo.core.batch_major_attention.LocalSelfAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.LocalSelfAttention</span></code></a></p>
<p>Local causal version of transformer-xl self attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for LocalSelfAttention.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogits">
<code class="sig-name descname">_AttenLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL._AttenLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – A Tensor of shape [B, T, N, H]</p></li>
<li><p><strong>key</strong> – A Tensor of shape [B, T, N, H]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [B, N, T, S]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogitsOneStep">
<code class="sig-name descname">_AttenLogitsOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">time_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL._AttenLogitsOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL._AttenLogitsOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Attention logits for one single target (query) step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, N, H].</p></li>
<li><p><strong>key</strong> – [S, B, N, H] or [S, B, N*H/128, 128].</p></li>
<li><p><strong>time_step</strong> – Current time step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Tensor of shape [S, B, N]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_step</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding, as opposed to
StreamStep which is for single step self attention.</p>
<p>Note: When the context window size is much smaller than target sequence
length, to make it run more efficent, T below can be just the window size.
Then, time_step should be the relative decode step and not bigger than T.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S] or None. Not used right now.</p></li>
<li><p><strong>per_step_padding</strong> – A mask used by decoder self-attention to prevent
information flow from future (causal padding). It has shape [B, 1, T] if
not None. Not used right now.</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization. Not
supported right now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_key_vec:   [T, B, N, H].
updated_value_vec: [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If right_context is non-zero.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#NotImplementedError" title="(in Python v3.7)"><strong>NotImplementedError</strong></a> – If use_short_seq_opt is true.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL.zero_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial state given the batch size.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LocalSelfAttentionXL.StreamStep">
<code class="sig-name descname">StreamStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">state0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LocalSelfAttentionXL.StreamStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LocalSelfAttentionXL.StreamStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This differs from ExtendStep() which requires key/value seq lengths being
known in advance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A NestedMap of layer params.</p></li>
<li><p><strong>query_vec</strong> – A query vector of shape [B, Q, D].</p></li>
<li><p><strong>paddings</strong> – A 0/1 valued tensor of shape [B, Q].</p></li>
<li><p><strong>state0</strong> – A NestedMap of the same structure as returned by zero_state().</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of the given query vector with shape [B, Q, D].
padding: the same as input paddings.
state1: Updated state of the same structure as state0.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.RoutingAttention">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">RoutingAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiHeadedAttention" title="lingvo.core.batch_major_attention.MultiHeadedAttention"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.MultiHeadedAttention</span></code></a></p>
<p>“Implements a sparse attention based on k-means clustering.</p>
<p>This is used in the routing transformer <a class="reference external" href="https://arxiv.org/pdf/2003.05997">https://arxiv.org/pdf/2003.05997</a>.</p>
<p>This verison of multi-headed attention differs from the full attention
in that it uses k-means clusterting to cluster the queries and keys first,
and each query only attend to a subset of keys that are close to the centroid
closest to that query. As Euclidean distance is used to determine closeness,
we layer normalize queries and keys first so that closeness lead to a larger
dot product.</p>
<dl>
<dt>TODO(zhouwk) This class is missing the following features:</dt><dd><ul class="simple">
<li><p>propagate clustering loss;</p></li>
<li><p>supporting packed inputs;</p></li>
<li><p>support attention dropout;</p></li>
<li><p>support relative position encoding;</p></li>
<li><p>support using local attention on some heads.</p></li>
</ul>
</dd>
<dt>We use the following capital letters to denote shape parameters:</dt><dd><p>B = batch size
S = length of the source sequence
T = length of the target sequence
N = number of attention heads
H = dimensions of each attention head
D = model dimension</p>
<p>K = number of clusters
W = attention window</p>
</dd>
</dl>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention._DotAtten">
<code class="sig-name descname">_DotAtten</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">query_paddings</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention._DotAtten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention._DotAtten" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the attention.</p>
<p>Each query selects ‘p.attention_window’ number of keys to attend to. First
we find the closest centroid to that query, and we only allow that query to
attend to the ‘p.attention_window’ closest keys to that centroid.</p>
<p>In order to use K-means, this implementation applies layer normalization
to both the queries and the keys, and uses the normalized results to compute
attention weights.</p>
<p>When ‘p.attention_window’ is the source length, this should evalue to the
full attention (using layer normalized queries and keys).</p>
<p>The caller should pass in the paddings for both ‘key’ and ‘query’ because
during training, when we update the clustering we need to know the paddings
for both. (For the inference path only ‘key_paddings’ is useful.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of the values of this layer’s weights.</p></li>
<li><p><strong>query</strong> – [B, T, N, H].</p></li>
<li><p><strong>key</strong> – [B, S, N, H].</p></li>
<li><p><strong>value</strong> – [B, S, N, H].</p></li>
<li><p><strong>paddings</strong> – [B, S], paddings for key.</p></li>
<li><p><strong>segment_mask</strong> – must be None.</p></li>
<li><p><strong>per_step_padding</strong> – must be None. Please use p.causal_masking.</p></li>
<li><p><strong>query_paddings</strong> – [B, T], or None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, N, H].
atten_probs: [B, T, N, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention.InitStates">
<code class="sig-name descname">InitStates</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">target_batch_size</span></em>, <em class="sig-param"><span class="n">target_max_length</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention.InitStates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention.InitStates" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize ‘states’ with .key, .value, and .key_dists.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<p>This function is used by autoregressive decoding. Used for self-attention
(hence S=T) with p.causal_masking is True.</p>
<p>We compute the key/value/key_dists at <code class="xref py py-obj docutils literal notranslate"><span class="pre">time_step</span></code> and cache the updated
full length results in <code class="xref py py-obj docutils literal notranslate"><span class="pre">cache_states</span></code> to reduce duplicate computation.</p>
<p>p.fast_path is ignored (as if p.fast_path=False) as at each step we only
compute for query of length 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D].</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. It contains .key
and .value with shape [T, B, N, H], and .key_dists with  shape [T, B, N,
K]. Note that they are all time-major.</p></li>
<li><p><strong>paddings</strong> – [B, T], or None if there is no padding.</p></li>
<li><p><strong>time_step</strong> – Scalar, the current decode step, 0-based.</p></li>
<li><p><strong>segment_mask</strong> – must be None.</p></li>
<li><p><strong>per_step_padding</strong> – must be None. We obey causal masking.</p></li>
<li><p><strong>use_short_seq_opt</strong> – must be False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D].
updated_states:    <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with .key, .value, .key_dists.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If value projection is disabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention._DotAttenOneStep">
<code class="sig-name descname">_DotAttenOneStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">states</span></em>, <em class="sig-param"><span class="n">query_paddings</span></em>, <em class="sig-param"><span class="n">key_paddings</span></em>, <em class="sig-param"><span class="n">time_step</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention._DotAttenOneStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention._DotAttenOneStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot attention function for queries with 1 time step.</p>
<p>Called from ExtendStep(). Used for self-attention with p.causal_masking
is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query</strong> – [B, 1, N, H], already normalized.</p></li>
<li><p><strong>states</strong> – .key and .value with shape [T, B, N, H], .key_dists with shape
[T, B, N, K]. .key is normalized.</p></li>
<li><p><strong>query_paddings</strong> – [B, 1].</p></li>
<li><p><strong>key_paddings</strong> – [B, T].</p></li>
<li><p><strong>time_step</strong> – Scalar, the current decode step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention._DotAttenSlowPath">
<code class="sig-name descname">_DotAttenSlowPath</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">q_dists</span></em>, <em class="sig-param"><span class="n">k_dists</span></em>, <em class="sig-param"><span class="n">query_paddings</span></em>, <em class="sig-param"><span class="n">key_paddings</span></em>, <em class="sig-param"><span class="n">query_relative_position_shift</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention._DotAttenSlowPath"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention._DotAttenSlowPath" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the attention via the slow path.</p>
<p>This implementation selects, on a per query basis, p.attention_window
number of keys/values to attend to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of the values of this layer’s weights.</p></li>
<li><p><strong>query</strong> – [B, T, N, H], already normalized.</p></li>
<li><p><strong>key</strong> – [B, S, N, H], already normalized.</p></li>
<li><p><strong>value</strong> – [B, S, N, H].</p></li>
<li><p><strong>q_dists</strong> – [B, T, N, K].</p></li>
<li><p><strong>k_dists</strong> – [B, S, N, K].</p></li>
<li><p><strong>query_paddings</strong> – [B, T].</p></li>
<li><p><strong>key_paddings</strong> – [B, S].</p></li>
<li><p><strong>query_relative_position_shift</strong> – scalar. The position (relative to key[0])
of query[0]. This impacts relative position encoding (not yet
implemented) and causal masking.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, N, H].
atten_probs: [B, T, N, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.RoutingAttention._DotAttenFastPath">
<code class="sig-name descname">_DotAttenFastPath</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">q_dists</span></em>, <em class="sig-param"><span class="n">k_dists</span></em>, <em class="sig-param"><span class="n">query_paddings</span></em>, <em class="sig-param"><span class="n">key_paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#RoutingAttention._DotAttenFastPath"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.RoutingAttention._DotAttenFastPath" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the attention via the fast path.</p>
<p>This implementation compute groups of queries, and for each group,
selects a set of p.attention_window number of keys/values that each
query in that group all attend to.</p>
<p>There is no guarantee a query uniquely belong to a single group, although
via clustering this should likely be the case. When a query belong to
multiple groups, the attention is averaged post softmax; when a query
does not belong to any group, the attention result is zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of the values of this layer’s weights.</p></li>
<li><p><strong>query</strong> – [B, T, N, H], already normalized.</p></li>
<li><p><strong>key</strong> – [B, S, N, H], already normalized.</p></li>
<li><p><strong>value</strong> – [B, S, N, H].</p></li>
<li><p><strong>q_dists</strong> – [B, T, N, K].</p></li>
<li><p><strong>k_dists</strong> – [B, S, N, K].</p></li>
<li><p><strong>query_paddings</strong> – [B, T].</p></li>
<li><p><strong>key_paddings</strong> – [B, S].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, N, H].
atten_probs: [B, T, N, S]. Note, N * S * T space complexity here.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>encoded</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiSourceAttention">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiSourceAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Batch major attention with multiple source sub-attentions.</p>
<p>It attends to multiple sources and uses one query as input to generates a
combined attention context. The dimension of the combined context vector is a
sum of all source context vectors. Each source attention has its separate
params and is associated with a source key.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceAttention.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceAttention.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceAttention.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceAttention.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">key_vec</span></em>, <em class="sig-param"><span class="n">value_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceAttention.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceAttention.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<p>The central interface that subclasses should implement. The caller
calls <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiSourceAttention.FProp" title="lingvo.core.batch_major_attention.MultiSourceAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> dictionary. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">foo</span> <span class="o">=</span> <span class="n">InstanceOfASubClassOfFoo</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiSourceAttention.FProp" title="lingvo.core.batch_major_attention.MultiSourceAttention.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp()</span></code></a> computes a function given
the theta and the inputs. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
<span class="c1"># The same layer applied twice.</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">a2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>*args</strong> – List args.</p></li>
<li><p><strong>**kwargs</strong> – Keyward args.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceAttention._CombineContext">
<code class="sig-name descname">_CombineContext</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">enc_map</span></em>, <em class="sig-param"><span class="n">query_vec</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceAttention._CombineContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceAttention._CombineContext" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceAttention.AttenProbs">
<code class="sig-name descname">AttenProbs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query</span></em>, <em class="sig-param"><span class="n">key</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">segment_mask</span></em>, <em class="sig-param"><span class="n">per_step_padding</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceAttention.AttenProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceAttention.AttenProbs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerAttentionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Multiheaded attention sub-layer in Transformer layer.</p>
<p>Input is first normalized using Layer Normalization. Output of layer
normalization is processed using multi-headed attention. And finally, the
output of the attention layer is combined with the residual connection.
This module also allows mixing different attention heads by making num_heads
and atten_tpl into lists of the same size, specifying the distribution of
heads for each attention type.</p>
<p>This layer will be used in the following two scenarios:</p>
<ol class="arabic simple">
<li><p>Multi-Headed Self-Attention, where attention keys, values (source_vecs) and
queries come from the same previous layer output.</p></li>
<li><p>Masked Multi-Headed Self-Attention, where attention keys, values and
queries all come from the same previous layer output, but rightward
activations are masked to prevent information flow from future. This is the
use case for Transformer decoder self-attention layers. Can be activated by
setting is_masked flag of this layer.</p></li>
<li><p>Multi-Headed Cross-Attention, where attention keys and values
(source_vecs) are coming from a different source (output of the encoder),
and queries coming from the previous layer outputs (decoder).</p></li>
<li><p>Mixture of different heads, for example 2 LocalSelfAttention heads
and 3 RoutingAttention heads can be specified by setting num_heads = [2, 3]
and atten_tpl = [LocalSelfAttention, RoutingAttention].</p></li>
</ol>
<p>We use the same capital letters to denote certain tensor parameters as
MultiHeadedAttention class.</p>
<blockquote>
<div><p>B = batch size
S = length of the key/value (source)
T = length of the query (target)
D = model dimension
N = number of attention heads
H = dimensions of each attention head.</p>
</div></blockquote>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.CommonParams">
<em class="property">classmethod </em><code class="sig-name descname">CommonParams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">num_heads</span></em>, <em class="sig-param"><span class="n">is_masked</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_relative_atten</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">relative_pos_emb_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">local_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">left_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">right_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout_prob</span><span class="o">=</span><span class="default_value">0.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.CommonParams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.CommonParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a hyperparam for the most representative cases.</p>
<p>CommonParams is not expected to be extended to an omnipotent/generic builder
method. Specific use cases should take the return value of it and apply
further customization. It should be kept lean and only extended cautiously
for very common cases.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer._InitAttentionParams">
<code class="sig-name descname">_InitAttentionParams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">atten_tpl</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer._InitAttentionParams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer._InitAttentionParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an initialized transformer attention parameters.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">per_step_padding_override</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the result of Transformer attention layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, T, D].</p></li>
<li><p><strong>source_vecs</strong> – [B, S, D] (cross_attention) or None (self-attention).</p></li>
<li><p><strong>paddings</strong> – [B, S].</p></li>
<li><p><strong>per_step_padding_override</strong> – [B, T, T] for self attention or [B, T, S] for
cross attention.</p></li>
<li><p><strong>segment_mask</strong> – [B, 1, T, S].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, T, D].
atten_probs: [B, N, T, S].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.InitStates">
<code class="sig-name descname">InitStates</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">target_batch_size</span></em>, <em class="sig-param"><span class="n">target_max_length</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.InitStates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.InitStates" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the result and update cached states for the current step.</p>
<p>This function is used by autoregressive decoding. This function knows the
length of full sequence, thus it is different from StreamStep.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [B, 1, D]</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   - [T, B,
N, H]. value - [T, B, N, H].</p></li>
<li><p><strong>time_step</strong> – A scalar or tensor with [B], current decode step, 0-based. if
it’s a scalar, all the time step are the same decode step. if it’s a
tensor, it represents current decode step for each sample.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[B, 1, D]
updated_states: A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing the updated states.
key   - [T, B, N, H].
value - [T, B, N, H].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cur_output</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.7/library/exceptions.html#ValueError" title="(in Python v3.7)"><strong>ValueError</strong></a> – If not used as masked/causal self-attention.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.zero_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial state given the batch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – the batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The initial state for streaming inference.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>state</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerAttentionLayer.StreamStep">
<code class="sig-name descname">StreamStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">state0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerAttentionLayer.StreamStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer.StreamStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the value vector given the query of the current step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – a NestedMap with layer weights.</p></li>
<li><p><strong>query_vec</strong> – A query vector of shape [B, T, D].</p></li>
<li><p><strong>paddings</strong> – A 0/1 valued tensor of shape [B, T].</p></li>
<li><p><strong>state0</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of the same structure as returned by zero_state().</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of the given query vector with shape [B, T, D].
padding: the same as input paddings.
state: updated state.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerMultiSourceAttentionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerMultiSourceAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.TransformerAttentionLayer" title="lingvo.core.batch_major_attention.TransformerAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.TransformerAttentionLayer</span></code></a></p>
<p>Batch major multi-source multi-headed attention.</p>
<p>Only supports scenarios 3 described by comments on TransformerAttentionLayer:</p>
<ol class="arabic simple" start="3">
<li><p>Multi-source multi-headed cross-attention, where attention keys and values
(source_vecs) are coming from different sources (one of them is usually
the outputs of the encoder), and queries coming from the previous layer
outputs (decoder). Specifically, attention keys and values are NestedMaps
containing encodings of different sources. This corresponds to a
multi-source decoder-to-encoder attention mechanism, i.e., decoder attends
to encoder outputs and other sources.</p></li>
</ol>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerMultiSourceAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer._InitAttentionParams">
<code class="sig-name descname">_InitAttentionParams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">atten_tpl</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerMultiSourceAttentionLayer._InitAttentionParams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerMultiSourceAttentionLayer._InitAttentionParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an initialized multi-source transformer attention parameters.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.TransformerLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Transformer layer with multiheaded attention.</p>
<p>Applies self-attention followed by a cross-attention and feed forward layer.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.CommonParams">
<em class="property">classmethod </em><code class="sig-name descname">CommonParams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">atten_num_heads</span></em>, <em class="sig-param"><span class="n">atten_is_relative</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">atten_local_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">atten_left_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">atten_right_context</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">has_aux_atten</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">mask_self_atten</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">fflayer_hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">fflayer_output_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout_prob</span><span class="o">=</span><span class="default_value">0.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.CommonParams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.CommonParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a hyperparam for the most representative cases.</p>
<p>CommonParams is not expected to be extended to an omnipotent/generic builder
method. Specific use cases should take the return value of it and apply
further customization. It should be kept lean and only extended cautiously
for very common cases.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.SetNumInputNodes">
<em class="property">classmethod </em><code class="sig-name descname">SetNumInputNodes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">num_input_nodes</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.SetNumInputNodes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.SetNumInputNodes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.NumOutputNodes">
<em class="property">classmethod </em><code class="sig-name descname">NumOutputNodes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.NumOutputNodes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.NumOutputNodes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer._GetSourceBatchSize">
<code class="sig-name descname">_GetSourceBatchSize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">aux_vec</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer._GetSourceBatchSize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer._GetSourceBatchSize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer._GetSourceLength">
<code class="sig-name descname">_GetSourceLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">aux_vec</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer._GetSourceLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer._GetSourceLength" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">aux_vec</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">per_step_padding_override</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_mask</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, target_time, dim].</p></li>
<li><p><strong>paddings</strong> – [target_batch, target_time].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim].</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time].</p></li>
<li><p><strong>per_step_padding_override</strong> – [target_batch, target_time, target_time].</p></li>
<li><p><strong>segment_mask</strong> – [target_batch, 1, target_time, target_time].</p></li>
<li><p><strong>aux_segment_mask</strong> – [source_batch, 1, target_time, source_time].</p></li>
</ul>
</dd>
</dl>
<p>target_batch can be a multiple of source_batch, where samples in
target_batch are arranged in the order of [m, source_batch] where m =
target_batch / source_batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The fflayer output with shape [target_batch, target_time, dim].
atten_probs: A NestedMap with keys <code class="xref py py-obj docutils literal notranslate"><span class="pre">self_atten</span></code> &lt;float&gt;[B, N, T, T], and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">aux_atten</span></code> (optional): &lt;float&gt;[B, N, T, S].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.InitStates">
<code class="sig-name descname">InitStates</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">target_batch_size</span></em>, <em class="sig-param"><span class="n">target_max_length</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.InitStates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.InitStates" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">aux_vec</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer, extend one step in autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, 1, dim].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time]</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   -
[target_time, target_batch, num_heads, dim_per_head]. value -
[target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[target_batch, 1, dim]
atten_probs: [target_batch, num_heads, target_time=1, source_time]
updated_states: A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing the updated states.
key   - [target_time, target_batch, num_heads, dim_per_head].
value - [target_time, target_batch, num_heads, dim_per_head].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cur_output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.TransformerFlops">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerFlops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">num_heads</span></em>, <em class="sig-param"><span class="n">ff_dim</span></em>, <em class="sig-param"><span class="n">atten_dim</span></em>, <em class="sig-param"><span class="n">model_dim</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerFlops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerFlops" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute FLOPs for Transformer layer without auxiliary attention.</p>
<blockquote>
<div><dl class="simple">
<dt>Attention Layer FLOPs (N = num attention heads, H = dim per head):</dt><dd><p>q, k, v projections, incl bias: 3 x ‘BTD,DNH-&gt;BTNH’ -&gt; 6*N*H*D*B*T
logits: ‘BTNH,BDNH-&gt;BNTD’ -&gt; (2*H-1)*N*B*T^2
softmax: 5 ops per element in BNTD -&gt; 5*N*D*B*T
context: ‘BNTD,BDNH-&gt;BTNH’ -&gt; (2*T-1)*N*H*B*T
output proj: ‘BTNH,DNH-&gt;BTD’ -&gt; (2*N-1)*(2*H-1)*D*B*T</p>
</dd>
</dl>
<p>2 residuals FLOPs: 2*D*B*T
1 FF layer FLOPs: 4*ff_hidden*D*B*T</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input dimensions to the layer, [Batch, Time, Dim].</p></li>
<li><p><strong>num_heads</strong> – Number of attention heads for layer.</p></li>
<li><p><strong>ff_dim</strong> – Feedforward hidden dimension.</p></li>
<li><p><strong>atten_dim</strong> – Attention hidden dimension.</p></li>
<li><p><strong>model_dim</strong> – Dimension of the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Total FLOPs of the transformer layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiSourceTransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.TransformerLayer" title="lingvo.core.batch_major_attention.TransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.TransformerLayer</span></code></a></p>
<p>Multi-source transformer layer with multiheaded attention.</p>
<p>Multi-source attention is used for cross attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerLayer.primary_source_key">
<em class="property">property </em><code class="sig-name descname">primary_source_key</code><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer.primary_source_key" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceBatchSize">
<code class="sig-name descname">_GetSourceBatchSize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">aux_vec</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerLayer._GetSourceBatchSize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceBatchSize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceLength">
<code class="sig-name descname">_GetSourceLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">aux_vec</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerLayer._GetSourceLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer._GetSourceLength" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.UseRelativeAttentionInTransformerLayer">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">UseRelativeAttentionInTransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">transformer_params</span></em>, <em class="sig-param"><span class="n">rel_pos_emb_dim</span></em>, <em class="sig-param"><span class="n">atten_type</span><span class="o">=</span><span class="default_value">'transformer_xl'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#UseRelativeAttentionInTransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.UseRelativeAttentionInTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses transformer-xl attention for self attention of a transformer layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transformer_params</strong> – A mt_attention_layer.TransformerLayer.Params() object.</p></li>
<li><p><strong>rel_pos_emb_dim</strong> – (int) Relative positional embedding dim to be set.</p></li>
<li><p><strong>atten_type</strong> – (string) Attention type. Supported:
- ‘transformer_xl’: mt_attention_layer.MultiHeadedAttentionXL
- ‘rpe’: mt_attention_layer.MultiHeadedAttentionRPE</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mt_attention_layer.TransformerLayer.Params() object with relative pos emb.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="lingvo.core.batch_major_attention.ClearRelativeAttentionInTransformerLayer">
<code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">ClearRelativeAttentionInTransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">transformer_params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#ClearRelativeAttentionInTransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.ClearRelativeAttentionInTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes relative position attention in the transformer layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>transformer_params</strong> – A mt_attention_layer.TransformerLayer param.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mt_attention_layer.TransformerLayer param without relative attention.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.TransformerDecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerDecoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerDecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerDecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.TransformerLayer" title="lingvo.core.batch_major_attention.TransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.TransformerLayer</span></code></a></p>
<p>Transformer decoder layer with multiheaded attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerDecoderLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerDecoderLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerDecoderLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">MultiSourceTransformerDecoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerDecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.MultiSourceTransformerLayer" title="lingvo.core.batch_major_attention.MultiSourceTransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.MultiSourceTransformerLayer</span></code></a></p>
<p>Multi-source transformer decoder layer with multiheaded attention.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#MultiSourceTransformerDecoderLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.MultiSourceTransformerDecoderLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">StackedTransformerLayers</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A stack of Batch-Major Transformer layers.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers.GetSplitForLayer">
<em class="property">classmethod </em><code class="sig-name descname">GetSplitForLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">buckets</span></em>, <em class="sig-param"><span class="n">layer_index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers.GetSplitForLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers.GetSplitForLayer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers._GetDeviceOfLayer">
<code class="sig-name descname">_GetDeviceOfLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer_idx</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers._GetDeviceOfLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers._GetDeviceOfLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the device for a given layer index based on our params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">aux_vec</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">segment_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_mask</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Stacked Transformer layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [batch, target_time, dim].</p></li>
<li><p><strong>paddings</strong> – [batch, target_time].</p></li>
<li><p><strong>aux_vec</strong> – [batch, source_time, dim].</p></li>
<li><p><strong>aux_paddings</strong> – [batch, source_time].</p></li>
<li><p><strong>segment_mask</strong> – [batch, 1, target_time, target_time]</p></li>
<li><p><strong>aux_segment_mask</strong> – [batch, 1, target_time, source_time]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(context, paddings), where the context vector has shape [batch,
target_time, dim].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers.InitStates">
<code class="sig-name descname">InitStates</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers.InitStates"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers.InitStates" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StackedTransformerLayers.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">aux_vec</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StackedTransformerLayers.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StackedTransformerLayers.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer, extend one step in autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, 1, dim].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time]</p></li>
<li><p><strong>cached_states</strong> – <p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding.
cached_states.x_layers is a list corresponding to self.x_layers, where
each element is a NestedMap with attention keys and values:</p>
<ul>
<li><p>key: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>value: [target_time, target_batch, num_heads, dim_per_head].</p></li>
</ul>
</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The last decoder layer output of shape [target_batch, 1, dim].
updated_states: A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing the updated states.
updated_states.x_layers is a list corresponding to self.x_layers, where
each element is a NestedMap with attention keys and values:</p>
<ul class="simple">
<li><p>key: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>value: [target_time, target_batch, num_heads, dim_per_head].</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cur_output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">TransformerFeedForwardLayerWithTaskId</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerFeedForwardLayerWithTaskId"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.layers_with_attention.html#lingvo.core.layers_with_attention.TransformerFeedForwardLayer" title="lingvo.core.layers_with_attention.TransformerFeedForwardLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.layers_with_attention.TransformerFeedForwardLayer</span></code></a></p>
<p>TransformerFeedForwardLayer with optional task_id input args.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerFeedForwardLayerWithTaskId.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em>, <em class="sig-param"><span class="n">task_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#TransformerFeedForwardLayerWithTaskId.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.TransformerFeedForwardLayerWithTaskId.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed-forward, residual and layer-norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>inputs</strong> – [batch, time, dim].</p></li>
<li><p><strong>paddings</strong> – [batch, time]</p></li>
<li><p><strong>task_id</strong> – optional task_id with shape [batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor of the same shape with inputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">GPipeTransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.TransformerLayer" title="lingvo.core.batch_major_attention.TransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.TransformerLayer</span></code></a></p>
<p>GPipe compatible transformer layer.</p>
<p>DEPRECATED: This layer and its use in GPipeTransformerStack is
deprecated. Consider using the new GPipeBatchMajorTransformerStack instead.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">target_vecs</span></em>, <em class="sig-param"><span class="n">target_paddings</span></em>, <em class="sig-param"><span class="n">source_segment_id</span></em>, <em class="sig-param"><span class="n">target_segment_id</span></em>, <em class="sig-param"><span class="n">transparent_acc</span></em>, <em class="sig-param"><span class="n">transparent_acc_helper</span></em>, <em class="sig-param"><span class="n">source_task_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">target_task_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, target_time, dim].</p></li>
<li><p><strong>paddings</strong> – [target_batch, target_time].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim].</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time].</p></li>
<li><p><strong>per_step_padding_override</strong> – [target_batch, target_time, target_time].</p></li>
<li><p><strong>segment_mask</strong> – [target_batch, 1, target_time, target_time].</p></li>
<li><p><strong>aux_segment_mask</strong> – [source_batch, 1, target_time, source_time].</p></li>
</ul>
</dd>
</dl>
<p>target_batch can be a multiple of source_batch, where samples in
target_batch are arranged in the order of [m, source_batch] where m =
target_batch / source_batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The fflayer output with shape [target_batch, target_time, dim].
atten_probs: A NestedMap with keys <code class="xref py py-obj docutils literal notranslate"><span class="pre">self_atten</span></code> &lt;float&gt;[B, N, T, T], and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">aux_atten</span></code> (optional): &lt;float&gt;[B, N, T, S].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp" title="lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp" title="lingvo.core.batch_major_attention.GPipeTransformerLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer.SetupDeterministicDropout">
<em class="property">classmethod </em><code class="sig-name descname">SetupDeterministicDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer.SetupDeterministicDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.SetupDeterministicDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Replaced dropout layers in transformer with deterministic ones.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeTransformerLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">aux_vec</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">task_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeTransformerLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeTransformerLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer, extend one step in autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, 1, dim].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time]</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   -
[target_time, target_batch, num_heads, dim_per_head]. value -
[target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>task_id</strong> – [batch_size]: the input task_id meta information.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[target_batch, 1, dim]
updated_states: A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing the updated states.
key   - [target_time, target_batch, num_heads, dim_per_head].
value - [target_time, target_batch, num_heads, dim_per_head].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cur_output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">GPipeBatchMajorTransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.TransformerLayer" title="lingvo.core.batch_major_attention.TransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.TransformerLayer</span></code></a></p>
<p>GPipe compatible batch majortransformer layer.</p>
<p>To be used with the new GPipeBatchMajorStack.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">target_vecs</span></em>, <em class="sig-param"><span class="n">target_paddings</span></em>, <em class="sig-param"><span class="n">encoder_self_atten_segment_mask</span></em>, <em class="sig-param"><span class="n">decoder_self_atten_segment_mask</span></em>, <em class="sig-param"><span class="n">decoder_cross_atten_segment_mask</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, target_time, dim].</p></li>
<li><p><strong>paddings</strong> – [target_batch, target_time].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim].</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time].</p></li>
<li><p><strong>per_step_padding_override</strong> – [target_batch, target_time, target_time].</p></li>
<li><p><strong>segment_mask</strong> – [target_batch, 1, target_time, target_time].</p></li>
<li><p><strong>aux_segment_mask</strong> – [source_batch, 1, target_time, source_time].</p></li>
</ul>
</dd>
</dl>
<p>target_batch can be a multiple of source_batch, where samples in
target_batch are arranged in the order of [m, source_batch] where m =
target_batch / source_batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The fflayer output with shape [target_batch, target_time, dim].
atten_probs: A NestedMap with keys <code class="xref py py-obj docutils literal notranslate"><span class="pre">self_atten</span></code> &lt;float&gt;[B, N, T, T], and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">aux_atten</span></code> (optional): &lt;float&gt;[B, N, T, S].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp" title="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp" title="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.SetupDeterministicDropout">
<em class="property">classmethod </em><code class="sig-name descname">SetupDeterministicDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer.SetupDeterministicDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.SetupDeterministicDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Replaced dropout layers in transformer with deterministic ones.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">aux_vec</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">cached_states</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#GPipeBatchMajorTransformerLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.GPipeBatchMajorTransformerLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer decoder layer, extend one step in autoregressive decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, 1, dim].</p></li>
<li><p><strong>aux_vec</strong> – [source_batch, source_time, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [source_batch, source_time]</p></li>
<li><p><strong>cached_states</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing tensors which are the
results of previous attentions, used for fast decoding. key   -
[target_time, target_batch, num_heads, dim_per_head]. value -
[target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[target_batch, 1, dim]
updated_states: A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing the updated states.
key   - [target_time, target_batch, num_heads, dim_per_head].
value - [target_time, target_batch, num_heads, dim_per_head].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cur_output</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.ResidualAddLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">ResidualAddLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#ResidualAddLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.ResidualAddLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer to add inputs with residual weight.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.ResidualAddLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#ResidualAddLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.ResidualAddLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for <a class="reference internal" href="#lingvo.core.batch_major_attention.ResidualAddLayer" title="lingvo.core.batch_major_attention.ResidualAddLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ResidualAddLayer</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.ResidualAddLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#ResidualAddLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.ResidualAddLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Return combined inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>x</strong> – input tensor.</p></li>
<li><p><strong>y</strong> – input tensor to apply weight to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Added tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.ResidualAddLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#ResidualAddLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.ResidualAddLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.ResidualAddLayer.FProp" title="lingvo.core.batch_major_attention.ResidualAddLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.ResidualAddLayer.FProp" title="lingvo.core.batch_major_attention.ResidualAddLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.PaddingLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">PaddingLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PaddingLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PaddingLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer that applies paddings to the inputs.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PaddingLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PaddingLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PaddingLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Return combined inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>inputs</strong> – input tensor.</p></li>
<li><p><strong>paddings</strong> – paddings tensor, should be of shape tf.shape(inputs)[:-1].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor with paddings applied.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.PaddingLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#PaddingLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.PaddingLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.PaddingLayer.FProp" title="lingvo.core.batch_major_attention.PaddingLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.PaddingLayer.FProp" title="lingvo.core.batch_major_attention.PaddingLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.StrideLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">StrideLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StrideLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StrideLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer that does stride.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StrideLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StrideLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StrideLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for <a class="reference internal" href="#lingvo.core.batch_major_attention.StrideLayer" title="lingvo.core.batch_major_attention.StrideLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StrideLayer</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StrideLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StrideLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StrideLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies stride to the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>x</strong> – input tensor, […, time, …]. Stride is applied to the time dim
as given by p.axis.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Strided tensor, with the stride applied to the time dim in x.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.StrideLayer.FPropMeta">
<em class="property">classmethod </em><code class="sig-name descname">FPropMeta</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#StrideLayer.FPropMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.StrideLayer.FPropMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns metadata about the <a class="reference internal" href="#lingvo.core.batch_major_attention.StrideLayer.FProp" title="lingvo.core.batch_major_attention.StrideLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> computation for this layer.</p>
<p><strong>Experimental feature.</strong>
Don’t use or depend on it without consulting Lingvo authors.</p>
<p>E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">SomeComplexLayer</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">FPropMeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;channels&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.flops</span></code> gives an estimate count of floating point operations done by
one <a class="reference internal" href="#lingvo.core.batch_major_attention.StrideLayer.FProp" title="lingvo.core.batch_major_attention.StrideLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> given an input tensor of shape [128, 20, 50, channels].
<code class="xref py py-obj docutils literal notranslate"><span class="pre">meta.out_shapes</span></code> is a tuple of TShape, which tells you what shape
of tensors this layer will return.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – The param of a layer of this layer type.</p></li>
<li><p><strong>*args</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
<li><p><strong>**kwargs</strong> – Corresponds to FProp with Tensors replaced by <code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorShape</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> with</p>
<ul class="simple">
<li><p>flops - The estimated number of floating point operations incurred by
this fprop.</p></li>
<li><p>out_shapes - A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">TShape</span></code>. I.e., <code class="xref py py-obj docutils literal notranslate"><span class="pre">out_shapes[i]</span></code>
represents the shape of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>-th returned tensor of the fprop.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.FunnelPoolingLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">FunnelPoolingLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelPoolingLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelPoolingLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.StrideLayer" title="lingvo.core.batch_major_attention.StrideLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.StrideLayer</span></code></a></p>
<p>A layer that does pooling in Funnel-Transformer.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/2006.03236.pdf">https://arxiv.org/pdf/2006.03236.pdf</a> section 2.2. for query-only pooling and
section A.1 for begin_intact &amp; trunc_seq.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.FunnelPoolingLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelPoolingLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelPoolingLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for <a class="reference internal" href="#lingvo.core.batch_major_attention.FunnelPoolingLayer" title="lingvo.core.batch_major_attention.FunnelPoolingLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FunnelPoolingLayer</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.FunnelPoolingLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelPoolingLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelPoolingLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies pooling to the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>x</strong> – input tensor of shape [batch, time, dim] or [batch, time], where the
pooling is applied to the time dim.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pooled tensor, with the pooling applied to the second dim in x.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.FunnelUpsampleLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">FunnelUpsampleLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelUpsampleLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelUpsampleLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer that does upsampling in Funnel-Transformer.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.FunnelUpsampleLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelUpsampleLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelUpsampleLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Params for <a class="reference internal" href="#lingvo.core.batch_major_attention.FunnelUpsampleLayer" title="lingvo.core.batch_major_attention.FunnelUpsampleLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FunnelUpsampleLayer</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.FunnelUpsampleLayer._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelUpsampleLayer._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelUpsampleLayer._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.FunnelUpsampleLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#FunnelUpsampleLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.FunnelUpsampleLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsample to the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – weights defined in this layer.</p></li>
<li><p><strong>x</strong> – input tensor, [batch, time, dim] upsampling is applied to the time dim.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Upsampled tensor, with the upsampling applied to the second dim in x.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.Builder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">Builder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.builder.html#lingvo.core.builder.Base" title="lingvo.core.builder.Base"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.builder.Base</span></code></a></p>
<p>Builder for self-attention layers.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>The params of this layer.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.SetCanonicalShardingParams">
<em class="property">classmethod </em><code class="sig-name descname">SetCanonicalShardingParams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.SetCanonicalShardingParams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.SetCanonicalShardingParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Set up canonical SPMD sharding params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Dropout">
<code class="sig-name descname">_Dropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">drop_prob</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a DropoutLayer Params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Add">
<code class="sig-name descname">_Add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">residual_weight</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">apply_residual</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._DefaultLN">
<code class="sig-name descname">_DefaultLN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._DefaultLN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._DefaultLN" title="Permalink to this definition">¶</a></dt>
<dd><p>Layer norm with default params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._ExpandDims">
<code class="sig-name descname">_ExpandDims</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._ExpandDims"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._ExpandDims" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Squeeze">
<code class="sig-name descname">_Squeeze</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Squeeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Squeeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Glu">
<code class="sig-name descname">_Glu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Glu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Pad">
<code class="sig-name descname">_Pad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Pad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._MultiHeadedAtten">
<code class="sig-name descname">_MultiHeadedAtten</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._MultiHeadedAtten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._MultiHeadedAtten" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a MultiHeadedAttention params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.GatedGeluFeedforward">
<code class="sig-name descname">GatedGeluFeedforward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">ff_hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.GatedGeluFeedforward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.GatedGeluFeedforward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.Feedforward">
<code class="sig-name descname">Feedforward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">ff_hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.Feedforward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.Feedforward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._MaybeSplit">
<code class="sig-name descname">_MaybeSplit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">blocks</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._MaybeSplit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._MaybeSplit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._DepthwiseConv2D">
<code class="sig-name descname">_DepthwiseConv2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">filter_size</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._DepthwiseConv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._DepthwiseConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>A depthwise convolution block for lightweight conv.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._NormalizedDepthwiseConv2D">
<code class="sig-name descname">_NormalizedDepthwiseConv2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._NormalizedDepthwiseConv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._NormalizedDepthwiseConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>A depthwise convolution block for lightweight conv.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.LConv">
<code class="sig-name descname">LConv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">convolution_fn</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.LConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.LConv" title="Permalink to this definition">¶</a></dt>
<dd><p>[DEPRECATED] A lightweight convolution block as described in.</p>
<p>Use conv_layers_builder.LConv() instead.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1901.10430">https://arxiv.org/abs/1901.10430</a>
Corresponding PyTorch Implementation (L587):
<a class="reference external" href="https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/models/lightconv.py">https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/models/lightconv.py</a></p>
<p>This block can be used as an alternative to self-attention block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of the params</p></li>
<li><p><strong>kernel_size</strong> – kernel size used in the conv layer.</p></li>
<li><p><strong>is_causal</strong> – is causal padding or not.</p></li>
<li><p><strong>convolution_fn</strong> – Convolution to apply, default _NormalizedDepthwiseConv2D.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A LightWeightConvLayerBlock layer params.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.LconvBlock">
<code class="sig-name descname">LconvBlock</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">is_causal</span></em>, <em class="sig-param"><span class="n">convolution_fn</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.LconvBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.LconvBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>A lightweight conv block followed by a feedforward one.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.Seq">
<code class="sig-name descname">Seq</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">subs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.Seq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.Seq" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stack of sequential layers.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.LConvStack">
<code class="sig-name descname">LConvStack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">kernel_sizes</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.LConvStack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.LConvStack" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stack of LConv layers with kernel size in kernel_sizes.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Stride">
<code class="sig-name descname">_Stride</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Stride"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Stride" title="Permalink to this definition">¶</a></dt>
<dd><p>Strides the input sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of this layer.</p></li>
<li><p><strong>stride</strong> – To use every k-th token, set the stride to k. When stride == 0,
only returns the first token of the input. When stride == 1, returns
every token in the input.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We use
[:first_n:stride] to select the output tokens. If first_n is None, this
flag is a no-op. If stride is positive, the output sequence length is
“(first_n-1) // stride + 1”. If stride is 0, first_n has to be None or
1. first_n can’t be 0. If first_n &lt;= stride, only the first token is
used.</p></li>
<li><p><strong>axis</strong> – along which axis to apply striding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A layer params that does stride.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._StridedAttention">
<code class="sig-name descname">_StridedAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._StridedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._StridedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes self attention with optional stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of this layer.</p></li>
<li><p><strong>stride</strong> – If omitted, the default is 1: use every token in the query. To use
every k-th token, set the stride to k. When set to 0, only use the first
token of the query.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We use
[:first_n:stride] to select the output tokens. If first_n is None, this
flag is a no-op. If stride is positive, the output sequence length is
“(first_n-1) // stride + 1”. If stride is 0, first_n has to be None or
1. first_n can’t be 0. If first_n &lt;= stride, only the first token is
used.</p></li>
<li><p><strong>num_heads</strong> – the number of heads.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A self attention layer params.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._Pool">
<code class="sig-name descname">_Pool</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._Pool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._Pool" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs pooling on the input sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of this layer.</p></li>
<li><p><strong>stride</strong> – To pool every k token, set the stride to k. When stride == 1,
returns every token in the input. When stride == 0, only returns the
first token of the input without perform any pooling.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We only pool
[:first_n] input tokens. If first_n is None, this flag is a no-op.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A layer params that does stride.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder._FunnelAttention">
<code class="sig-name descname">_FunnelAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder._FunnelAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder._FunnelAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes self attention with optional stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of this layer.</p></li>
<li><p><strong>stride</strong> – If omitted, the default is 1: use every token in the query. To use
every k-th token, set the stride to k. When set to 0, only use the first
token of the query.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We use
[:first_n:stride] to select the output tokens. If first_n is None, this
flag is a no-op. If stride is positive, the output sequence length is
“(first_n-1) // stride + 1”. If stride is 0, first_n has to be None or
1. first_n can’t be 0. If first_n &lt;= stride, only the first token is
used.</p></li>
<li><p><strong>num_heads</strong> – the number of heads.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A self attention layer params.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.FunnelEncoderLayer">
<code class="sig-name descname">FunnelEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ff_hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">gated_gelu</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.FunnelEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.FunnelEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>(inputs, paddings) -&gt; (encoded, paddings).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the string name of the encoder layer params.</p></li>
<li><p><strong>stride</strong> – To pool every k tokens, set the stride to k. When stride == 0,
only returns the first token of the input. When stride == 1, returns
every token in the input.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We use
pool([:first_n]) to select the output tokens. If first_n is None, this
flag is a no-op. If stride is positive, the output sequence length is
“(first_n-1) // stride + 1”. If stride is 0, first_n has to be None or
1. first_n can’t be 0. If first_n &lt;= stride, only the first token is
used.</p></li>
<li><p><strong>ff_hidden_dim</strong> – The feed forward layer’s hidden dimension. If specified,
this will override p.ff_hidden_dim.</p></li>
<li><p><strong>num_heads</strong> – The number of heads for the multi-head attention module. If
specified, this will override p.num_heads.</p></li>
<li><p><strong>gated_gelu</strong> – Use gated_gelu feedforward layer nor not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A transformer encoder layer params that supports optional stride.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.TransformerEncoderLayer">
<code class="sig-name descname">TransformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">stride</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">first_n</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ff_hidden_dim</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.TransformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.TransformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>(inputs, paddings) -&gt; (encoded, paddings).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the string name of the encoder layer params.</p></li>
<li><p><strong>stride</strong> – To use every k-th token, set the stride to k. When stride == 0,
only returns the first token of the input. When stride == 1, returns
every token in the input.</p></li>
<li><p><strong>first_n</strong> – only considers the first N tokens for the output. We use
[:first_n:stride] to select the output tokens. If first_n is None, this
flag is a no-op. If stride is positive, the output sequence length is
“(first_n-1) // stride + 1”. If stride is 0, first_n has to be None or
1. first_n can’t be 0. If first_n &lt;= stride, only the first token is
used.</p></li>
<li><p><strong>ff_hidden_dim</strong> – The feed forward layer’s hidden dimension. If specified,
this will override p.ff_hidden_dim.</p></li>
<li><p><strong>num_heads</strong> – The number of heads for the multi-head attention module. If
specified, this will override p.num_heads.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A transformer encoder layer params that supports optional stride.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.Stack">
<code class="sig-name descname">Stack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">blocks</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.Stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.Stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stack of sequential layers.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.Builder.TransformerEncoderStack">
<code class="sig-name descname">TransformerEncoderStack</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">num_layers</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#Builder.TransformerEncoderStack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.Builder.TransformerEncoderStack" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a stack of num_layers self-attention layers.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.batch_major_attention.LmBuilder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.batch_major_attention.</code><code class="sig-name descname">LmBuilder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.batch_major_attention.Builder" title="lingvo.core.batch_major_attention.Builder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.batch_major_attention.Builder</span></code></a></p>
<p>Langange model builder with causal padding.</p>
<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>The params of this layer.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._Var">
<code class="sig-name descname">_Var</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">weights</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._Var"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._Var" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._ShardedVar">
<code class="sig-name descname">_ShardedVar</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">mesh_split</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._ShardedVar"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._ShardedVar" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._LinearWeight">
<code class="sig-name descname">_LinearWeight</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">mesh_split</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._LinearWeight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._LinearWeight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._Linear">
<code class="sig-name descname">_Linear</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">input_dim</span></em>, <em class="sig-param"><span class="n">output_dim</span></em>, <em class="sig-param"><span class="n">mesh_split</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear layer. y = matmul([…, idims], [idims, odims]).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._BiasWeight">
<code class="sig-name descname">_BiasWeight</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._BiasWeight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._BiasWeight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._Bias">
<code class="sig-name descname">_Bias</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._Bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._Bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Bias layer. The bias is added to the last dimension of the input.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._MeshSplit">
<code class="sig-name descname">_MeshSplit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">tensor_split_dims_mapping</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._MeshSplit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._MeshSplit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder.MeshSplit">
<code class="sig-name descname">MeshSplit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">tensor_split_dims_mapping</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder.MeshSplit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder.MeshSplit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder.Feedforward">
<code class="sig-name descname">Feedforward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder.Feedforward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder.Feedforward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder._Attention">
<code class="sig-name descname">_Attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder._Attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder._Attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes self attention with optional stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of this layer.</p></li>
<li><p><strong>is_causal</strong> – If true, add cause per_step padding to the attention layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A self attention layer params.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.batch_major_attention.LmBuilder.TransformerEncoderLayer">
<code class="sig-name descname">TransformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">is_causal</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/batch_major_attention.html#LmBuilder.TransformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.batch_major_attention.LmBuilder.TransformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>(inputs, paddings) -&gt; (encoded, paddings).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the string name of the encoder layer params.</p></li>
<li><p><strong>is_causal</strong> – If true, add cause per_step padding to the attention layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A transformer encoder layer params that supports optional stride.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="lingvo.core.batch_utils.html" class="btn btn-neutral float-right" title="lingvo.core.batch_utils module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="lingvo.core.base_model_params.html" class="btn btn-neutral float-left" title="lingvo.core.base_model_params module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>