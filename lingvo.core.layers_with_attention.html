

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.core.layers_with_attention module &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.core.layers_with_gpipe module" href="lingvo.core.layers_with_gpipe.html" />
    <link rel="prev" title="lingvo.core.layers module" href="lingvo.core.layers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="lingvo.core.html#subpackages">Subpackages</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.core.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
        
          <li><a href="lingvo.core.html">lingvo.core package</a> &raquo;</li>
        
      <li>lingvo.core.layers_with_attention module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/lingvo.core.layers_with_attention.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-lingvo.core.layers_with_attention">
<span id="lingvo-core-layers-with-attention-module"></span><h1>lingvo.core.layers_with_attention module<a class="headerlink" href="#module-lingvo.core.layers_with_attention" title="Permalink to this headline">¶</a></h1>
<p>Lingvo layers that depend on attention layers but are not recurrent.</p>
<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerAttentionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Multi-headed attention, add and norm used by ‘Attention Is All You Need’.</p>
<p>This class implements the first sub-layer of Transformer Layer. Input is
first processed using a multi-headed (self) attention. Output of the
attention layer is combined with the residual connection. And the finally,
output is normalized using Layer Normalization.</p>
<p>Layer can be used in five scenarios:</p>
<ol class="arabic simple">
<li><p>Multi-Headed Self-Attention, where attention keys (source vectors),
attention values (context vectors) and queries come from the same previous
layer output, <code class="xref py py-obj docutils literal notranslate"><span class="pre">query_vec</span></code>. This is the general use case for encoder
Transformer Layers.</p></li>
<li><p>Masked Multi-Headed Self-Attention, where attention keys, attention values
and queries all come from the same previous layer output, but rightward
activations are masked to prevent information flow from future. This is the
use case for decoder self-attention Transformer Layers. Can be activated by
setting <code class="xref py py-obj docutils literal notranslate"><span class="pre">is_masked</span></code> flag of this layer.</p></li>
<li><p>Multi-Headed Attention, where attention keys and attention values
<code class="xref py py-obj docutils literal notranslate"><span class="pre">source_vecs</span></code>, are coming from a different source (output of the encoder)
and queries <code class="xref py py-obj docutils literal notranslate"><span class="pre">query_vec</span></code>, coming from the previous layer outputs (decoder).
This corresponds to the standard attention mechanism, decoder attending the
encoder outputs.</p></li>
<li><p>Multi-Headed Attention, where attention values <code class="xref py py-obj docutils literal notranslate"><span class="pre">context_vecs</span></code> are coming
from a different source than queries and keys, e.g. for positional
attention, where keys and queries are positional encodings and values are
decoder states.</p></li>
<li><p>Masked Multi-Headed Self-Attention, where attention keys, attention values
and queries all come from the same previous layer output, but the
activations for the current position are masked to reduce the impact of
high self-similarity. This is the use case for non-autoregressive decoder
self-attention Transformer Layers. Can be activated by setting <code class="xref py py-obj docutils literal notranslate"><span class="pre">is_masked</span></code>
flag of this layer and setting <code class="xref py py-obj docutils literal notranslate"><span class="pre">mask_type=&quot;eye&quot;</span></code>.</p></li>
<li><p>Masked Multi-Headed Self-Attention, where attention keys, attention values
and queries all come from the same previous layer output, but:
. rightward activations are masked to prevent information flow from future.
. leftward activations are also masked to prevent information flow from
past tokens that are beyond the N-gram context [K-N+1, K-1] when predicting
the target token in position K. This is the use case for decoder
self-attention Transformer Layers in N-gram mode. Can be activated by
setting <code class="xref py py-obj docutils literal notranslate"><span class="pre">is_masked</span></code> flag of this layer, and setting both
<code class="xref py py-obj docutils literal notranslate"><span class="pre">mask_type=&quot;ngram&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">mask_ngram_order=N-1</span></code> to use as context only the
previous N-1 tokens (as expected for an N-gram model); for details and
experimental results see <a class="reference external" href="https://arxiv.org/abs/2001.04589">https://arxiv.org/abs/2001.04589</a>.</p></li>
</ol>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer._InitAttention">
<code class="sig-name descname">_InitAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">atten_tpl</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer._InitAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer._InitAttention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer._GetSourceLength">
<code class="sig-name descname">_GetSourceLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer._GetSourceLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer._GetSourceLength" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">source_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">query_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">context_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer attention, residual and normalization layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_time, target_batch, dim]</p></li>
<li><p><strong>source_paddings</strong> – [source_time, source_batch]</p></li>
<li><p><strong>source_vecs</strong> – [source_time, source_batch, dim].</p></li>
<li><p><strong>query_segment_id</strong> – [target_time, target_batch]</p></li>
<li><p><strong>source_segment_id</strong> – [source_time, source_batch]</p></li>
<li><p><strong>context_vecs</strong> – [source_time, target_batch, dim]</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(output, atten_probs). output is of shape [target_time, target_batch,
context_dim], atten_probs is of shape [target_time, target_batch,
source_time].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer._FinishExtendStep">
<code class="sig-name descname">_FinishExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">unnormalized_query_vec</span></em>, <em class="sig-param"><span class="n">extended_packed_src</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer._FinishExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer._FinishExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Finish extending prefix by one more time step.</p>
<p>Isolating this function from ExtendStep allows generalizing self-attention
to causal attention on other inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>unnormalized_query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>extended_packed_src</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing source_vecs,
source_contexts, source_paddings, and source_segment_ids</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A triplet (cur_output, atten_prob, new_state) where cur_output is a tensor
representing the output from the current state, and new_state is the new
state <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerAttentionLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">prefix_state</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerAttentionLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend prefix by one more time step.</p>
<p>This function is expected to be called during fast decoding of the
Transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>prefix_state</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A triplet (cur_output, atten_prob, new_state) where cur_output is a tensor
representing the output from the current state, and new_state is the new
state <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerMultiSourceAttentionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerMultiSourceAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer" title="lingvo.core.layers_with_attention.TransformerAttentionLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.layers_with_attention.TransformerAttentionLayer</span></code></a></p>
<p>Multi-source multi-headed attention.</p>
<p>Only supports scenarios 3 and 4 in the base class. Now the two scenarios are:</p>
<ol class="arabic simple" start="3">
<li><p>Multi-source multi-Headed Attention, where attention keys and attention
values <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_vecs</span></code>, are different encodings and queries <code class="xref py py-obj docutils literal notranslate"><span class="pre">query_vec</span></code>,
coming from the previous layer outputs (decoder). In addition,
attention keys and values are NestedMaps containing encodings of different
sources. This corresponds to a multi-source decoder-to-encoder attention
mechanism, i.e., decoder attends to encoder outputs and other sources.</p></li>
<li><p>Similar to 3 but attention values <code class="xref py py-obj docutils literal notranslate"><span class="pre">context_vecs</span></code> are coming from a
different source than queries and keys.</p></li>
</ol>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerMultiSourceAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer._InitAttention">
<code class="sig-name descname">_InitAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">atten_tpl</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerMultiSourceAttentionLayer._InitAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer._InitAttention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer._GetSourceLength">
<code class="sig-name descname">_GetSourceLength</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerMultiSourceAttentionLayer._GetSourceLength"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerMultiSourceAttentionLayer._GetSourceLength" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerFeedForwardLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerFeedForwardLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerFeedForwardLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerFeedForwardLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Feed-forward, add and norm layer used by ‘Attention Is All You Need’.</p>
<p>This class implements the second sub-layer of Transformer Layer. First,
input passes through a feed-forward neural network with one hidden layer and
then projected back to the original input dimension to apply residual. Output
of the layer, is then normalized using Layer Normalization.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerFeedForwardLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerFeedForwardLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerFeedForwardLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerFeedForwardLayer.output_dim">
<em class="property">property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerFeedForwardLayer.output_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns output dimension of the transformer layer.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerFeedForwardLayer.NumOutputNodes">
<em class="property">classmethod </em><code class="sig-name descname">NumOutputNodes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerFeedForwardLayer.NumOutputNodes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerFeedForwardLayer.NumOutputNodes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerFeedForwardLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerFeedForwardLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerFeedForwardLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed-forward, residual and layer-norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>inputs</strong> – [time, batch, dim].</p></li>
<li><p><strong>paddings</strong> – [time, batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor of the same shape with inputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Transformer Layer proposed by ‘Attention Is All You Need’.</p>
<p>Applies self-attention followed by a feed forward network and
layer normalization. Uses residual connections between each consecutive
layer. In particular, adds residuals from layer input and attention output
and from attention output (feed-forward input) to feed-forward output.</p>
<p>Implements the transformer block in ‘Attention is All You Need’:
<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayer.output_dim">
<em class="property">property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer.output_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns output dimension of the transformer layer.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayer.NumOutputNodes">
<em class="property">classmethod </em><code class="sig-name descname">NumOutputNodes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayer.NumOutputNodes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer.NumOutputNodes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer.</p>
<p>Transformer layer has the naming scheme as follows: <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_vecs</span></code> and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">source_paddings</span></code> are all assumed to be coming from the activations of the
layer below. When <a class="reference internal" href="#lingvo.core.layers_with_attention.TransformerLayer" title="lingvo.core.layers_with_attention.TransformerLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TransformerLayer</span></code></a> is used in the Encoder (default
behavior of this layer) <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_*</span></code> tensors correspond to the outputs of
previous encoder layer. Further, keys, values and queries are all
forked from <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_vecs</span></code>. When TransformerLayer is used in the Decoder
(has_aux_atten=True), <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_*</span></code> tensors correspond to the outputs of
previous decoder layer and used as the queries.</p>
<p>For the cases when <a class="reference internal" href="#lingvo.core.layers_with_attention.TransformerLayer" title="lingvo.core.layers_with_attention.TransformerLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TransformerLayer</span></code></a> is used in the decoder
(has_aux_atten=True) <code class="xref py py-obj docutils literal notranslate"><span class="pre">aux_*</span></code> tensors have to be provided.  Auxiliary inputs,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">aux_*</span></code> tensors, are then correspond to the top-most layer encoder outputs
and used by the second <a class="reference internal" href="#lingvo.core.layers_with_attention.TransformerAttentionLayer" title="lingvo.core.layers_with_attention.TransformerAttentionLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TransformerAttentionLayer</span></code></a> as keys and values.</p>
<p>Regardless of the encoder or decoder, queries are always assumed to be
coming from the activations of layer below, in particular <code class="xref py py-obj docutils literal notranslate"><span class="pre">source_vecs</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_time, source_batch, dim].</p></li>
<li><p><strong>source_paddings</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>source_segment_id</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_segment_id</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [source_time, source_batch, dim].</p>
<p>The attention probability vector, [source_time, source_batch, source_time]
if has_aux_atten is False, otherwise [source_time, source_batch,
aux_time].</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer, extend one step in decoding.</p>
<p>This function is expected to be called during fast decoding of Transformer
models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_batch, dim].</p></li>
<li><p><strong>prefix_states</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [target_batch, source_dim]</p>
<p>The attention probability vector, [source_time, target_batch]</p>
<p>Updated prefix states</p>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">EvolvedTransformerEncoderBranchedConvsLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderBranchedConvsLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Evolved Transformer encoder branched convolutions layer.</p>
<p>This constructs the branched convolution portion of the Evolved Transformer
encoder described in <a class="reference external" href="https://arxiv.org/abs/1901.11117">https://arxiv.org/abs/1901.11117</a> .</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderBranchedConvsLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderBranchedConvsLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<p>The central interface that subclasses should implement. The caller
calls <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> dictionary. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">foo</span> <span class="o">=</span> <span class="n">InstanceOfASubClassOfFoo</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerEncoderBranchedConvsLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp()</span></code></a> computes a function given
the theta and the inputs. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
<span class="c1"># The same layer applied twice.</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">a2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>*args</strong> – List args.</p></li>
<li><p><strong>**kwargs</strong> – Keyward args.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">EvolvedTransformerDecoderBranchedConvsLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderBranchedConvsLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Evolved Transformer decoder branched convolutions layer.</p>
<p>This constructs the branched convolution portion of the Evolved Transformer
decoder described in <a class="reference external" href="https://arxiv.org/abs/1901.11117">https://arxiv.org/abs/1901.11117</a> .</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderBranchedConvsLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderBranchedConvsLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<p>The central interface that subclasses should implement. The caller
calls <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> dictionary. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">foo</span> <span class="o">=</span> <span class="n">InstanceOfASubClassOfFoo</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerDecoderBranchedConvsLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp()</span></code></a> computes a function given
the theta and the inputs. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
<span class="c1"># The same layer applied twice.</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">a2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>*args</strong> – List args.</p></li>
<li><p><strong>**kwargs</strong> – Keyward args.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">EvolvedTransformerBaseLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerBaseLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Base layer for the Evolved Transformer.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerBaseLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">EvolvedTransformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer" title="lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer</span></code></a></p>
<p>Evolved Transformer encoder layer.</p>
<p>An Evolved Transformer encoder layer as described in
<a class="reference external" href="https://arxiv.org/abs/1901.11117">https://arxiv.org/abs/1901.11117</a> .</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerEncoderLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<p>The central interface that subclasses should implement. The caller
calls <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> dictionary. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">foo</span> <span class="o">=</span> <span class="n">InstanceOfASubClassOfFoo</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerEncoderLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp()</span></code></a> computes a function given
the theta and the inputs. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
<span class="c1"># The same layer applied twice.</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">a2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>*args</strong> – List args.</p></li>
<li><p><strong>**kwargs</strong> – Keyward args.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">EvolvedTransformerDecoderLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer" title="lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.layers_with_attention.EvolvedTransformerBaseLayer</span></code></a></p>
<p>Evolved Transformer decoder layer.</p>
<p>An Evolved Transformer decoder layer as described in
<a class="reference external" href="https://arxiv.org/abs/1901.11117">https://arxiv.org/abs/1901.11117</a> .</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation.</p>
<p>The central interface that subclasses should implement. The caller
calls <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp</span></code></a> with a <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> dictionary. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">foo</span> <span class="o">=</span> <span class="n">InstanceOfASubClassOfFoo</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">foo</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of <a class="reference internal" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp" title="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.FProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FProp()</span></code></a> computes a function given
the theta and the inputs. E.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">subs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">a0</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
<span class="c1"># The same layer applied twice.</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">subs</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">FProp</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
<span class="k">return</span> <span class="n">a2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>*args</strong> – List args.</p></li>
<li><p><strong>**kwargs</strong> – Keyward args.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#EvolvedTransformerDecoderLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.EvolvedTransformerDecoderLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Evolved Transformer decoder layer, extended one step in decoding.</p>
<p>This function is expected to be called during fast decoding of Evolved
Transformer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_batch, dim].</p></li>
<li><p><strong>prefix_states</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [target_batch, source_dim].</p>
<p>The attention probability vector, [source_time, target_batch].</p>
<p>Updated prefix states.</p>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.StyleLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">StyleLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A layer that performs weighted style emb lookup.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.StyleLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.StyleLayer._CreateLayerVariables">
<code class="sig-name descname">_CreateLayerVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer._CreateLayerVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer._CreateLayerVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Actually create variables for this layer.</p>
<p>Subclasses should override this function.</p>
<p>Variables are created inside of tf.variable_scope(self.params.name).</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.StyleLayer.EmbLookup">
<code class="sig-name descname">EmbLookup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">ids</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer.EmbLookup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer.EmbLookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Looks up style embedding vectors for ids only for test purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – Named tuple with the weight matrix for the embedding.</p></li>
<li><p><strong>ids</strong> – A rank-N int32 tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>embs, A rank-(N+1) params.dtype tensor.
embs[indices, :] is the embedding vector for ids[indices].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.StyleLayer.StyleEmbFromProbs">
<code class="sig-name descname">StyleEmbFromProbs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inp</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer.StyleEmbFromProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer.StyleEmbFromProbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Look up style embedding based on feedin probabilities.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – params for this layer and its sub-layers.</p></li>
<li><p><strong>inp</strong> – attention probabilities of shape [batch_size, num_styles].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>style_emb - weighted combined style embedding based on inp.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.StyleLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inp</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#StyleLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.StyleLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Look up style embedding.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerLayerWithMultitaskAdapters</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayerWithMultitaskAdapters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.core.layers_with_attention.TransformerLayer" title="lingvo.core.layers_with_attention.TransformerLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.layers_with_attention.TransformerLayer</span></code></a></p>
<p>Transformer Layer with multitask residual adapters.</p>
<p>Applies transformer layer, followed by multitask adapters. Requires an
additional input specifying the task_id for each input.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayerWithMultitaskAdapters.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_task_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayerWithMultitaskAdapters.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer with multitask adapters.</p>
<p>First applies the standard transformer layer. Then applies adapter layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_time, source_batch, dim].</p></li>
<li><p><strong>source_paddings</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>source_segment_id</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_segment_id</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>source_task_id</strong> – [source_time, source_batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [source_time, source_batch, dim].</p>
<p>The attention probability vector, [source_time, source_batch, source_time]
if has_aux_atten is False, otherwise [source_time, source_batch,
aux_time].</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em>, <em class="sig-param"><span class="n">aux_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_paddings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">timestep</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_task_id</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerLayerWithMultitaskAdapters.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerLayerWithMultitaskAdapters.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer with adapters, extend one step in decoding.</p>
<p>Applies TransformerLayer.ExtendStep, then applies adapters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_batch, dim].</p></li>
<li><p><strong>prefix_states</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>timestep</strong> – a scalar, the current time step, 0-based.</p></li>
<li><p><strong>source_task_id</strong> – [source_batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [target_batch, source_dim]</p>
<p>The attention probability vector, [source_time, target_batch]</p>
<p>Updated prefix states</p>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.CCTAttentionLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">CCTAttentionLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTAttentionLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTAttentionLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Multi-headed attention, add and norm used by ‘Attention Is All You Need’.</p>
<p>Supports CCT attention gating as in the paper here:
<a class="reference external" href="https://arxiv.org/abs/2002.07106">https://arxiv.org/abs/2002.07106</a></p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTAttentionLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTAttentionLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTAttentionLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTAttentionLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">source_vecs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">query_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTAttentionLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTAttentionLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>CCT attention, residual and normalization layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_time, target_batch, dim]</p></li>
<li><p><strong>source_paddings</strong> – [source_time, source_batch]</p></li>
<li><p><strong>source_vecs</strong> – [source_time, source_batch, dim].</p></li>
<li><p><strong>query_segment_id</strong> – [target_time, target_batch]</p></li>
<li><p><strong>source_segment_id</strong> – [source_time, source_batch]</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(output, atten_probs). output is of shape [target_time, target_batch,
context_dim], atten_probs is of shape [target_time, target_batch,
source_time].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTAttentionLayer._FinishExtendStep">
<code class="sig-name descname">_FinishExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">unnormalized_query_vec</span></em>, <em class="sig-param"><span class="n">extended_packed_src</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTAttentionLayer._FinishExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTAttentionLayer._FinishExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Finish extending prefix by one more time step.</p>
<p>Isolating this function from ExtendStep allows generalizing self-attention
to causal attention on other inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>unnormalized_query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>extended_packed_src</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing source_vecs,
source_contexts, source_paddings, and source_segment_ids</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A triplet (cur_output, atten_prob, new_state) where cur_output is a tensor
representing the output from the current state, and new_state is the new
state <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTAttentionLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">query_vec</span></em>, <em class="sig-param"><span class="n">prefix_state</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTAttentionLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTAttentionLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend prefix by one more time step.</p>
<p>This function is expected to be called during fast decoding of the
Transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>query_vec</strong> – [target_batch, dim]</p></li>
<li><p><strong>prefix_state</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A triplet (cur_output, atten_prob, new_state) where cur_output is a tensor
representing the output from the current state, and new_state is the new
state <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.CCTFeedForwardLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">CCTFeedForwardLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTFeedForwardLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTFeedForwardLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>Transformer FF layer with CCT gating.</p>
<p><a class="reference external" href="https://arxiv.org/abs/2002.07106">https://arxiv.org/abs/2002.07106</a></p>
<p>Differences from standard Transformer FF layer:
1. Each feedforward layer is divided into num_blocks smaller layers (divided
along the hidden dimension).
2. Each block has its separate input layer norm.
3. Each block has its separate output layer norm.
4. Outputs from each block are gated with CCTGatingNetwork output - which is
between 0 and 1 for training and either 0 or 1 during inference.</p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTFeedForwardLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTFeedForwardLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTFeedForwardLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.CCTFeedForwardLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">paddings</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#CCTFeedForwardLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.CCTFeedForwardLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed-forward, layer-norm, residual, gating and layer-norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>inputs</strong> – [time, batch, dim].</p></li>
<li><p><strong>paddings</strong> – [time, batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor of the same shape with inputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.core.layers_with_attention.TransformerWithContextLayer">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.core.layers_with_attention.</code><code class="sig-name descname">TransformerWithContextLayer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerWithContextLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerWithContextLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_layer.html#lingvo.core.base_layer.BaseLayer" title="lingvo.core.base_layer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_layer.BaseLayer</span></code></a></p>
<p>A transformer layer with 3 attention layers.</p>
<p>The same as layers_with_attention.TransformerLayer, but with an
additional attention layer to attend to a third transformer stack
representing context.</p>
<p>self-attention =&gt; context attention (newly added as tertiary_atten) =&gt;
encoder attention (named aux_atten in TransformerLayer).</p>
<p>The weights are <em>not</em> shared between these three attention layers.</p>
<p>See <a class="reference external" href="https://arxiv.org/pdf/1810.03581.pdf">https://arxiv.org/pdf/1810.03581.pdf</a></p>
<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerWithContextLayer.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerWithContextLayer.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerWithContextLayer.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerWithContextLayer.FProp">
<code class="sig-name descname">FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">aux_vecs</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">tertiary_vecs</span></em>, <em class="sig-param"><span class="n">tertiary_paddings</span></em>, <em class="sig-param"><span class="n">source_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tertiary_segment_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerWithContextLayer.FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerWithContextLayer.FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer.</p>
<p>Please see docstring of TransformerAttentionLayer.FProp.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_time, source_batch, dim].</p></li>
<li><p><strong>source_paddings</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>tertiary_vecs</strong> – [tertiary_time, tertiary_batch, dim]</p></li>
<li><p><strong>tertiary_paddings</strong> – [tertiary_time, tertiary_batch]</p></li>
<li><p><strong>source_segment_id</strong> – [source_time, source_batch]</p></li>
<li><p><strong>aux_segment_id</strong> – [aux_time, aux_batch]</p></li>
<li><p><strong>tertiary_segment_id</strong> – [tertiary_time, tertiary_batch]</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [source_time, source_batch, dim].</p>
<p>The attention probability vector, [source_time, source_batch, aux_time].</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.core.layers_with_attention.TransformerWithContextLayer.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">source_vecs</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em>, <em class="sig-param"><span class="n">aux_vecs</span></em>, <em class="sig-param"><span class="n">aux_paddings</span></em>, <em class="sig-param"><span class="n">tertiary_vecs</span></em>, <em class="sig-param"><span class="n">tertiary_paddings</span></em>, <em class="sig-param"><span class="n">t</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/layers_with_attention.html#TransformerWithContextLayer.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.core.layers_with_attention.TransformerWithContextLayer.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Layer, extend one step in decoding.</p>
<p>Please see docstring of TransformerAttentionLayer.ExtendStep.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap" title="lingvo.core.py_utils.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>source_vecs</strong> – [source_batch, dim].</p></li>
<li><p><strong>prefix_states</strong> – dict, containing tensors which are the results of previous
attentions, used for fast decoding.</p></li>
<li><p><strong>aux_vecs</strong> – [aux_time, aux_batch, dim]</p></li>
<li><p><strong>aux_paddings</strong> – [aux_time, aux_batch] tertiary_vecs=None,
tertiary_paddings=None,</p></li>
<li><p><strong>tertiary_vecs</strong> – [tertiary_time, tertiary_batch, dim]</p></li>
<li><p><strong>tertiary_paddings</strong> – [tertiary_time, tertiary_batch]</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
<li><p><strong>**kwargs</strong> – Can be optional params for the attention layer, eg. attention
projection index tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The attention context vector, [target_batch, source_dim]</p>
<p>The attention probability vector from the encoder attention layer (the
last attention layer) only, [source_time, target_batch].
TODO(zhouwk): Return also the attention prob from the tertiary attention.</p>
<p>Updated prefix states</p>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="lingvo.core.layers_with_gpipe.html" class="btn btn-neutral float-right" title="lingvo.core.layers_with_gpipe module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="lingvo.core.layers.html" class="btn btn-neutral float-left" title="lingvo.core.layers module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>