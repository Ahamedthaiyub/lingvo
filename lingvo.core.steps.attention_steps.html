<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lingvo.core.steps.attention_steps module &mdash; Lingvo  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.core.steps.embedding_steps module" href="lingvo.core.steps.embedding_steps.html" />
    <link rel="prev" title="lingvo.core.steps package" href="lingvo.core.steps.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Lingvo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.core.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="lingvo.core.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Lingvo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
          <li><a href="lingvo.core.html">lingvo.core package</a> &raquo;</li>
          <li><a href="lingvo.core.steps.html">lingvo.core.steps package</a> &raquo;</li>
      <li>lingvo.core.steps.attention_steps module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lingvo.core.steps.attention_steps.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-lingvo.core.steps.attention_steps">
<span id="lingvo-core-steps-attention-steps-module"></span><h1>lingvo.core.steps.attention_steps module<a class="headerlink" href="#module-lingvo.core.steps.attention_steps" title="Permalink to this headline"></a></h1>
<p>Steps for attention computation.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lingvo.core.steps.attention_steps.</span></span><span class="sig-name descname"><span class="pre">AttentionStep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.step.html#lingvo.core.step.Step" title="lingvo.core.step.Step"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.step.Step</span></code></a></p>
<p>AttentionStep wraps an attention layer in the Step interface.</p>
<p>An attention algorithm outputs a targeted summary of a set of input vectors.</p>
<p>At each step, the query vector (input to FProp as step_inputs.input)
describes what data should be returned. The attention algorithm compares the
query vector with the source vectors (external_inputs.src) to compute
weights (attention_probs).</p>
<p>The result is the weighted sum (using attention_probs) of a set of vectors.
By default that set of vectors is also external_inputs.src, but it can
optionally be external_inputs.context if a context tensor is specified.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep.Params">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep.Params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep.Params" title="Permalink to this definition"></a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep.PrepareExternalInputs">
<span class="sig-name descname"><span class="pre">PrepareExternalInputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">external_inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep.PrepareExternalInputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep.PrepareExternalInputs" title="Permalink to this definition"></a></dt>
<dd><p>Prepare encoded source data for processing.</p>
<p>In some attention algorithms, this step will pre-process the source
feature data so that FProp runs faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>external_inputs</strong> – <p>A NestedMap containing tensors:</p>
<ul>
<li><p>src: a [time, batch, depth] tensor that forms the input to the
attention layer.</p></li>
<li><p>padding: a [time, batch] 0/1 tensor indicating which parts of
src contain useful information.</p></li>
<li><p>context: Optional. See the class documentation for more details.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>state0, a state parameter to pass to FProp on its first invocation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep._GetMaxSeqLength">
<span class="sig-name descname"><span class="pre">_GetMaxSeqLength</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_encs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep._GetMaxSeqLength"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep._GetMaxSeqLength" title="Permalink to this definition"></a></dt>
<dd><p>Compute the maximum sequence length of the encoded source sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>src_encs</strong> – Encoded source sequence pre-processed by using
PrepareExternalInputs. It can be either a [time, batch, depth] tensor
(when there is only one source) or a NestedMap of [time, batch, depth]
tensors (when there are more than one source).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the maximum sequence length of the encoded source
sequence. It can be either a scalar (when there is only one source) or
a NestedMap of scalars (when there are more than one source).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>max_seq_length</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep.ZeroState">
<span class="sig-name descname"><span class="pre">ZeroState</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepared_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep.ZeroState"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep.ZeroState" title="Permalink to this definition"></a></dt>
<dd><p>Produce a zero state for this step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>prepared_inputs</strong> – A set of inputs pre-processed by using
PrepareExternalInputs.</p></li>
<li><p><strong>batch_size</strong> – Number of elements in the batched input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>state0, a state parameter to pass to FProp on its first invocation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionStep.FProp">
<span class="sig-name descname"><span class="pre">FProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepared_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionStep.FProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionStep.FProp" title="Permalink to this definition"></a></dt>
<dd><p>Produces a context vector from the attention algorithm.</p>
<p>The context vector is a summary of the inputs from external_inputs
which the attention algorithm has determined would be useful for decoding
the next output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A NestedMap containing weights’ values of this layer and its
children layers.</p></li>
<li><p><strong>prepared_inputs</strong> – A set of encoded tensors that have been pre-processed by
PrepareExternalInputs.</p></li>
<li><p><strong>step_inputs</strong> – A NestedMap containing an ‘inputs’ tensor with the query
vector to use.</p></li>
<li><p><strong>padding</strong> – A [batch, 1] 0/1 float tensor, where 1.0 means that this batch
slot is not used.</p></li>
<li><p><strong>state0</strong> – A NestedMap of state, either produced by ZeroState or a previous
invocation of this graph.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>output: a NestedMap containing a query tensor, a context tensor, and
cum_atten_probs, the log of attention probabilities for each input
vector.</p></li>
<li><p>state1: a NestedMap of state to be used in subsequent invocations of
this graph.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output, state1, defined as follows</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionBlockStep">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lingvo.core.steps.attention_steps.</span></span><span class="sig-name descname"><span class="pre">AttentionBlockStep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionBlockStep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionBlockStep" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.step.html#lingvo.core.step.Step" title="lingvo.core.step.Step"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.step.Step</span></code></a></p>
<p>Computes attention queries and context vectors.</p>
<p>An attention algorithm produces a summary of a set of input vectors.
A query vector is used as an input to the process; we can think of this as
describing what information we’re hoping to retrieve in the summary.
The summary output is called a context vector.</p>
<p>This class uses attention as a way to view the input to a decoder.
In each step, we hope to read a summary of the encoded input that would be
most useful for generating the next output from the decoder.</p>
<p>To do this, we combine a query generator and an attention algorithm.
The query generator’s job is to build a query vector that represents the
current decoder state. The attention algorithm then uses that query vector
as a key to decide which encoded inputs are most important, and it combines
those into a context vector.</p>
<p>The query generator takes two inputs: one is the context vector output by
the attention algorithm in the previous step, and the other is the label
output by the decoder in the previous step. It combines this information
in an implementation-dependent way to produce a query vector.</p>
<p>In previous implementations, the query generator was just the first layer
of a stack of RNN layers (rnn_cell[0]). In this implementation, the query
generator can be any suitable Step, but in practice is one or more RNN layers.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionBlockStep.Params">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionBlockStep.Params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionBlockStep.Params" title="Permalink to this definition"></a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionBlockStep.ZeroState">
<span class="sig-name descname"><span class="pre">ZeroState</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepared_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionBlockStep.ZeroState"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionBlockStep.ZeroState" title="Permalink to this definition"></a></dt>
<dd><p>Produce a zero state for this step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>prepared_inputs</strong> – A set of inputs pre-processed by using
PrepareExternalInputs.</p></li>
<li><p><strong>batch_size</strong> – Number of elements in the batched input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>state0, a state parameter to pass to FProp on its first invocation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lingvo.core.steps.attention_steps.AttentionBlockStep.FProp">
<span class="sig-name descname"><span class="pre">FProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">theta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepared_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/core/steps/attention_steps.html#AttentionBlockStep.FProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lingvo.core.steps.attention_steps.AttentionBlockStep.FProp" title="Permalink to this definition"></a></dt>
<dd><p>Produces a query vector and a context vector for the next decoder step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>prepared_inputs</strong> – A set of encoded tensors that have been pre-processed by
PrepareExternalInputs.</p></li>
<li><p><strong>step_inputs</strong> – Unused. All of the input for this step comes from
external_inputs and previous step state.</p></li>
<li><p><strong>padding</strong> – A [batch, 1] 0/1 float tensor, where 1.0 means that this batch
slot is not used.</p></li>
<li><p><strong>state0</strong> – A NestedMap of state, either produced by ZeroState or a previous
invocation of this graph.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output, state1, are defined as follows.
output, a NestedMap containing an atten_query tensor,
an atten_context tensor, and atten_probs, attention probabilities for each
input vector.
state1, a NestedMap of state to be used in subsequent invocations of this
graph.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lingvo.core.steps.html" class="btn btn-neutral float-left" title="lingvo.core.steps package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lingvo.core.steps.embedding_steps.html" class="btn btn-neutral float-right" title="lingvo.core.steps.embedding_steps module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>