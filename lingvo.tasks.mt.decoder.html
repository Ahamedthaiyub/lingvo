

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>lingvo.tasks.mt.decoder module &mdash; Lingvo  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lingvo.tasks.mt.encoder module" href="lingvo.tasks.mt.encoder.html" />
    <link rel="prev" title="lingvo.tasks.mt.data_augmenter module" href="lingvo.tasks.mt.data_augmenter.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="lingvo.html">lingvo package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lingvo.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="lingvo.core.html">lingvo.core package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="lingvo.tasks.html">lingvo.tasks package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="lingvo.tasks.html#subpackages">Subpackages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lingvo.tools.html">lingvo.tools package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lingvo.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="lingvo.html">lingvo package</a> &raquo;</li>
        
          <li><a href="lingvo.tasks.html">lingvo.tasks package</a> &raquo;</li>
        
          <li><a href="lingvo.tasks.mt.html">lingvo.tasks.mt package</a> &raquo;</li>
        
      <li>lingvo.tasks.mt.decoder module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/lingvo.tasks.mt.decoder.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-lingvo.tasks.mt.decoder">
<span id="lingvo-tasks-mt-decoder-module"></span><h1>lingvo.tasks.mt.decoder module<a class="headerlink" href="#module-lingvo.tasks.mt.decoder" title="Permalink to this headline">¶</a></h1>
<p>Machine translation decoder.</p>
<dl class="py class">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.tasks.mt.decoder.</code><code class="sig-name descname">MTBaseDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_decoder.html#lingvo.core.base_decoder.BaseBeamSearchDecoder" title="lingvo.core.base_decoder.BaseBeamSearchDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_decoder.BaseBeamSearchDecoder</span></code></a></p>
<p>Base class for Lingvo MT decoders.</p>
<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder.UpdateTargetVocabSize">
<em class="property">classmethod </em><code class="sig-name descname">UpdateTargetVocabSize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">vocab_size</span></em>, <em class="sig-param"><span class="n">wpm_model</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder.UpdateTargetVocabSize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder.UpdateTargetVocabSize" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the params with the given vocab size and wpm model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – model params.</p></li>
<li><p><strong>vocab_size</strong> – size of the vocabulary.</p></li>
<li><p><strong>wpm_model</strong> – file name prefix pointing to a wordpiece model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Model params updated with the vocab size and wpm model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeXentLoss">
<code class="sig-name descname">_ComputeXentLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">softmax_input</span></em>, <em class="sig-param"><span class="n">target_labels</span></em>, <em class="sig-param"><span class="n">target_weights</span></em>, <em class="sig-param"><span class="n">target_paddings</span></em>, <em class="sig-param"><span class="n">target_segment_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_axis</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._ComputeXentLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeXentLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes cross-entropy loss given the softmax input, labels and weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this
layer and its children layers.</p></li>
<li><p><strong>softmax_input</strong> – A tensor of shape [time, batch, p.softmax.input_dim].</p></li>
<li><p><strong>target_labels</strong> – A matrix of tf.int32. [time, batch].</p></li>
<li><p><strong>target_weights</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>target_paddings</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>target_segment_ids</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>time_axis</strong> – If 0, the inputs are time-major: [time, batch, …]; if 1, the
inputs are batch-major: [batch, time, …].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The cross entropy loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeSoftmaxMetrics">
<code class="sig-name descname">_ComputeSoftmaxMetrics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">xent_loss</span></em>, <em class="sig-param"><span class="n">target_labels</span></em>, <em class="sig-param"><span class="n">target_weights</span></em>, <em class="sig-param"><span class="n">target_segment_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_axis</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._ComputeSoftmaxMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeSoftmaxMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes cross-entropy metrics given the cross-entropy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xent_loss</strong> – The output of <a class="reference internal" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeXentLoss" title="lingvo.tasks.mt.decoder.MTBaseDecoder._ComputeXentLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_ComputeXentLoss</span></code></a>.</p></li>
<li><p><strong>target_labels</strong> – A matrix of tf.int32. [time, batch].</p></li>
<li><p><strong>target_weights</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>target_segment_ids</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>time_axis</strong> – If 0, the inputs are time-major: [time, batch, …]; if 1, the
inputs are batch-major: [batch, time, …].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (metrics, per_example_tensors).</dt><dd><dl class="simple">
<dt>metrics:</dt><dd><p>A dictionary containing metrics for the xent loss and prediction
accuracy.</p>
</dd>
<dt>per_example_tensors:</dt><dd><p>A dictionary of per-example tensors.</p>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._FPropSoftmax">
<code class="sig-name descname">_FPropSoftmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">softmax_input</span></em>, <em class="sig-param"><span class="n">target_labels</span></em>, <em class="sig-param"><span class="n">target_weights</span></em>, <em class="sig-param"><span class="n">target_paddings</span></em>, <em class="sig-param"><span class="n">target_segment_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">time_axis</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._FPropSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._FPropSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes cross-entropy loss given the softmax input, labels and weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>softmax_input</strong> – A tensor of shape [time, batch, p.softmax.input_dim].</p></li>
<li><p><strong>target_labels</strong> – A matrix of tf.int32. [time, batch].</p></li>
<li><p><strong>target_weights</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>target_paddings</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>target_segment_ids</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>time_axis</strong> – If 0, the inputs are time-major: [time, batch, …]; if 1, the
inputs are batch-major: [batch, time, …].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (metrics, per_example_tensors).</dt><dd><dl class="simple">
<dt>metrics:</dt><dd><p>A dictionary containing metrics for the xent loss and prediction
accuracy.</p>
</dd>
<dt>per_example_tensors:</dt><dd><p>A dictionary of per-example tensors.</p>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder.ComputeLoss">
<code class="sig-name descname">ComputeLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">predictions</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder.ComputeLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder.ComputeLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates a metrics dictionary based on the output of ComputePredictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – Nested map describing decoder model parameters.</p></li>
<li><p><strong>predictions</strong> – NestedMap describing the decoding process, requiring:
.softmax_input: Tensor of shape [time, batch, params.softmax.input_dim].</p></li>
<li><p><strong>targets</strong> – NestedMap describing the target sequences.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Two dicts.</p>
<blockquote>
<div><ul class="simple">
<li><p>A map from metric name (a python string) to a tuple (value, weight).
Both value and weight are scalar Tensors.</p></li>
<li><p>A map from name to arbitrary tensors, where the first dimension must
be the batch index.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._TruncateTargetSequence">
<code class="sig-name descname">_TruncateTargetSequence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._TruncateTargetSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._TruncateTargetSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Truncate padded time steps from all sequences.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsSummary">
<code class="sig-name descname">_AddAttenProbsSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">targets</span></em>, <em class="sig-param"><span class="n">atten_probs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._AddAttenProbsSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsSummary" title="Permalink to this definition">¶</a></dt>
<dd><p>Add summary of attention probs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_paddings</strong> – source padding, of shape [src_len, src_batch].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [tgt_batch, tgt_len].</p></li>
<li><p><strong>atten_probs</strong> – a list of attention probs, each element is of shape [tgt_len,
tgt_batch, src_len].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsHistogramSummary">
<code class="sig-name descname">_AddAttenProbsHistogramSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">atten_probs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._AddAttenProbsHistogramSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsHistogramSummary" title="Permalink to this definition">¶</a></dt>
<dd><p>Add histogram summary of attention probs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>atten_probs</strong> – a list of attention probs, each element is of shape [tgt_len,
tgt_batch, src_len].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsImageSummary">
<code class="sig-name descname">_AddAttenProbsImageSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">targets</span></em>, <em class="sig-param"><span class="n">atten_probs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._AddAttenProbsImageSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._AddAttenProbsImageSummary" title="Permalink to this definition">¶</a></dt>
<dd><p>Add image summary of attention probs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_paddings</strong> – source padding, of shape [src_len, src_batch].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [tgt_batch, tgt_len].</p></li>
<li><p><strong>atten_probs</strong> – a list of attention probs, each element is of shape [tgt_len,
tgt_batch, src_len].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTBaseDecoder._ExpandToNumHyps">
<code class="sig-name descname">_ExpandToNumHyps</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_enc_len</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTBaseDecoder._ExpandToNumHyps"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTBaseDecoder._ExpandToNumHyps" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat each value according to num hyps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_enc_len</strong> – source encoder length; int [batch].</p></li>
<li><p><strong>num_hyps_per_beam</strong> – number of hypotheses</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>New version of source_enc_len; int [batch * num_hyps_per_beam].
Target_batch is (num_hyps_per_beam * batch).
Example: src_enc_len = [3, 2, 1] and num_hyps_per_beam = 2
–&gt; [3, 2, 1, 3, 2, 1]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.tasks.mt.decoder.</code><code class="sig-name descname">MTDecoderV1</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.tasks.mt.decoder.MTBaseDecoder" title="lingvo.tasks.mt.decoder.MTBaseDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.tasks.mt.decoder.MTBaseDecoder</span></code></a>, <a class="reference internal" href="lingvo.core.quant_utils.html#lingvo.core.quant_utils.QuantizableLayer" title="lingvo.core.quant_utils.QuantizableLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.quant_utils.QuantizableLayer</span></code></a></p>
<p>MT decoder v1.</p>
<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.UpdateTargetVocabSize">
<em class="property">classmethod </em><code class="sig-name descname">UpdateTargetVocabSize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">vocab_size</span></em>, <em class="sig-param"><span class="n">wpm_model</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1.UpdateTargetVocabSize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.UpdateTargetVocabSize" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the params with the input vocab_size and WPM model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – model params.</p></li>
<li><p><strong>vocab_size</strong> – size of the vocabulary.</p></li>
<li><p><strong>wpm_model</strong> – file name prefix pointing to a wordpiece model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Model params updated with the vocab size and wpm model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._CreateChildrenVariables">
<code class="sig-name descname">_CreateChildrenVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1._CreateChildrenVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._CreateChildrenVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Create variables for child layers.</p>
<p>Should be rarely overridden, only in cases when control over the context of
children InstantiateVariables calls are needed. eg, if children variables
need to be created inside of a specific context manager.</p>
<p>There are a few cases of this in the codebase marked as for backwards
compability. This is only to ensure that variable scopes remain compatible
through the code migration. New layers should not copy that pattern, and
instead follow the standard pattern of self.CreateChild() in __init__() and
self.CreateVariable() in _CreateLayerVariables(). If you are okay with
breaking old checkpoints, you can go ahead and delete those functions.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.ApplyDropout">
<code class="sig-name descname">ApplyDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x_in</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1.ApplyDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.ApplyDropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.ApplyClipping">
<code class="sig-name descname">ApplyClipping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1.ApplyClipping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.ApplyClipping" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.ComputePredictions">
<code class="sig-name descname">ComputePredictions</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.ComputePredictions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1.AddExtraDecodingInfo">
<code class="sig-name descname">AddExtraDecodingInfo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1.AddExtraDecodingInfo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1.AddExtraDecodingInfo" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds extra decoding information to encoded_outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder.</p></li>
<li><p><strong>targets</strong> – a NestedMap containing target input fields.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>encoder_ouputs with extra information used for decoding.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._InitDecoder">
<code class="sig-name descname">_InitDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._InitDecoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._DecodeStep">
<code class="sig-name descname">_DecodeStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._DecodeStep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._GetAttentionInitState">
<code class="sig-name descname">_GetAttentionInitState</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1._GetAttentionInitState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._GetAttentionInitState" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the attention initialization state.</p>
<p>It is valid to call this after <code class="xref py py-obj docutils literal notranslate"><span class="pre">_DecoderInit()</span></code>. Inference subclasses use
this to split computation across subgraph boundaries.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of attention source states.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._SetAttentionInitState">
<code class="sig-name descname">_SetAttentionInitState</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_init_state</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1._SetAttentionInitState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._SetAttentionInitState" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the attention initialization state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_init_state</strong> – <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> compatible with that returned from
<code class="xref py py-obj docutils literal notranslate"><span class="pre">_GetAttentionSourceState</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._InitBeamSearchStateCallback">
<code class="sig-name descname">_InitBeamSearchStateCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1._InitBeamSearchStateCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._InitBeamSearchStateCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns initial beams search states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – a NestedMap of parameters.</p></li>
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder.</p></li>
<li><p><strong>num_hyps_per_beam</strong> – An int, number hyps to keep for source sentence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (initial_results, states).</dt><dd><dl class="simple">
<dt>initial_results: a <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial results.</dt><dd><dl class="simple">
<dt>atten_probs:</dt><dd><p>The initial attention probs, of shape [tgt_batch, src_len].</p>
</dd>
</dl>
</dd>
<dt>states: a <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial model states.</dt><dd><dl class="simple">
<dt>rnn_states:</dt><dd><p>Initial state of the RNN.</p>
</dd>
<dt>atten_context:</dt><dd><p>Initial attention context vector.</p>
</dd>
<dt>atten_states:</dt><dd><p>Initial attention state.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._PreBeamSearchStepCallback">
<code class="sig-name descname">_PreBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._PreBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.MTDecoderV1._PostBeamSearchStepCallback">
<code class="sig-name descname">_PostBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_step_ids</span></em>, <em class="sig-param"><span class="n">states</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#MTDecoderV1._PostBeamSearchStepCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.MTDecoderV1._PostBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.tasks.mt.decoder.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.tasks.mt.decoder.MTBaseDecoder" title="lingvo.tasks.mt.decoder.MTBaseDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.tasks.mt.decoder.MTBaseDecoder</span></code></a></p>
<p>Transformer decoder.</p>
<p>Implements the decoder of Transformer model:
<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._CreateChildrenVariables">
<code class="sig-name descname">_CreateChildrenVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._CreateChildrenVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._CreateChildrenVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Create variables for child layers.</p>
<p>Should be rarely overridden, only in cases when control over the context of
children InstantiateVariables calls are needed. eg, if children variables
need to be created inside of a specific context manager.</p>
<p>There are a few cases of this in the codebase marked as for backwards
compability. This is only to ensure that variable scopes remain compatible
through the code migration. New layers should not copy that pattern, and
instead follow the standard pattern of self.CreateChild() in __init__() and
self.CreateVariable() in _CreateLayerVariables(). If you are okay with
breaking old checkpoints, you can go ahead and delete those functions.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._RemoveEOSProbs">
<code class="sig-name descname">_RemoveEOSProbs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">probs</span></em>, <em class="sig-param"><span class="n">source_enc_len</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._RemoveEOSProbs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._RemoveEOSProbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the attention probs on EOS symbol and renormalize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – decoder params.</p></li>
<li><p><strong>probs</strong> – attention probs matrix; float [batch, target_len, source_len].</p></li>
<li><p><strong>source_enc_len</strong> – source encoder length; int [batch].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>probs with value on last actual token (EOS token) replaced by 0 and
renormalized so that final dim (src_len) sums to 1 again; float
[batch, target_len, source_len].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._ZeroOutFirstTimeStep">
<code class="sig-name descname">_ZeroOutFirstTimeStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_embs</span></em>, <em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">target_time</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._ZeroOutFirstTimeStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._ZeroOutFirstTimeStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Zeroes out the first time step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_embs</strong> – [batch, time, model_dim] embeding lookups</p></li>
<li><p><strong>batch</strong> – Batch size scalar</p></li>
<li><p><strong>target_time</strong> – Target sequence length scalar.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>modified token_embs with the first time step zeroed out.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._FProp">
<code class="sig-name descname">_FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes <code class="xref py py-obj docutils literal notranslate"><span class="pre">targets</span></code> given encoded source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder. Expected to contain:
encoded - source encoding. When <code class="xref py py-obj docutils literal notranslate"><span class="pre">p.is_transparent</span></code> is False, it is a
tensor of shape [time, batch, depth]. When <code class="xref py py-obj docutils literal notranslate"><span class="pre">p.is_transparent</span></code> is True,
it is a tensor of shape [time, batch, depth, num_trans_layers] if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">self.do_eval</span></code> is True, and a list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_trans_layers</span></code> tensors of
shape [time, batch, depth] if <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.do_eval</span></code> is False.  padding - source
encoding’s padding, of shape [time, batch]. segment_id - source segment
id, of shape [time, batch].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [batch, time].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> containing output of last decoder layer and attention probs</p>
<ul class="simple">
<li><p>softmax_input: Tensor of shape [time, batch, params.softmax.input_dim].</p></li>
<li><p>attention: <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of attention distributions of shape
[batch, target_length, source_length].</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder.AddExtraDecodingInfo">
<code class="sig-name descname">AddExtraDecodingInfo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder.AddExtraDecodingInfo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder.AddExtraDecodingInfo" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds extra decoding information to encoded_outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder.</p></li>
<li><p><strong>targets</strong> – a NestedMap containing target input fields.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>encoder_ouputs with extra information used for decoding.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_ids</span></em>, <em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend prefix as represented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">prefix_states</span></code> by one more step.</p>
<p>This function is expected to be called during fast decoding of Transformer
models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – <p>a NestedMap computed by encoder, containing:</p>
<ul>
<li><p>encoded: source encoding, of shape [time, batch, depth]. Can be [time,
bs, depth, num_trans_layers] if is_transparent is set.</p></li>
<li><p>padding: source encoding’s padding, of shape [time, batch].</p></li>
</ul>
</p></li>
<li><p><strong>new_ids</strong> – new input ids, of shape [batch].</p></li>
<li><p><strong>t</strong> – a scalar, the current time step, 0-based.</p></li>
<li><p><strong>prefix_states</strong> – a <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the prefix that has already
been decoded.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple (last_decoder_out, prefix_states, atten_probs), where
last_decoder_out is the output of the last decoder layer of
shape [batch, model_dim], <code class="xref py py-obj docutils literal notranslate"><span class="pre">prefix_states</span></code> is the update prefix states,
and atten_probs contains attention in shape [batch, src_len] for the
given target position.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder.ComputePredictions">
<code class="sig-name descname">ComputePredictions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder.ComputePredictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder.ComputePredictions" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes <code class="xref py py-obj docutils literal notranslate"><span class="pre">targets</span></code> given encoded source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – <p>a NestedMap computed by encoder. Expected to contain:</p>
<dl class="simple">
<dt>encoded - source encoding, of shape [time, batch, depth]. Can be [time,</dt><dd><p>batch, depth, num_layers] if is_transparent is set.</p>
</dd>
</dl>
<p>padding - source encoding’s padding, of shape [time, batch].
segment_id - source segment id, of shape [time, batch].</p>
</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [batch, time].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> containing output of last decoder layer and attention probs</p>
<ul class="simple">
<li><p>softmax_input: Tensor of shape [time, batch, params.softmax.input_dim].</p></li>
<li><p>attention: <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of attention distributions of shape
[batch, time, source_len].</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder.SampleSequenceDecode">
<code class="sig-name descname">SampleSequenceDecode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_outputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder.SampleSequenceDecode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder.SampleSequenceDecode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode via sampling from softmax at each step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>encoder_outputs</strong> – the outputs of the encoder.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>BeamSearchDecodeOutput, same as what BeamSearchDecode returns.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._InitBeamSearchStateCallback">
<code class="sig-name descname">_InitBeamSearchStateCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._InitBeamSearchStateCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._InitBeamSearchStateCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns initial beams search states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder.</p></li>
<li><p><strong>num_hyps_per_beam</strong> – An int, number hyps to keep for source sentence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (initial_results, states).</dt><dd><dl class="simple">
<dt>initial_results: a <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial results.</dt><dd><dl class="simple">
<dt>atten_probs:</dt><dd><p>The initial attention probs, of shape [tgt_batch, src_len].</p>
</dd>
</dl>
</dd>
<dt>states: a <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial model states.</dt><dd><dl class="simple">
<dt>source_encs:</dt><dd><p>A tensor of shape [src_batch, src_len, source_dim].</p>
</dd>
<dt>source_paddings:</dt><dd><p>A tensor of shape [src_batch, src_len].</p>
</dd>
<dt>target_ids:</dt><dd><p>Initial empty list of decoded ids. [num_hyps, 0].</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._PreBeamSearchStepCallback">
<code class="sig-name descname">_PreBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">step_ids</span></em>, <em class="sig-param"><span class="n">states</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._PreBeamSearchStepCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._PreBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns logits for sampling ids and the next model states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – a NestedMap computed by encoder.</p></li>
<li><p><strong>step_ids</strong> – A tensor of shape [tgt_batch, 1].</p></li>
<li><p><strong>states</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of tensors representing states that the clients
would like to keep track of for each of the active hyps.</p></li>
<li><p><strong>num_hyps_per_beam</strong> – Beam size.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (results, out_states).</dt><dd><dl class="simple">
<dt>results: A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of beam search results.</dt><dd><dl class="simple">
<dt>atten_probs:</dt><dd><p>The updated attention probs, of shape [tgt_batch, src_len].</p>
</dd>
<dt>log_probs:</dt><dd><p>Log prob for each of the tokens in the target vocab. This is of
shape [tgt_batch, vocab_size].</p>
</dd>
</dl>
</dd>
<dt>out_states: A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>. The updated states.</dt><dd><dl class="simple">
<dt>source_encs:</dt><dd><p>A tensor of shape [src_batch, src_len, source_dim].</p>
</dd>
<dt>source_paddings:</dt><dd><p>A tensor of shape [src_batch, src_len].</p>
</dd>
<dt>target_ids:</dt><dd><p>Updated list of decoded ids. [num_hyps, Num of decoded ids].</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._PostBeamSearchStepCallback">
<code class="sig-name descname">_PostBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_step_ids</span></em>, <em class="sig-param"><span class="n">states</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._PostBeamSearchStepCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._PostBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._AddAttenProbsScalarSummary">
<code class="sig-name descname">_AddAttenProbsScalarSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">targets</span></em>, <em class="sig-param"><span class="n">atten_probs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._AddAttenProbsScalarSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._AddAttenProbsScalarSummary" title="Permalink to this definition">¶</a></dt>
<dd><p>Add scalar summary of multi-headed transformer attention probs.</p>
<p>This summary is primarily used to show statistics of the multi-headed
attention that reveals potential sparsity related properties. The
multi-headed attention probability tensors are exposed by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiHeadedAttention.ComputeContextVectorWithSource</span></code> with the name
<code class="xref py py-obj docutils literal notranslate"><span class="pre">multi_headed_atten_prob</span></code>. The following statistics are summarized:</p>
<ul class="simple">
<li><p>1_v_2: margin of the largest value vs. the 2nd largest</p></li>
<li><p>1_v_3: similar, but vs the 3rd largest</p></li>
<li><dl class="simple">
<dt>mean: mean of the attention probs. NOTE: the sequences in a mini-batch</dt><dd><p>are not always of the same length. The attention probability for the
padded time index in target sequences are removed. However, the padding
for the source sequences are left unchanged. As a result, the atten
probs vectors will have some extra zero entries, so the mean calculated
here will be smaller than the true mean.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>source_padding_ratio: as explained above, the source paddings are not</dt><dd><p>handled when computing the mean. This summary show the average ratio
of time-steps that are padded values in the source sequences, to give
a reference of roughly how much the mean summarized above should be
adjusted.</p>
</dd>
</dl>
</li>
<li><p>1_v_mean: margin of the largest value vs the mean value.</p></li>
<li><dl class="simple">
<dt>sum: the sum of the attention prob vectors. Should always be 1, for sanity</dt><dd><p>check only.</p>
</dd>
</dl>
</li>
</ul>
<p>The quantity above are computed for each sequence in the mini-batch, each
valid (target) sequence index, and each attention head, and then the
average value is reported to the tensorboard as a scalar summary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_paddings</strong> – source padding, of shape [src_len, src_batch].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [tgt_batch, tgt_len].</p></li>
<li><p><strong>atten_probs</strong> – a list of attention probs, each element is of shape [tgt_len,
tgt_batch, src_len].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerDecoder._AddAttenProbsSummary">
<code class="sig-name descname">_AddAttenProbsSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">source_paddings</span></em>, <em class="sig-param"><span class="n">targets</span></em>, <em class="sig-param"><span class="n">atten_probs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerDecoder._AddAttenProbsSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerDecoder._AddAttenProbsSummary" title="Permalink to this definition">¶</a></dt>
<dd><p>Add summary of attention probs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source_paddings</strong> – source padding, of shape [src_len, src_batch].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [tgt_batch, tgt_len].</p></li>
<li><p><strong>atten_probs</strong> – a list of attention probs, each element is of shape [tgt_len,
tgt_batch, src_len].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.tasks.mt.decoder.InsertionDecoder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.tasks.mt.decoder.</code><code class="sig-name descname">InsertionDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#InsertionDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.InsertionDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="lingvo.core.base_decoder.html#lingvo.core.base_decoder.BaseBeamSearchDecoder" title="lingvo.core.base_decoder.BaseBeamSearchDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.core.base_decoder.BaseBeamSearchDecoder</span></code></a></p>
<p>Basic Insertion decoder for MT (or any symbol based sequence).</p>
<p class="rubric">References</p>
<p>KERMIT: <a class="reference external" href="https://arxiv.org/pdf/1906.01604.pdf">https://arxiv.org/pdf/1906.01604.pdf</a>
Insertion Transformer: <a class="reference external" href="https://arxiv.org/pdf/1902.03249.pdf">https://arxiv.org/pdf/1902.03249.pdf</a></p>
<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.InsertionDecoder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#InsertionDecoder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.InsertionDecoder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.InsertionDecoder.UpdateTargetVocabSize">
<em class="property">classmethod </em><code class="sig-name descname">UpdateTargetVocabSize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">p</span></em>, <em class="sig-param"><span class="n">vocab_size</span></em>, <em class="sig-param"><span class="n">wpm_model</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#InsertionDecoder.UpdateTargetVocabSize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.InsertionDecoder.UpdateTargetVocabSize" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the vocab size in the params.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – model params.</p></li>
<li><p><strong>vocab_size</strong> – size of the vocabulary.</p></li>
<li><p><strong>wpm_model</strong> – file name prefix pointing to a wordpiece model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Model params updated with the vocab size and wpm model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.InsertionDecoder.ComputePredictions">
<code class="sig-name descname">ComputePredictions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#InsertionDecoder.ComputePredictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.InsertionDecoder.ComputePredictions" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute 1-step of the insertion iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – This should be None.</p></li>
<li><p><strong>targets</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.
- ids: The target ids of shape [batch_size, time_dim].
- paddings: The target paddings of shape [batch_size, time_dim].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</dt><dd><ul class="simple">
<li><p>outputs: The contextualized output vectors of shape
[batch_size, time_dim, model_dim].</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.InsertionDecoder.ComputeLoss">
<code class="sig-name descname">ComputeLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">predictions</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#InsertionDecoder.ComputeLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.InsertionDecoder.ComputeLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the insertion loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object capturing decoder model parameters.</p></li>
<li><p><strong>predictions</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> describing the decoding process, requiring
.outputs: Tensor of shape [time, batch, params.softmax.input_dim].</p></li>
<li><p><strong>targets</strong> – <p>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a>.</p>
<ul>
<li><p>target_indices: A Tensor capturing the relevant insertion tokens to
tf.gather_nd the log-probs.</p></li>
<li><p>target_weights: A Tensor capturing the relevant insertion tokens’
weights.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Two dicts.</dt><dd><ul class="simple">
<li><p>A map from metric name (a python string) to a tuple (value, weight).
Both value and weight are scalar Tensors.</p></li>
<li><p>A map from name to arbitrary tensors, where the first dimension must
be the batch index.</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder">
<em class="property">class </em><code class="sig-prename descclassname">lingvo.tasks.mt.decoder.</code><code class="sig-name descname">TransformerBatchMajorDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#lingvo.tasks.mt.decoder.MTBaseDecoder" title="lingvo.tasks.mt.decoder.MTBaseDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">lingvo.tasks.mt.decoder.MTBaseDecoder</span></code></a></p>
<p>Transformer decoder with batch major implementation.</p>
<p>Implements the decoder of Transformer model:
<a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.Params">
<em class="property">classmethod </em><code class="sig-name descname">Params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder.Params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.Params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the layer params.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._CreateChildrenVariables">
<code class="sig-name descname">_CreateChildrenVariables</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._CreateChildrenVariables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._CreateChildrenVariables" title="Permalink to this definition">¶</a></dt>
<dd><p>Create variables for child layers.</p>
<p>Should be rarely overridden, only in cases when control over the context of
children InstantiateVariables calls are needed. eg, if children variables
need to be created inside of a specific context manager.</p>
<p>There are a few cases of this in the codebase marked as for backwards
compability. This is only to ensure that variable scopes remain compatible
through the code migration. New layers should not copy that pattern, and
instead follow the standard pattern of self.CreateChild() in __init__() and
self.CreateVariable() in _CreateLayerVariables(). If you are okay with
breaking old checkpoints, you can go ahead and delete those functions.</p>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._MaybeTransposeEncoderOutputs">
<code class="sig-name descname">_MaybeTransposeEncoderOutputs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">target_data_format</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._MaybeTransposeEncoderOutputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._MaybeTransposeEncoderOutputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._MaybeTransposeTargets">
<code class="sig-name descname">_MaybeTransposeTargets</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._MaybeTransposeTargets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._MaybeTransposeTargets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._FProp">
<code class="sig-name descname">_FProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._FProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._FProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes <code class="xref py py-obj docutils literal notranslate"><span class="pre">targets</span></code> given encoded source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – A ‘.NestedMap’ object computed by encoder. * encoded -
Source encoding of shape [source_time, source_batch, dim] or
[source_batch, source_time, dim], depending on p.input_data_format. *
paddings - Source encoding’s padding of shape [source_time,
source_batch] or [source_batch, source_time].</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [batch, target_time].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor of shape [target_time, batch, dim].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>softmax_input</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ExtendStep">
<code class="sig-name descname">ExtendStep</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_ids</span></em>, <em class="sig-param"><span class="n">time_step</span></em>, <em class="sig-param"><span class="n">prefix_states</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder.ExtendStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ExtendStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Extend prefix as represented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">prefix_states</span></code> by one more step.</p>
<p>This function is expected to be called during fast decoding of Transformer
models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – <p>A ‘.NestedMap’ object computed by encoder.</p>
<ul>
<li><p>encoded: Source encoding of shape [source_time, source_batch, dim] or
[source_batch, source_time, dim], depending on p.input_data_format.</p></li>
<li><p>paddings: Source encoding’s padding of shape
[source_time, source_batch] or [source_batch, source_time].</p></li>
</ul>
</p></li>
<li><p><strong>new_ids</strong> – New input ids, of shape [target_batch, 1].</p></li>
<li><p><strong>time_step</strong> – A scalar, the current decode step, 0-based.</p></li>
<li><p><strong>prefix_states</strong> – <p>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the previous decoded states.</p>
<ul>
<li><p>key: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>value: [target_time, target_batch, num_heads, dim_per_head].</p></li>
</ul>
</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The last decoder layer of shape [target_batch, dim].
updated_prefix_states: A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the updated states.</p>
<blockquote>
<div><ul class="simple">
<li><p>key: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>value: [target_time, target_batch, num_heads, dim_per_head].</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>last_decoder_out</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ComputePredictions">
<code class="sig-name descname">ComputePredictions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder.ComputePredictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ComputePredictions" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes <code class="xref py py-obj docutils literal notranslate"><span class="pre">targets</span></code> given encoded source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – <p>A ‘.NestedMap’ object computed by encoder.</p>
<ul>
<li><p>encoded: Source encoding of shape [source_time, source_batch, dim] or
[source_batch, source_time, dim], depending on p.input_data_format.</p></li>
<li><p>paddings: Source encoding’s padding of shape
[source_time, source_batch] or [source_batch, source_time].</p></li>
</ul>
</p></li>
<li><p><strong>targets</strong> – A dict of string to tensors representing the targets one try to
predict. Each tensor in targets is of shape [batch, target_time].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of the last decoder layer, of shape [target_time, batch, dim].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._FPropFastSoftmax">
<code class="sig-name descname">_FPropFastSoftmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">softmax_input</span></em>, <em class="sig-param"><span class="n">target_labels</span></em>, <em class="sig-param"><span class="n">target_weights</span></em>, <em class="sig-param"><span class="n">time_axis</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._FPropFastSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._FPropFastSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes cross-entropy loss with label smoothing.</p>
<p>As compared to the _FPropSoftmax, this version is faster by removing the
data formatting overheads and bias of the linear projection. A normalizing
factor is also added to the xentropy result be better model quality.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>softmax_input</strong> – A tensor of shape [time, batch, p.softmax.input_dim].</p></li>
<li><p><strong>target_labels</strong> – A matrix of tf.int32. [time, batch].</p></li>
<li><p><strong>target_weights</strong> – A matrix of params.dtype. [time, batch].</p></li>
<li><p><strong>time_axis</strong> – If 0, the inputs are time-major: [time, batch, …]; if 1, the
inputs are batch-major: [batch, time, …].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple (metrics, per_example_tensors).</dt><dd><dl class="simple">
<dt>metrics:</dt><dd><p>A dictionary containing metrics for the xent loss and prediction
accuracy.</p>
</dd>
<dt>per_example_tensors:</dt><dd><p>A dictionary of per-example tensors.</p>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ComputeLoss">
<code class="sig-name descname">ComputeLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">predictions</span></em>, <em class="sig-param"><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder.ComputeLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder.ComputeLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates a metrics dictionary based on the output of ComputePredictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – Nested map describing decoder model parameters.</p></li>
<li><p><strong>predictions</strong> – NestedMap describing the decoding process, requiring:
.softmax_input: Tensor of shape [time, batch, params.softmax.input_dim].</p></li>
<li><p><strong>targets</strong> – NestedMap describing the target sequences.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Two dicts.</p>
<blockquote>
<div><ul class="simple">
<li><p>A map from metric name (a python string) to a tuple (value, weight).
Both value and weight are scalar Tensors.</p></li>
<li><p>A map from name to arbitrary tensors, where the first dimension must
be the batch index.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._InitBeamSearchStateCallback">
<code class="sig-name descname">_InitBeamSearchStateCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._InitBeamSearchStateCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._InitBeamSearchStateCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns initial beams search states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – A ‘.NestedMap’ object computed by encoder. * encoded -
Source encoding of shape [source_time, source_batch, dim] or
[source_batch, source_time, dim], depending on p.input_data_format. *
paddings - Source encoding’s padding of shape [source_time,
source_batch] or [source_batch, source_time].</p></li>
<li><p><strong>num_hyps_per_beam</strong> – An int, number hyps to keep for source sentence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial beam search results.</dt><dd><dl class="simple">
<dt>log_probs - Log prob for each of the tokens in the target vocab,</dt><dd><p>of shape [target_batch, vocab_size].</p>
</dd>
<dt>atten_probs - The updated attention probs, of shape</dt><dd><p>[target_batch, source_time].</p>
</dd>
</dl>
</dd>
<dt>states: A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of initial model states.</dt><dd><p>prefix_states - A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the empty decoded states.
key   - [target_time, target_batch, num_heads, dim_per_head].
value - [target_time, target_batch, num_heads, dim_per_head].
time_step - A scalar, the initial decode step (0).</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>initial_results</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._PreBeamSearchStepCallback">
<code class="sig-name descname">_PreBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_ids</span></em>, <em class="sig-param"><span class="n">states</span></em>, <em class="sig-param"><span class="n">num_hyps_per_beam</span></em>, <em class="sig-param"><span class="n">use_short_seq_opt</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._PreBeamSearchStepCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._PreBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns logits for sampling ids and the next model states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> – A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object containing weights’ values of this layer and
its children layers.</p></li>
<li><p><strong>encoder_outputs</strong> – <p>A ‘.NestedMap’ object computed by encoder.</p>
<ul>
<li><p>encoded: Source encoding of shape [source_time, source_batch, dim] or
[source_batch, source_time, dim], depending on p.input_data_format.</p></li>
<li><p>paddings: Source encoding’s padding of shape
[source_time, source_batch] or [source_batch, source_time].</p></li>
</ul>
</p></li>
<li><p><strong>new_ids</strong> – A tensor of shape [target_batch, 1].</p></li>
<li><p><strong>states</strong> – <p>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of tensors representing states that the clients
would like to keep track of for each of the active hyps. prefix_states -
A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the previous decoded states.</p>
<blockquote>
<div><ul>
<li><p>key: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>value: [target_time, target_batch, num_heads, dim_per_head].</p></li>
<li><p>time_step: A scalar, the current decode step, 0-based.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>num_hyps_per_beam</strong> – A scalar, beam size.</p></li>
<li><p><strong>use_short_seq_opt</strong> – A bool, whether using short sequence optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> of beam search results.</dt><dd><dl class="simple">
<dt>log_probs - Log prob for each of the tokens in the target vocab,</dt><dd><p>of shape [target_batch, vocab_size].</p>
</dd>
<dt>atten_probs - The updated attention probs, of shape</dt><dd><p>[target_batch, source_time].</p>
</dd>
</dl>
</dd>
<dt>new_states: A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> object. The updated states.</dt><dd><p>prefix_states - A <a class="reference internal" href="lingvo.core.nested_map.html#lingvo.core.nested_map.NestedMap" title="lingvo.core.nested_map.NestedMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NestedMap</span></code></a> representing the updated decoded states.
key   - [target_time, target_batch, num_heads, dim_per_head].
value - [target_time, target_batch, num_heads, dim_per_head].
time_step - A scalar, the current decode step, 0-based.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bs_results</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._PostBeamSearchStepCallback">
<code class="sig-name descname">_PostBeamSearchStepCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">theta</span></em>, <em class="sig-param"><span class="n">encoder_outputs</span></em>, <em class="sig-param"><span class="n">new_step_ids</span></em>, <em class="sig-param"><span class="n">states</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lingvo/tasks/mt/decoder.html#TransformerBatchMajorDecoder._PostBeamSearchStepCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lingvo.tasks.mt.decoder.TransformerBatchMajorDecoder._PostBeamSearchStepCallback" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="lingvo.tasks.mt.encoder.html" class="btn btn-neutral float-right" title="lingvo.tasks.mt.encoder module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="lingvo.tasks.mt.data_augmenter.html" class="btn btn-neutral float-left" title="lingvo.tasks.mt.data_augmenter module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>